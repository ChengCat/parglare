{
    "docs": [
        {
            "location": "/",
            "text": "A pure Python LR/GLR parser with integrated scanner.\n\n\n\n\nNote\n\n\nThe docs is work in progress. Stay tuned. :)\n\n\nUntil the docs is completed you can use \ntests\n and \nexamples\n as a good source\nof information on the possiblities and usage patterns.\n\n\n\n\nFeature highlights\n\u00b6\n\n\n\n\n\n\nScannerless parsing\n\n\nThere is no lexing as a separate phase. There is no separate lexer grammar.\nThe parser will try to reconize token during parsing at the given location.\nThis brings more parsing power as there are no lexical ambiguities\nintroduced by a separate lexing stage. You want variable names in your\nlanguage to be allowed to be like some of the keywords? No problem.\n\n\n\n\n\n\nGeneralized parsing - GLR\n\n\nparglare gives you powerful tools to see where non-determinism in your\ngrammar lies (the notorious shift-reduce and reduce-reduce conflicts) and\ngives detailed info on why that happened. In case your language needs\nnon-deterministic parsing \u2014 either it needs additional lookahead to decide\nor your language is inherently ambiguous \u2014 you can resort to the GLR\nalgorithm by a simple change of the parser class. The grammar stays the\nsame.\n\n\nIn the case of non-determinism (unability for a parser to deterministically\ndecide what to do) the parser will fork and investigate each possibility.\nEventualy, parsers that decided wrong will die leaving only the right one.\nIn case there are multiple interpretation of your input you will get all the\ntrees (a.k.a. \"the parse forest\").\n\n\n\n\n\n\nDeclarative associativity and priority rules\n\n\nThese problems arise a lot when building expression languages. Even a little\narithmetic expression as \n3 + 4 * 5 * 2\n have multiple interpretation\ndepending on the associativity and priority of operations. In parglare it is\neasy to specify these rules in the grammar (see the quick intro bellow\nor\n\nthe calc example\n).\n\n\n\n\n\n\nTracing/debuging, visualization and error reporting\n\n\nThere is an extensive support for grammar checking, debugging, automata\nvisualization, and parse tracing. Check out \ntodo: pglr command\n.\n\n\n\n\n\n\nParsing arbitrary list of object\n\n\nparglare is not used only to parse the textual content. It can parse (create\na tree) of an arbitrary list of objects (numbers, bytes, whatever) based on\nthe common parglare grammar. For this you have to\ndefine \ntodo: token recognizers\n for your input stream. The built-in\nrecognizers are string and regex recognizers for parsing textual inputs.\nSee \nrecognizers\n parameter to grammar construction in\nthe \ntest_parse_list_of_objects.py test\n.\n\n\n\n\n\n\nFlexible actions calling strategies\n\n\nDuring parsing you will want to do something when the grammar rule matches.\nThe whole point of parsing is that you want to transform your input to some\noutput. There are several options:\n- do nothing - this way your parser is a mere recognizer, it produces\n  nothing but just verifies that your input adhere to the grammar;\n- call default actions - the default actions will build a parse tree.\n- call user supplied actions - you write a Python function that is called\n  when the rule matches. You can do whatever you want at this place and the\n  result returned is used in parent rules/actions.\n\n\nBesides calling your actions in-line - during the parsing process - you can\ndecide to build the tree first and call custom actions afterwards. This is a\ngood option if you want to evaluate your tree in a multiple ways or if you\nare using GLR and want to be sure that actions are called only for the\nsurviving tree.\n\n\n\n\n\n\nSupport for whitespaces/comments\n\n\nSupport for language comments/whitespaces is done using the special rule\n\nLAYOUT\n. By default whitespaces are skipped. This is controlled by \nws\n\nparameter to the parser constructor which is by default set to \n\\t\\n\n. If\nset to \nNone\n no whitespace skipping is provided. If there is a rule\n\nLAYOUT\n in the grammar this rule is used instead. An additional parser with\nthe layout grammar will be built to handle whitespaces.\n\n\n\n\n\n\nError recovery\n\n\nThis is something that often lacks in parsing libraries. More often than not\nyou will want your parser to recover from an error, report it, and continue\nparsing. parglare has a built-in error recovery strategy which is currently\na simplistic one -- it will skip current character and try to continue --\nbut gives you possibility to provide your own. You will write a strategy\nthat will either skip input or introduce non-existing but expected tokens.\n\n\n\n\n\n\nTest coverage\n\n\nTest coverage is high and I'll try to keep it that way.\n\n\n\n\n\n\nTODO/Planed\n\u00b6\n\n\n\n\n\n\nDocs\n\n\nThis docs is currently work in progress. It should be done soon. Stay tuned.\n\n\n\n\n\n\nTable caching\n\n\nAt the moment parser tables are constructed on-the-fly which might be slow\nfor larger grammars. In the future tables will be recalculated only if the\ngrammar has changed and cached.\n\n\n\n\n\n\nSpecify common actions in the grammar\n\n\nparglare provides some commonly used custom actions. It would reduce\nboiler-plate in specification of these actions if a syntax is added to provide\nthat information in the grammar directly.\n\n\nExample:\n\n\n@collect\nsome_objects = some_objects some_object | some_object;\n\n\n\n\n\n\n\nSupport for named matches\n\n\nAt the moment, as a parameter to action you get a list of matched elements. It\nwould be useful to reference these element by name rather than by position.\n\n\nmy_rule = first:first_match_rule second:second_match_rule;\nfirst_match_rule = ...;\nsecond_match_rule = ...;\n\n\n\nNow in your action for \nmy_rule\n you will get \nfirst\n and \nsecond\n as a parameters.\nThis would make it easy to provide a new common action that will return a Python\nobject with supplied parameters as object attributes.\n\n\n\n\n\n\nGLR performance optimization\n\n\nThe GLR parsing has a lot of overhead compared to LR which makes it slower.\nThere are some technique that could be used to cut on this difference in\nspeed.\n\n\n\n\n\n\nQuick intro\n\u00b6\n\n\nThis is just a small example to get the general idea. This example shows how to\nparse and evaluate expressions with 5 operations with different priority and\nassociativity. Evaluation is done using semantic/reduction actions.\n\n\nThe whole expression evaluator is done in under 30 lines of code!\n\n\nfrom parglare import Parser, Grammar\n\ngrammar = r\"\"\"\nE = E '+' E  {left, 1}\n  | E '-' E  {left, 1}\n  | E '*' E  {left, 2}\n  | E '/' E  {left, 2}\n  | E '^' E  {right, 3}\n  | '(' E ')'\n  | number;\nnumber = /\\d+(\\.\\d+)?/;\n\"\"\"\n\nactions = {\n    \"E\": [lambda _, nodes: nodes[0] + nodes[2],\n          lambda _, nodes: nodes[0] - nodes[2],\n          lambda _, nodes: nodes[0] * nodes[2],\n          lambda _, nodes: nodes[0] / nodes[2],\n          lambda _, nodes: nodes[0] ** nodes[2],\n          lambda _, nodes: nodes[1],\n          lambda _, nodes: nodes[0]],\n    \"number\": lambda _, value: float(value),\n}\n\ng = Grammar.from_string(grammar)\nparser = Parser(g, debug=True, actions=actions)\n\nresult = parser.parse(\"34 + 4.6 / 2 * 4^2^2 + 78\")\n\nprint(\"Result = \", result)\n\n# Output\n# -- Debuging/tracing output with detailed info about grammar, productions,\n# -- terminals and nonterminals, DFA states, parsing progress,\n# -- and at the end of the output:\n# Result = 700.8\n\n\n\n\nNote on LR tables calculation\n\u00b6\n\n\nparglare provides both SLR and LALR tables calculation (LALR is the default).\nLALR is modified to avoid REDUCE/REDUCE conflicts on state merging. Although\nnot proven, this should enable handling of all LR(1) grammars with reduced set\nof states and without conflicts. For grammars that are not LR(1) a GLR parsing\nis provided.\n\n\nNote on lexical disambiguation\n\u00b6\n\n\nLexical ambiguity arise if multiple recognizers match at the same location.\nLexical disambiguation is done in the following order:\n\n\n\n\nPriorities are used first.\n\n\nString recognizers are preferred over regexes (i.e. the most specific match).\n\n\nThe longest-match strategy is used if multiple regexes matches with the same\n  priority. For further disambiguation if longest-match fails \nprefer\n rule\n  may be given in terminal definition.\n\n\n\n\nNotice that, since the scanning is integrated into parser, the lexical ambiguity\nmay arise only if there is a real ambiguity in the language definition, i.e. you\nmight have two or more tokens recognized at some place during parsing. For\napproaches where scanning is separate, lexical ambiguities arise whenever you\nhave token recognition overlap no matter what the context is, which is a real\nPITA.\n\n\nWhat does \nparglare\n mean?\n\u00b6\n\n\nIt is an amalgam of the words \nparser\n and \nglare\n where the second word is\nchosen to contain letters GLR and to be easy for pronunciation. I also like one\nof the translations for the word - \nto be very bright and intense\n\n(by \nThe Free Dictionary\n)\n\n\nOh, and the name is non-generic and unique which make it easy to find on the\nnet. ;)\n\n\nThe project logo is my take (not very successful) on drawing the Hydra, a\ncreature with multiple heads from the Greek mythology. According to the legend,\nthe Hydra had a regeneration feature: for every head chopped off, the Hydra\nwould regrow a couple of new heads. That reminds me a lot of the GLR parsing ;)\n\n\nLicense\n\u00b6\n\n\nMIT\n\n\nPython versions\n\u00b6\n\n\nTested with 2.7, 3.3-3.6",
            "title": "Home"
        },
        {
            "location": "/#feature-highlights",
            "text": "Scannerless parsing  There is no lexing as a separate phase. There is no separate lexer grammar.\nThe parser will try to reconize token during parsing at the given location.\nThis brings more parsing power as there are no lexical ambiguities\nintroduced by a separate lexing stage. You want variable names in your\nlanguage to be allowed to be like some of the keywords? No problem.    Generalized parsing - GLR  parglare gives you powerful tools to see where non-determinism in your\ngrammar lies (the notorious shift-reduce and reduce-reduce conflicts) and\ngives detailed info on why that happened. In case your language needs\nnon-deterministic parsing \u2014 either it needs additional lookahead to decide\nor your language is inherently ambiguous \u2014 you can resort to the GLR\nalgorithm by a simple change of the parser class. The grammar stays the\nsame.  In the case of non-determinism (unability for a parser to deterministically\ndecide what to do) the parser will fork and investigate each possibility.\nEventualy, parsers that decided wrong will die leaving only the right one.\nIn case there are multiple interpretation of your input you will get all the\ntrees (a.k.a. \"the parse forest\").    Declarative associativity and priority rules  These problems arise a lot when building expression languages. Even a little\narithmetic expression as  3 + 4 * 5 * 2  have multiple interpretation\ndepending on the associativity and priority of operations. In parglare it is\neasy to specify these rules in the grammar (see the quick intro bellow\nor the calc example ).    Tracing/debuging, visualization and error reporting  There is an extensive support for grammar checking, debugging, automata\nvisualization, and parse tracing. Check out  todo: pglr command .    Parsing arbitrary list of object  parglare is not used only to parse the textual content. It can parse (create\na tree) of an arbitrary list of objects (numbers, bytes, whatever) based on\nthe common parglare grammar. For this you have to\ndefine  todo: token recognizers  for your input stream. The built-in\nrecognizers are string and regex recognizers for parsing textual inputs.\nSee  recognizers  parameter to grammar construction in\nthe  test_parse_list_of_objects.py test .    Flexible actions calling strategies  During parsing you will want to do something when the grammar rule matches.\nThe whole point of parsing is that you want to transform your input to some\noutput. There are several options:\n- do nothing - this way your parser is a mere recognizer, it produces\n  nothing but just verifies that your input adhere to the grammar;\n- call default actions - the default actions will build a parse tree.\n- call user supplied actions - you write a Python function that is called\n  when the rule matches. You can do whatever you want at this place and the\n  result returned is used in parent rules/actions.  Besides calling your actions in-line - during the parsing process - you can\ndecide to build the tree first and call custom actions afterwards. This is a\ngood option if you want to evaluate your tree in a multiple ways or if you\nare using GLR and want to be sure that actions are called only for the\nsurviving tree.    Support for whitespaces/comments  Support for language comments/whitespaces is done using the special rule LAYOUT . By default whitespaces are skipped. This is controlled by  ws \nparameter to the parser constructor which is by default set to  \\t\\n . If\nset to  None  no whitespace skipping is provided. If there is a rule LAYOUT  in the grammar this rule is used instead. An additional parser with\nthe layout grammar will be built to handle whitespaces.    Error recovery  This is something that often lacks in parsing libraries. More often than not\nyou will want your parser to recover from an error, report it, and continue\nparsing. parglare has a built-in error recovery strategy which is currently\na simplistic one -- it will skip current character and try to continue --\nbut gives you possibility to provide your own. You will write a strategy\nthat will either skip input or introduce non-existing but expected tokens.    Test coverage  Test coverage is high and I'll try to keep it that way.",
            "title": "Feature highlights"
        },
        {
            "location": "/#todoplaned",
            "text": "Docs  This docs is currently work in progress. It should be done soon. Stay tuned.    Table caching  At the moment parser tables are constructed on-the-fly which might be slow\nfor larger grammars. In the future tables will be recalculated only if the\ngrammar has changed and cached.    Specify common actions in the grammar  parglare provides some commonly used custom actions. It would reduce\nboiler-plate in specification of these actions if a syntax is added to provide\nthat information in the grammar directly.  Example:  @collect\nsome_objects = some_objects some_object | some_object;    Support for named matches  At the moment, as a parameter to action you get a list of matched elements. It\nwould be useful to reference these element by name rather than by position.  my_rule = first:first_match_rule second:second_match_rule;\nfirst_match_rule = ...;\nsecond_match_rule = ...;  Now in your action for  my_rule  you will get  first  and  second  as a parameters.\nThis would make it easy to provide a new common action that will return a Python\nobject with supplied parameters as object attributes.    GLR performance optimization  The GLR parsing has a lot of overhead compared to LR which makes it slower.\nThere are some technique that could be used to cut on this difference in\nspeed.",
            "title": "TODO/Planed"
        },
        {
            "location": "/#quick-intro",
            "text": "This is just a small example to get the general idea. This example shows how to\nparse and evaluate expressions with 5 operations with different priority and\nassociativity. Evaluation is done using semantic/reduction actions.  The whole expression evaluator is done in under 30 lines of code!  from parglare import Parser, Grammar\n\ngrammar = r\"\"\"\nE = E '+' E  {left, 1}\n  | E '-' E  {left, 1}\n  | E '*' E  {left, 2}\n  | E '/' E  {left, 2}\n  | E '^' E  {right, 3}\n  | '(' E ')'\n  | number;\nnumber = /\\d+(\\.\\d+)?/;\n\"\"\"\n\nactions = {\n    \"E\": [lambda _, nodes: nodes[0] + nodes[2],\n          lambda _, nodes: nodes[0] - nodes[2],\n          lambda _, nodes: nodes[0] * nodes[2],\n          lambda _, nodes: nodes[0] / nodes[2],\n          lambda _, nodes: nodes[0] ** nodes[2],\n          lambda _, nodes: nodes[1],\n          lambda _, nodes: nodes[0]],\n    \"number\": lambda _, value: float(value),\n}\n\ng = Grammar.from_string(grammar)\nparser = Parser(g, debug=True, actions=actions)\n\nresult = parser.parse(\"34 + 4.6 / 2 * 4^2^2 + 78\")\n\nprint(\"Result = \", result)\n\n# Output\n# -- Debuging/tracing output with detailed info about grammar, productions,\n# -- terminals and nonterminals, DFA states, parsing progress,\n# -- and at the end of the output:\n# Result = 700.8",
            "title": "Quick intro"
        },
        {
            "location": "/#note-on-lr-tables-calculation",
            "text": "parglare provides both SLR and LALR tables calculation (LALR is the default).\nLALR is modified to avoid REDUCE/REDUCE conflicts on state merging. Although\nnot proven, this should enable handling of all LR(1) grammars with reduced set\nof states and without conflicts. For grammars that are not LR(1) a GLR parsing\nis provided.",
            "title": "Note on LR tables calculation"
        },
        {
            "location": "/#note-on-lexical-disambiguation",
            "text": "Lexical ambiguity arise if multiple recognizers match at the same location.\nLexical disambiguation is done in the following order:   Priorities are used first.  String recognizers are preferred over regexes (i.e. the most specific match).  The longest-match strategy is used if multiple regexes matches with the same\n  priority. For further disambiguation if longest-match fails  prefer  rule\n  may be given in terminal definition.   Notice that, since the scanning is integrated into parser, the lexical ambiguity\nmay arise only if there is a real ambiguity in the language definition, i.e. you\nmight have two or more tokens recognized at some place during parsing. For\napproaches where scanning is separate, lexical ambiguities arise whenever you\nhave token recognition overlap no matter what the context is, which is a real\nPITA.",
            "title": "Note on lexical disambiguation"
        },
        {
            "location": "/#what-does-parglare-mean",
            "text": "It is an amalgam of the words  parser  and  glare  where the second word is\nchosen to contain letters GLR and to be easy for pronunciation. I also like one\nof the translations for the word -  to be very bright and intense \n(by  The Free Dictionary )  Oh, and the name is non-generic and unique which make it easy to find on the\nnet. ;)  The project logo is my take (not very successful) on drawing the Hydra, a\ncreature with multiple heads from the Greek mythology. According to the legend,\nthe Hydra had a regeneration feature: for every head chopped off, the Hydra\nwould regrow a couple of new heads. That reminds me a lot of the GLR parsing ;)",
            "title": "What does parglare mean?"
        },
        {
            "location": "/#license",
            "text": "MIT",
            "title": "License"
        },
        {
            "location": "/#python-versions",
            "text": "Tested with 2.7, 3.3-3.6",
            "title": "Python versions"
        },
        {
            "location": "/getting_started/",
            "text": "Getting started\n\u00b6\n\n\nThe first thing to do is to write your language grammar using\nthe \nparglare grammar language\n. You write the grammar either as a\nPython string in your source code or as a separate file. In case you are writing\na grammar of a complex language I would suggest the separate file approach.\n\n\nThe next step is to create the instance of the \nGrammar\n class. This is\nachieved by importing the \nGrammar\n class and calling either \nfrom_file\n or\n\nfrom_str\n methods supplying the file name for the former and the Python string\nfor the later call.\n\n\nfrom parglare import Grammar\n\nfile_name = .....\ngrammar = Grammar.from_file(file_name)\n\n\n\n\nIf there is no errors in the grammar you now have the grammar instance.\n\n\n\n\nNote\n\n\nThere is also a handy \nTODO:pglr command line tool\n that can be used for grammar\nchecking, visualization and debugging.\n\n\n\n\nThe next step is to create an instance of the parser. There are two options. If\nyou want to use LR parser instantiate \nParser\n class. For GLR instantiate\n\nGLRParser\n class.\n\n\nfrom parglare import Parser\nparser = Parser(grammar)\n\n\n\n\nor\n\n\nfrom parglare import GLRParser\nparser = GLRParser(grammar)\n\n\n\n\nYou can provide additional \nTODO:parser parameters\n during instantiation.\n\n\n\n\nNote\n\n\nLR parser is faster as the GLR machinery brings a significant\noverhead. So, the general advice is to stick to the LR parsing until you are\nsure that you need additional power of GLR, i.e. either you need more than one\ntoken of lookahead or your language is inherently ambiguous. pglr tool will\nhelp you in investigating why you have LR conflicts in your grammar and\nthere are some nice \nTODO:disambiguation features\n in parglare that will help you\nresolve some of those conflicts.\n\n\n\n\nNow parse your input calling \nparse\n method on the parser instance.\n\n\nresult = parser.parse(input_str)\n\n\n\n\nDepending on whether you have configured \nactions\n or not you will\nget a parse tree or some other representation of your input. In case of the GLR\nparser you will get the list of all possible results (a.k.a. \nthe parser\nforest\n).",
            "title": "Getting started"
        },
        {
            "location": "/getting_started/#getting-started",
            "text": "The first thing to do is to write your language grammar using\nthe  parglare grammar language . You write the grammar either as a\nPython string in your source code or as a separate file. In case you are writing\na grammar of a complex language I would suggest the separate file approach.  The next step is to create the instance of the  Grammar  class. This is\nachieved by importing the  Grammar  class and calling either  from_file  or from_str  methods supplying the file name for the former and the Python string\nfor the later call.  from parglare import Grammar\n\nfile_name = .....\ngrammar = Grammar.from_file(file_name)  If there is no errors in the grammar you now have the grammar instance.   Note  There is also a handy  TODO:pglr command line tool  that can be used for grammar\nchecking, visualization and debugging.   The next step is to create an instance of the parser. There are two options. If\nyou want to use LR parser instantiate  Parser  class. For GLR instantiate GLRParser  class.  from parglare import Parser\nparser = Parser(grammar)  or  from parglare import GLRParser\nparser = GLRParser(grammar)  You can provide additional  TODO:parser parameters  during instantiation.   Note  LR parser is faster as the GLR machinery brings a significant\noverhead. So, the general advice is to stick to the LR parsing until you are\nsure that you need additional power of GLR, i.e. either you need more than one\ntoken of lookahead or your language is inherently ambiguous. pglr tool will\nhelp you in investigating why you have LR conflicts in your grammar and\nthere are some nice  TODO:disambiguation features  in parglare that will help you\nresolve some of those conflicts.   Now parse your input calling  parse  method on the parser instance.  result = parser.parse(input_str)  Depending on whether you have configured  actions  or not you will\nget a parse tree or some other representation of your input. In case of the GLR\nparser you will get the list of all possible results (a.k.a.  the parser\nforest ).",
            "title": "Getting started"
        },
        {
            "location": "/grammar/",
            "text": "The parglare grammar language\n\u00b6\n\n\nparglare grammar specification language is based\non \nBNF\n. parglare is\nbased\non\n\nContext-Free Grammars (CFGs)\n and\ngrammar is given declaratively. You don't have to think about the parsing\nprocess like in\ne.g. \nPEGs\n.\nAmbiguities are dealt with explicitely (see section on conflicts...).\n\n\nGrammar consists of a set of derivation rules where each rule is of the form:\n\n\n<symbol> = <expression> ;\n\n\n\n\nwhere \n<symbol>\n is grammar non-terminal and \n<expression>\n is a sequence of\nterminals and non-terminals separated by choice operator \n|\n.\n\n\nFor example:\n\n\nFields = Field | Fields \",\" Field;\n\n\n\n\nHere \nFields\n is a non-terminal grammar symbol and it is defined as either a\nsingle \nField\n or, recursively, as \nFields\n followed by a string terminal \n,\n\nand than by another \nField\n. It is not given here but \nField\n could also be\ndefined as a non-terminal. For example:\n\n\nField = QuotedField | FieldContent;\n\n\n\n\nOr it could be defined as a terminal:\n\n\nField = /[A-Z]*/;\n\n\n\n\nThis terminal definition uses regular expression recognizer.\n\n\n\n\nNote\n\n\nIf you got use to various BNF extensions\n(like \nKleene star\n) you might find\nthis awkward because you must build \nzero or more\n or \none or more\n pattern from\nscratch using just a sequence, choice and recursion. The grammars are indeed\nmore verbose but, on the other hand, actions are much easier to write and you\nhave full control over tree construction process. parglare might provide some\nsyntactic sugar later that would make some constructs shorter to write.\n\n\n\n\nTerminals\n\u00b6\n\n\nTerminal symbols of the grammar define the fundamental or atomic elements of\nyour language -- tokens or lexemes (e.g. keywords, numbers). In parglare\nterminal is connected to recognizer which is an object used to recognize token\nof particular type in the input. Most of the time you will do parsing of textual\ncontent and you will need textual recognizers. These recognizers are built-in\nand there are two type of textual recognizers:\n\n\n\n\nstring recognizer\n\n\nregular expression recognizer\n\n\n\n\nString recognizer\n\u00b6\n\n\nString recognizer is defined as a plain string inside of double quotes:\n\n\nmy_rule = \"start\" other_rule \"end\";\n\n\n\nIn this example \n\"start\"\n and \n\"end\"\n will be terminals with string recognizers\nthat match exactly the words \nstart\n and \nend\n.\n\n\nYou can write string recognizing terminal directly in the rule expression or you\ncan define terminal separately and reference it by name, like:\n\n\nmy_rule = start other_rule end;\nstart = \"start\";\nend = \"end\";\n\n\n\nEither way it will be the same terminal. You will usually write as a separate\nterminal if the terminal is used at multiple places in the grammar.\n\n\nRegular expression recognizer\n\u00b6\n\n\nOr regex recognizer for short is a regex pattern written inside slashes\n(\n/.../\n).\n\n\nFor example:\n\n\n number = /\\d+/;\n\n\n\nThis rule defines terminal symbol \nnumber\n which has a regex recognizer and will\nrecognize one or more digits as a number.\n\n\nCustom recognizers\n\u00b6\n\n\nIf you are parsing arbitrary input (non-textual) you'll have to provide your own\nrecognizers. In the grammar, you just have to reference your terminal symbol but\nyou don't have to provide the definition. You will provide missing recognizers\nduring grammar instantiation from Python.\n\n\nLets say that we have a list of integers (real list of Python ints, not a text\nwith numbers) and we have some weird requirement to break those numbers\naccording to the following grammar:\n\n\n  Numbers = all_less_than_five  ascending  all_less_than_five EOF;\n  all_less_than_five = all_less_than_five  int_less_than_five\n                     | int_less_than_five;\n\n\n\nSo, we should first match all numbers less than five and collect those, than we\nshould match a list of ascending numbers and than list of less than five again.\n\nint_less_than_five\n and \nascending\n are terminals/recognizers that will be\ndefined in Python and passed to grammar construction. \nint_less_than_five\n will\nrecognize Python integer that is less than five. \nascending\n will recognize a\nsublist of integers in ascending order.\n\n\nFor more details on the usage see \nthis test\n.\n\n\nMore on this topic will be written in a separate section.\n\n\n\n\nNote\n\n\nYou can directly write regex or string recognizer at the place of terminal:\n\n\nsome_rule = \"a\" aterm \"a\";\naterm = \"a\";\n\n\n\nWritting \n\"a\"\n in \nsome_rule\n is equivalent to writing terminal reference\n\naterm\n. Rule \naterm\n is terminal definition rule. All occurences of \n\"a\"\n\nas well as \naterm\n will result in the same terminal in the grammar.\n\n\n\n\nUsual patterns\n\u00b6\n\n\nOne or more\n\u00b6\n\n\ndocument = sections;\n// sections rule bellow will match one or more section.\nsections = sections section | section;\n\n\n\nIn this example \nsections\n will match one or more \nsection\n. Notice the\nrecursive definition of the rule. You can read this as \nsections\n consist of\nsections and a section at the end or \nsections\n is just a single section\n.\n\n\n\n\nNote\n\n\nPlease note that you could do the same with this rule:\n\n\nsections = section sections | section;\n\n\n\nwhich will give you similar result but the resulting tree will be different.\nFormer example will reduce sections early and than add another section to it,\nthus the tree will be expanding to the left. The later example will collect all\nthe sections and than start reducing from the end, thus building a tree\nexpanding to the right. These are subtle differences that are important when you\nstart writing your semantic actions. Most of the time you don't care about this\nso use the first version as it is slightly efficient and parglare provides\ncommon actions for these common cases.\n\n\n\n\nZero or more\n\u00b6\n\n\ndocument = sections;\n// sections rule bellow will match zero or more section.\nsections = sections section | section | EMPTY;\n\n\n\nIn this example \nsections\n will match zero or more \nsection\n. Notice the\naddition of the \nEMPTY\n choice at the end. This means that matching nothing is a\nvalid \nsections\n non-terminal.\n\n\nSame note from above applies here to.\n\n\nOptional\n\u00b6\n\n\ndocument = optheader body;\noptheader = header | EMPTY;\n\n\n\nIn this example \noptheader\n is either a header or nothing.\n\n\nGrammar comments\n\u00b6\n\n\nIn grammar comments are available as both line comments and block comments:\n\n\n// This is a line comment. Everything from the '//' to the end of line is a comment.\n\n/*\n  This is a block comment.\n  Everything in between `/*`  and '*/' is a comment.\n*/\n\n\n\nHandling whitespaces and comments\n\u00b6\n\n\nBy default parser will skip whitespaces. Whitespace skipping is controlled by\n\nws\n parameter to the parser which is by default set to \n'\\n\\t '\n.\n\n\nIf you need more control of the layout, i.e. handling of not only whitespaces by\ncomments also, you can use a special rule \nLAYOUT\n:\n\n\n  LAYOUT = LayoutItem | LAYOUT LayoutItem;\n  LayoutItem = WS | Comment | EMPTY;\n  WS = /\\s+/;\n  Comment = /\\/\\/.*/;\n\n\n\nThis will form a separate layout parser that will parse in-between each matched\ntokens. In this example spaces and line-comments will get consumed by the layout\nparser.\n\n\nIf this special rule is found in the grammar \nws\n parser parameter is ignored.\n\n\nAnother example that gives support for both line comments and block comments\nlike the one used in the grammar language itself:\n\n\n  LAYOUT = LayoutItem | LAYOUT LayoutItem;\n  LayoutItem = WS | Comment | EMPTY;\n  WS = /\\s+/;\n  Comment = '/*' CorNCs '*/' | /\\/\\/.*/;\n  CorNCs = CorNC | CorNCs CorNC | EMPTY;\n  CorNC = Comment | NotComment | WS;\n  NotComment = /((\\*[^\\/])|[^\\s*\\/]|\\/[^\\*])+/;",
            "title": "Grammar"
        },
        {
            "location": "/grammar/#the-parglare-grammar-language",
            "text": "parglare grammar specification language is based\non  BNF . parglare is\nbased\non Context-Free Grammars (CFGs)  and\ngrammar is given declaratively. You don't have to think about the parsing\nprocess like in\ne.g.  PEGs .\nAmbiguities are dealt with explicitely (see section on conflicts...).  Grammar consists of a set of derivation rules where each rule is of the form:  <symbol> = <expression> ;  where  <symbol>  is grammar non-terminal and  <expression>  is a sequence of\nterminals and non-terminals separated by choice operator  | .  For example:  Fields = Field | Fields \",\" Field;  Here  Fields  is a non-terminal grammar symbol and it is defined as either a\nsingle  Field  or, recursively, as  Fields  followed by a string terminal  , \nand than by another  Field . It is not given here but  Field  could also be\ndefined as a non-terminal. For example:  Field = QuotedField | FieldContent;  Or it could be defined as a terminal:  Field = /[A-Z]*/;  This terminal definition uses regular expression recognizer.   Note  If you got use to various BNF extensions\n(like  Kleene star ) you might find\nthis awkward because you must build  zero or more  or  one or more  pattern from\nscratch using just a sequence, choice and recursion. The grammars are indeed\nmore verbose but, on the other hand, actions are much easier to write and you\nhave full control over tree construction process. parglare might provide some\nsyntactic sugar later that would make some constructs shorter to write.",
            "title": "The parglare grammar language"
        },
        {
            "location": "/grammar/#terminals",
            "text": "Terminal symbols of the grammar define the fundamental or atomic elements of\nyour language -- tokens or lexemes (e.g. keywords, numbers). In parglare\nterminal is connected to recognizer which is an object used to recognize token\nof particular type in the input. Most of the time you will do parsing of textual\ncontent and you will need textual recognizers. These recognizers are built-in\nand there are two type of textual recognizers:   string recognizer  regular expression recognizer",
            "title": "Terminals"
        },
        {
            "location": "/grammar/#string-recognizer",
            "text": "String recognizer is defined as a plain string inside of double quotes:  my_rule = \"start\" other_rule \"end\";  In this example  \"start\"  and  \"end\"  will be terminals with string recognizers\nthat match exactly the words  start  and  end .  You can write string recognizing terminal directly in the rule expression or you\ncan define terminal separately and reference it by name, like:  my_rule = start other_rule end;\nstart = \"start\";\nend = \"end\";  Either way it will be the same terminal. You will usually write as a separate\nterminal if the terminal is used at multiple places in the grammar.",
            "title": "String recognizer"
        },
        {
            "location": "/grammar/#regular-expression-recognizer",
            "text": "Or regex recognizer for short is a regex pattern written inside slashes\n( /.../ ).  For example:   number = /\\d+/;  This rule defines terminal symbol  number  which has a regex recognizer and will\nrecognize one or more digits as a number.",
            "title": "Regular expression recognizer"
        },
        {
            "location": "/grammar/#custom-recognizers",
            "text": "If you are parsing arbitrary input (non-textual) you'll have to provide your own\nrecognizers. In the grammar, you just have to reference your terminal symbol but\nyou don't have to provide the definition. You will provide missing recognizers\nduring grammar instantiation from Python.  Lets say that we have a list of integers (real list of Python ints, not a text\nwith numbers) and we have some weird requirement to break those numbers\naccording to the following grammar:    Numbers = all_less_than_five  ascending  all_less_than_five EOF;\n  all_less_than_five = all_less_than_five  int_less_than_five\n                     | int_less_than_five;  So, we should first match all numbers less than five and collect those, than we\nshould match a list of ascending numbers and than list of less than five again. int_less_than_five  and  ascending  are terminals/recognizers that will be\ndefined in Python and passed to grammar construction.  int_less_than_five  will\nrecognize Python integer that is less than five.  ascending  will recognize a\nsublist of integers in ascending order.  For more details on the usage see  this test .  More on this topic will be written in a separate section.   Note  You can directly write regex or string recognizer at the place of terminal:  some_rule = \"a\" aterm \"a\";\naterm = \"a\";  Writting  \"a\"  in  some_rule  is equivalent to writing terminal reference aterm . Rule  aterm  is terminal definition rule. All occurences of  \"a\" \nas well as  aterm  will result in the same terminal in the grammar.",
            "title": "Custom recognizers"
        },
        {
            "location": "/grammar/#usual-patterns",
            "text": "",
            "title": "Usual patterns"
        },
        {
            "location": "/grammar/#one-or-more",
            "text": "document = sections;\n// sections rule bellow will match one or more section.\nsections = sections section | section;  In this example  sections  will match one or more  section . Notice the\nrecursive definition of the rule. You can read this as  sections  consist of\nsections and a section at the end or  sections  is just a single section .   Note  Please note that you could do the same with this rule:  sections = section sections | section;  which will give you similar result but the resulting tree will be different.\nFormer example will reduce sections early and than add another section to it,\nthus the tree will be expanding to the left. The later example will collect all\nthe sections and than start reducing from the end, thus building a tree\nexpanding to the right. These are subtle differences that are important when you\nstart writing your semantic actions. Most of the time you don't care about this\nso use the first version as it is slightly efficient and parglare provides\ncommon actions for these common cases.",
            "title": "One or more"
        },
        {
            "location": "/grammar/#zero-or-more",
            "text": "document = sections;\n// sections rule bellow will match zero or more section.\nsections = sections section | section | EMPTY;  In this example  sections  will match zero or more  section . Notice the\naddition of the  EMPTY  choice at the end. This means that matching nothing is a\nvalid  sections  non-terminal.  Same note from above applies here to.",
            "title": "Zero or more"
        },
        {
            "location": "/grammar/#optional",
            "text": "document = optheader body;\noptheader = header | EMPTY;  In this example  optheader  is either a header or nothing.",
            "title": "Optional"
        },
        {
            "location": "/grammar/#grammar-comments",
            "text": "In grammar comments are available as both line comments and block comments:  // This is a line comment. Everything from the '//' to the end of line is a comment.\n\n/*\n  This is a block comment.\n  Everything in between `/*`  and '*/' is a comment.\n*/",
            "title": "Grammar comments"
        },
        {
            "location": "/grammar/#handling-whitespaces-and-comments",
            "text": "By default parser will skip whitespaces. Whitespace skipping is controlled by ws  parameter to the parser which is by default set to  '\\n\\t ' .  If you need more control of the layout, i.e. handling of not only whitespaces by\ncomments also, you can use a special rule  LAYOUT :    LAYOUT = LayoutItem | LAYOUT LayoutItem;\n  LayoutItem = WS | Comment | EMPTY;\n  WS = /\\s+/;\n  Comment = /\\/\\/.*/;  This will form a separate layout parser that will parse in-between each matched\ntokens. In this example spaces and line-comments will get consumed by the layout\nparser.  If this special rule is found in the grammar  ws  parser parameter is ignored.  Another example that gives support for both line comments and block comments\nlike the one used in the grammar language itself:    LAYOUT = LayoutItem | LAYOUT LayoutItem;\n  LayoutItem = WS | Comment | EMPTY;\n  WS = /\\s+/;\n  Comment = '/*' CorNCs '*/' | /\\/\\/.*/;\n  CorNCs = CorNC | CorNCs CorNC | EMPTY;\n  CorNC = Comment | NotComment | WS;\n  NotComment = /((\\*[^\\/])|[^\\s*\\/]|\\/[^\\*])+/;",
            "title": "Handling whitespaces and comments"
        },
        {
            "location": "/actions/",
            "text": "Actions\n\u00b6\n\n\nActions (a.k.a. \nsemantic actions\n or \nreductions actions\n) are Python callables\n(functions or lambdas mostly) that get called to reduce the recognized pattern\nto some higher concept. E.g. in the calc example actions are called to calculate\nsubexpressions.\n\n\nThere are two consideration to think of:\n\n\n\n\nWhich actions are called?\n\n\nWhen actions are called?\n\n\n\n\nCustom actions and default actions\n\u00b6\n\n\nIf you don't provide actions of your own the default parglare actions will build\nthe parse tree.\n\n\nActions are provided to the parser during parser instantiation as \nactions\n\nparameter which must be a Python dict where the keys are the names of the rules\nfrom the grammar and values are the action callables or a list of callables if\nthe rule has more than one production/choice.\n\n\nLets take a closer look at the quick intro example:\n\n\ngrammar = r\"\"\"\nE = E '+' E  {left, 1}\n  | E '-' E  {left, 1}\n  | E '*' E  {left, 2}\n  | E '/' E  {left, 2}\n  | E '^' E  {right, 3}\n  | '(' E ')'\n  | number;\nnumber = /\\d+(\\.\\d+)?/;\n\"\"\"\n\nactions = {\n    \"E\": [lambda _, nodes: nodes[0] + nodes[2],\n          lambda _, nodes: nodes[0] - nodes[2],\n          lambda _, nodes: nodes[0] * nodes[2],\n          lambda _, nodes: nodes[0] / nodes[2],\n          lambda _, nodes: nodes[0] ** nodes[2],\n          lambda _, nodes: nodes[1],\n          lambda _, nodes: nodes[0]],\n    \"number\": lambda _, value: float(value),\n}\n\ng = Grammar.from_string(grammar)\nparser = Parser(g, actions=actions)\nresult = parser.parse(\"34 + 4.6 / 2 * 4^2^2 + 78\")\n\n\n\nHere you can see that for rule \nE\n we provide a list of lambdas, one lambda for\neach operation. The first element of the list corresponds to the first\nproduction of the \nE\n rule (\nE '+' E {left, 1}\n), the second to the second and\nso on. For \nnumber\n rule there is only a single lambda which converts the\nmatched string to the Python \nfloat\n type, because \nnumber\n has only a single\nproduction. Actually, \nnumber\n is a terminal definition and thus the second\nparameter in action call will not be a list but a matched value itself. At the\nend we instantiate the parser and pass in our \nactions\n using the parameter.\n\n\nEach action callable receive two parameters. The first is the context object\nwhich gives parsing context information (like the start end end position where\nthe match occured, the parser instance etc.). The second parameters \nnodes\n is\nthe actual results of subexpressions given in the order defined in the grammar.\n\n\nFor example:\n\n\nlambda _, nodes: nodes[0] * nodes[2],\n\n\n\nIn this line we don't care about context thus giving it the \n_\n name. \nnodes[0]\n\nwill cary the value of the left subexpression while \nnodes[2]\n will carry the\nresult of the right subexpression. \nnodes[1]\n must be \n*\n and we don't need to\ncheck that as the parser already did that for us.\n\n\nThe result of the parsing will be the evaluated expression as the actions will\nget called along the way and the result of each actions will be used as an\nelement of the \nnodes\n parameter in calling actions higher in the hierarchy.\n\n\nIf we don't provide \nactions\n the default will be used and we end up with the\nparse tree as the result of the parsing.\n\n\nTime of actions call\n\u00b6\n\n\nIn parglare actions can be called during parsing (i.e. on the fly) which you\ncould use if you want to transform input imediately without building the parse\ntree. But there are times when you would like to build a tree first and call\nactions afterwards. For example, you might want to do several different\ntransformation of the tree. Or, another very good reason is, you are using GLR\nand you want to be sure that actions are called only on the final tree.\n\n\n\n\nNote\n\n\nIf you are using GLR be sure to call actions after the tree has been built.\nFirst, there are a number of things you would have to take care otherwise,\nlike common subtree sharing, actions side-effects etc.\n\n\n\n\nTo get the tree and call actions afterwards you don't supply \nactions\n parameter\nto the parser. That way you get the parse tree as a result (or a list of parse\ntrees in case of GLR). After that you call \ncall_actions\n method on the parser\ngiving it the root of the tree and the actions dict:\n\n\nparser = Parser(g)\ntree = parser.parse(\"34 + 4.6 / 2 * 4^2^2 + 78\")\nresult = parser.call_actions(tree, actions=actions)\n\n\n\nThe Context object\n\u00b6\n\n\nThe first parameter passed to the action function is the Context object. This\nobject provides us with the parsing context of where the match occured.\n\n\nThese attributes are available on the context object:\n\n\n\n\n\n\nstart_position\n/\nend_position\n - the beginning and the end in the input\n  stream where the match occured. \nstart_position\n is the location of the first\n  element in the input while the \nend_position\n is one past the last element of\n  the match. Thus \nend_position - start_position\n will give the lenght of the\n  match including the layout. You can use \nparglare.pos_to_line_col(input,\n  position)\n function to get line, column of the position.\n\n\n\n\n\n\nlayout_content\n - is the layout (whitespaces, comments etc.) that are\n  collected from the previous non-layout match.\n\n\n\n\n\n\nsymbol\n - the grammar symbol this match is for.\n\n\n\n\n\n\nproduction\n - an instance of \nparglare.grammar.Production\n class available\n  only on reduction actions (not on shifts). Represents the grammar production.\n\n\n\n\n\n\nnode\n - this is available only if the actions are called over the parse tree\n  using \ncall_actions\n. It represens the instance of \nNodeNonTerm\n or \nNodeTerm\n\n  classes from the parse tree where the actions is executed.\n\n\n\n\n\n\nparser\n - is the reference to the parser instance. You should use this only\n  to investigate parser configuration not to alter its state.\n\n\n\n\n\n\nYou can also use context object to pass information between lower level and\nupper level actions. You can attach any attribute you like, the context object\nis shared between action calls.",
            "title": "Actions"
        },
        {
            "location": "/actions/#actions",
            "text": "Actions (a.k.a.  semantic actions  or  reductions actions ) are Python callables\n(functions or lambdas mostly) that get called to reduce the recognized pattern\nto some higher concept. E.g. in the calc example actions are called to calculate\nsubexpressions.  There are two consideration to think of:   Which actions are called?  When actions are called?",
            "title": "Actions"
        },
        {
            "location": "/actions/#custom-actions-and-default-actions",
            "text": "If you don't provide actions of your own the default parglare actions will build\nthe parse tree.  Actions are provided to the parser during parser instantiation as  actions \nparameter which must be a Python dict where the keys are the names of the rules\nfrom the grammar and values are the action callables or a list of callables if\nthe rule has more than one production/choice.  Lets take a closer look at the quick intro example:  grammar = r\"\"\"\nE = E '+' E  {left, 1}\n  | E '-' E  {left, 1}\n  | E '*' E  {left, 2}\n  | E '/' E  {left, 2}\n  | E '^' E  {right, 3}\n  | '(' E ')'\n  | number;\nnumber = /\\d+(\\.\\d+)?/;\n\"\"\"\n\nactions = {\n    \"E\": [lambda _, nodes: nodes[0] + nodes[2],\n          lambda _, nodes: nodes[0] - nodes[2],\n          lambda _, nodes: nodes[0] * nodes[2],\n          lambda _, nodes: nodes[0] / nodes[2],\n          lambda _, nodes: nodes[0] ** nodes[2],\n          lambda _, nodes: nodes[1],\n          lambda _, nodes: nodes[0]],\n    \"number\": lambda _, value: float(value),\n}\n\ng = Grammar.from_string(grammar)\nparser = Parser(g, actions=actions)\nresult = parser.parse(\"34 + 4.6 / 2 * 4^2^2 + 78\")  Here you can see that for rule  E  we provide a list of lambdas, one lambda for\neach operation. The first element of the list corresponds to the first\nproduction of the  E  rule ( E '+' E {left, 1} ), the second to the second and\nso on. For  number  rule there is only a single lambda which converts the\nmatched string to the Python  float  type, because  number  has only a single\nproduction. Actually,  number  is a terminal definition and thus the second\nparameter in action call will not be a list but a matched value itself. At the\nend we instantiate the parser and pass in our  actions  using the parameter.  Each action callable receive two parameters. The first is the context object\nwhich gives parsing context information (like the start end end position where\nthe match occured, the parser instance etc.). The second parameters  nodes  is\nthe actual results of subexpressions given in the order defined in the grammar.  For example:  lambda _, nodes: nodes[0] * nodes[2],  In this line we don't care about context thus giving it the  _  name.  nodes[0] \nwill cary the value of the left subexpression while  nodes[2]  will carry the\nresult of the right subexpression.  nodes[1]  must be  *  and we don't need to\ncheck that as the parser already did that for us.  The result of the parsing will be the evaluated expression as the actions will\nget called along the way and the result of each actions will be used as an\nelement of the  nodes  parameter in calling actions higher in the hierarchy.  If we don't provide  actions  the default will be used and we end up with the\nparse tree as the result of the parsing.",
            "title": "Custom actions and default actions"
        },
        {
            "location": "/actions/#time-of-actions-call",
            "text": "In parglare actions can be called during parsing (i.e. on the fly) which you\ncould use if you want to transform input imediately without building the parse\ntree. But there are times when you would like to build a tree first and call\nactions afterwards. For example, you might want to do several different\ntransformation of the tree. Or, another very good reason is, you are using GLR\nand you want to be sure that actions are called only on the final tree.   Note  If you are using GLR be sure to call actions after the tree has been built.\nFirst, there are a number of things you would have to take care otherwise,\nlike common subtree sharing, actions side-effects etc.   To get the tree and call actions afterwards you don't supply  actions  parameter\nto the parser. That way you get the parse tree as a result (or a list of parse\ntrees in case of GLR). After that you call  call_actions  method on the parser\ngiving it the root of the tree and the actions dict:  parser = Parser(g)\ntree = parser.parse(\"34 + 4.6 / 2 * 4^2^2 + 78\")\nresult = parser.call_actions(tree, actions=actions)",
            "title": "Time of actions call"
        },
        {
            "location": "/actions/#the-context-object",
            "text": "The first parameter passed to the action function is the Context object. This\nobject provides us with the parsing context of where the match occured.  These attributes are available on the context object:    start_position / end_position  - the beginning and the end in the input\n  stream where the match occured.  start_position  is the location of the first\n  element in the input while the  end_position  is one past the last element of\n  the match. Thus  end_position - start_position  will give the lenght of the\n  match including the layout. You can use  parglare.pos_to_line_col(input,\n  position)  function to get line, column of the position.    layout_content  - is the layout (whitespaces, comments etc.) that are\n  collected from the previous non-layout match.    symbol  - the grammar symbol this match is for.    production  - an instance of  parglare.grammar.Production  class available\n  only on reduction actions (not on shifts). Represents the grammar production.    node  - this is available only if the actions are called over the parse tree\n  using  call_actions . It represens the instance of  NodeNonTerm  or  NodeTerm \n  classes from the parse tree where the actions is executed.    parser  - is the reference to the parser instance. You should use this only\n  to investigate parser configuration not to alter its state.    You can also use context object to pass information between lower level and\nupper level actions. You can attach any attribute you like, the context object\nis shared between action calls.",
            "title": "The Context object"
        },
        {
            "location": "/about/CONTRIBUTING/",
            "text": "Contributing\n\u00b6\n\n\nContributions are welcome, and they are greatly appreciated! Every\nlittle bit helps, and credit will always be given.\n\n\nYou can contribute in many ways:\n\n\nTypes of Contributions\n\u00b6\n\n\nReport Bugs\n\u00b6\n\n\nReport bugs at https://github.com/igordejanovic/parglare/issues.\n\n\nIf you are reporting a bug, please include:\n\n\n\n\nYour operating system name and version.\n\n\nAny details about your local setup that might be helpful in troubleshooting.\n\n\nDetailed steps to reproduce the bug.\n\n\n\n\nFix Bugs\n\u00b6\n\n\nLook through the GitHub issues for bugs. Anything tagged with \"bug\"\nand \"help wanted\" is open to whoever wants to implement it.\n\n\nImplement Features\n\u00b6\n\n\nLook through the GitHub issues for features. Anything tagged with \"enhancement\"\nand \"help wanted\" is open to whoever wants to implement it.\n\n\nWrite Documentation\n\u00b6\n\n\nparglare could always use more documentation, whether as part of the\nofficial parglare docs, in docstrings, or even on the web in blog posts,\narticles, and such.\n\n\nSubmit Feedback\n\u00b6\n\n\nThe best way to send feedback is to file an issue at https://github.com/igordejanovic/parglare/issues.\n\n\nIf you are proposing a feature:\n\n\n\n\nExplain in detail how it would work.\n\n\nKeep the scope as narrow as possible, to make it easier to implement.\n\n\nRemember that this is a volunteer-driven project, and that contributions\n  are welcome :)\n\n\n\n\nGet Started!\n\u00b6\n\n\nReady to contribute? Here's how to set up \nparglare\n for local development.\n\n\n\n\nFork the \nparglare\n repo on GitHub.\n\n\n\n\nClone your fork locally:\n\n\nbash\n  $ git clone git@github.com:your_name_here/parglare.git\n\n\n\n\n\n\nInstall your local copy into a virtualenv. Assuming you have\n   virtualenvwrapper installed, this is how you set up your fork for local\n   development:\n\n\nbash\n$ mkvirtualenv parglare\n$ cd parglare/\n$ python setup.py develop\n\n\n\n\n\n\nCreate a branch for local development::\n\n\nbash\n  $ git checkout -b name-of-your-bugfix-or-feature\n\n\n\n\n\n\nNow you can make your changes locally.\n\n\n\n\n\n\nWhen you're done making changes, check that your changes pass flake8 and the\n   tests, including testing other Python versions with tox:\n\n\nbash\n$ flake8 parglare tests\n$ python setup.py test or py.test\n$ tox\n\n\n\n\n\n\nTo get flake8 and tox, just pip install them into your virtualenv.\n\n\n\n\n\n\nCommit your changes and push your branch to GitHub:\n\n\nbash\n$ git add .\n$ git commit -m \"Your detailed description of your changes.\"\n$ git push origin name-of-your-bugfix-or-feature\n\n\n\n\n\n\nSubmit a pull request through the GitHub website.\n\n\n\n\n\n\nPull Request Guidelines\n\u00b6\n\n\nBefore you submit a pull request, check that it meets these guidelines:\n\n\n\n\nThe pull request should include tests.\n\n\nIf the pull request adds functionality, the docs should be updated. Put\n   your new functionality into a function with a docstring, and add the\n   feature to the list in README.rst.\n\n\nThe pull request should work for Python 2.6, 2.7, 3.3-3.6, and for\n   PyPy. Check https://travis-ci.org/igordejanovic/parglare/pull_requests and\n   make sure that the tests pass for all supported Python versions.\n\n\n\n\nTips\n\u00b6\n\n\nTo run a subset of tests:\n\n\n$ py.test tests.test_parglare",
            "title": "Contributing"
        },
        {
            "location": "/about/CONTRIBUTING/#contributing",
            "text": "Contributions are welcome, and they are greatly appreciated! Every\nlittle bit helps, and credit will always be given.  You can contribute in many ways:",
            "title": "Contributing"
        },
        {
            "location": "/about/CONTRIBUTING/#types-of-contributions",
            "text": "",
            "title": "Types of Contributions"
        },
        {
            "location": "/about/CONTRIBUTING/#report-bugs",
            "text": "Report bugs at https://github.com/igordejanovic/parglare/issues.  If you are reporting a bug, please include:   Your operating system name and version.  Any details about your local setup that might be helpful in troubleshooting.  Detailed steps to reproduce the bug.",
            "title": "Report Bugs"
        },
        {
            "location": "/about/CONTRIBUTING/#fix-bugs",
            "text": "Look through the GitHub issues for bugs. Anything tagged with \"bug\"\nand \"help wanted\" is open to whoever wants to implement it.",
            "title": "Fix Bugs"
        },
        {
            "location": "/about/CONTRIBUTING/#implement-features",
            "text": "Look through the GitHub issues for features. Anything tagged with \"enhancement\"\nand \"help wanted\" is open to whoever wants to implement it.",
            "title": "Implement Features"
        },
        {
            "location": "/about/CONTRIBUTING/#write-documentation",
            "text": "parglare could always use more documentation, whether as part of the\nofficial parglare docs, in docstrings, or even on the web in blog posts,\narticles, and such.",
            "title": "Write Documentation"
        },
        {
            "location": "/about/CONTRIBUTING/#submit-feedback",
            "text": "The best way to send feedback is to file an issue at https://github.com/igordejanovic/parglare/issues.  If you are proposing a feature:   Explain in detail how it would work.  Keep the scope as narrow as possible, to make it easier to implement.  Remember that this is a volunteer-driven project, and that contributions\n  are welcome :)",
            "title": "Submit Feedback"
        },
        {
            "location": "/about/CONTRIBUTING/#get-started",
            "text": "Ready to contribute? Here's how to set up  parglare  for local development.   Fork the  parglare  repo on GitHub.   Clone your fork locally:  bash\n  $ git clone git@github.com:your_name_here/parglare.git    Install your local copy into a virtualenv. Assuming you have\n   virtualenvwrapper installed, this is how you set up your fork for local\n   development:  bash\n$ mkvirtualenv parglare\n$ cd parglare/\n$ python setup.py develop    Create a branch for local development::  bash\n  $ git checkout -b name-of-your-bugfix-or-feature    Now you can make your changes locally.    When you're done making changes, check that your changes pass flake8 and the\n   tests, including testing other Python versions with tox:  bash\n$ flake8 parglare tests\n$ python setup.py test or py.test\n$ tox    To get flake8 and tox, just pip install them into your virtualenv.    Commit your changes and push your branch to GitHub:  bash\n$ git add .\n$ git commit -m \"Your detailed description of your changes.\"\n$ git push origin name-of-your-bugfix-or-feature    Submit a pull request through the GitHub website.",
            "title": "Get Started!"
        },
        {
            "location": "/about/CONTRIBUTING/#pull-request-guidelines",
            "text": "Before you submit a pull request, check that it meets these guidelines:   The pull request should include tests.  If the pull request adds functionality, the docs should be updated. Put\n   your new functionality into a function with a docstring, and add the\n   feature to the list in README.rst.  The pull request should work for Python 2.6, 2.7, 3.3-3.6, and for\n   PyPy. Check https://travis-ci.org/igordejanovic/parglare/pull_requests and\n   make sure that the tests pass for all supported Python versions.",
            "title": "Pull Request Guidelines"
        },
        {
            "location": "/about/CONTRIBUTING/#tips",
            "text": "To run a subset of tests:  $ py.test tests.test_parglare",
            "title": "Tips"
        },
        {
            "location": "/about/LICENSE/",
            "text": "MIT License\n\n\nCopyright (c) 2016-2017, Igor R. Dejanovic\n\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of\nthis software and associated documentation files (the \"Software\"), to deal in\nthe Software without restriction, including without limitation the rights to\nuse, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\nthe Software, and to permit persons to whom the Software is furnished to do so,\nsubject to the following conditions:\n\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\nFOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\nCOPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\nIN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\nCONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.",
            "title": "License"
        }
    ]
}