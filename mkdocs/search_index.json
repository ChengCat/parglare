{
    "docs": [
        {
            "location": "/",
            "text": "A pure Python LR/GLR parser.\n\n\nFeature highlights\n\u00b6\n\n\n\n\n\n\nIntegrated scanner\n\n\nThere is no lexing as a separate phase. There is no separate lexer grammar.\nThe parser will try to reconize token during parsing at the given location.\nThis brings more parsing power as there are no lexical ambiguities\nintroduced by a separate lexing stage. You want variable names in your\nlanguage to be allowed to be like some of the keywords? No problem.\n\n\n\n\n\n\nGeneralized parsing - GLR\n\n\nparglare gives you powerful tools to see where non-determinism in your\ngrammar lies (the notorious shift-reduce and reduce-reduce conflicts) and\ngives detailed info on why that happened. In case your language needs\nnon-deterministic parsing \u2014 either it needs additional lookahead to decide\nor your language is inherently ambiguous \u2014 you can resort to the GLR\nalgorithm by a simple change of the parser class. The grammar stays the\nsame.\n\n\nIn the case of non-determinism (unability for a parser to deterministically\ndecide what to do) the parser will fork and investigate each possibility.\nEventualy, parsers that decided wrong will die leaving only the right one.\nIn case there are multiple interpretation of your input you will get all the\ntrees (a.k.a. \"the parse forest\").\n\n\n\n\n\n\nDeclarative associativity and priority rules\n\n\nThese problems arise a lot when building expression languages. Even a little\narithmetic expression as \n3 + 4 * 5 * 2\n have multiple interpretation\ndepending on the associativity and priority of operations. In parglare it is\neasy to specify these rules in the grammar (see the quick intro bellow\nor\n\nthe calc example\n).\n\n\n\n\n\n\nTracing/debuging, visualization and error reporting\n\n\nThere is an extensive support for grammar checking, debugging, automata\nvisualization, and parse tracing. Check out \npglr command\n.\n\n\n\n\n\n\nParsing arbitrary list of object\n\n\nparglare is not used only to parse the textual content. It can parse (create\na tree) of an arbitrary list of objects (numbers, bytes, whatever) based on\nthe common parglare grammar. For this you have to\ndefine \ntoken recognizers\n for your input stream. The\nbuilt-in recognizers are string and regex recognizers for parsing textual\ninputs. See \nrecognizers\n parameter to grammar construction in\nthe \ntest_recognizers.py test\n.\n\n\n\n\n\n\nFlexible actions calling strategies\n\n\nDuring parsing you will want to do something when the grammar rule matches.\nThe whole point of parsing is that you want to transform your input to some\noutput. There are several options:\n\n\n\n\nby default parser builds nested lists;\n\n\nyou can build a tree using \nbuild_tree=True\n parameter to the parser;\n\n\ncall user supplied actions - you write a Python function that is called\n  when the rule matches. You can do whatever you want at this place and the\n  result returned is used in parent rules/actions. There are some handy\n  build-in actions in the \nparglare.actions\n module.\n\n\nUser actions may be postponed and called on the parse tree - this is handy\n  if you want to process your tree in multiple ways, or you are using GLR\n  parsing and the actions are introducing side-effects and you would like to\n  avoid those effects created from wrong parsers/trees.\n\n\n\n\nBesides calling your actions in-line - during the parsing process - you can\ndecide to build the tree first and call custom actions afterwards. This is a\ngood option if you want to evaluate your tree in a multiple ways or if you\nare using GLR and want to be sure that actions are called only for the\nsurviving tree.\n\n\n\n\n\n\nSupport for whitespaces/comments\n\n\nSupport for language comments/whitespaces is done using the special rule\n\nLAYOUT\n. By default whitespaces are skipped. This is controlled by \nws\n\nparameter to the parser constructor which is by default set to \n\\t\\n\n. If\nset to \nNone\n no whitespace skipping is provided. If there is a rule\n\nLAYOUT\n in the grammar this rule is used instead. An additional parser with\nthe layout grammar will be built to handle whitespaces.\n\n\n\n\n\n\nError recovery\n\n\nThis is something that often lacks in parsing libraries. More often than not\nyou will want your parser to recover from an error, report it, and continue\nparsing. parglare has a built-in error recovery strategy which is currently\na simplistic one -- it will skip current character and try to continue --\nbut gives you possibility to provide your own. You will write a strategy\nthat will either skip input or introduce non-existing but expected tokens.\n\n\n\n\n\n\nTest coverage\n\n\nTest coverage is high and I'll try to keep it that way.\n\n\n\n\n\n\nTODO/Planed\n\u00b6\n\n\n\n\n\n\nTable caching\n\n\nAt the moment parser tables are constructed on-the-fly which might be slow\nfor larger grammars. This is usually a problem only for parsers which need\nto be instantiated before each parse (e.g. when called from CLI). For\nlong-running processes this is not a problem as parser instance can be kept\nand reused. In the future, tables will be recalculated only if the grammar\nhas changed and loaded from cache otherwise.\n\n\n\n\n\n\nGLR performance optimization\n\n\nThe GLR parsing has a lot of overhead compared to LR which makes it slower.\nThere are some technique that could be used to cut on this difference in\nspeed.\n\n\n\n\n\n\nInstall\n\u00b6\n\n\n\n\n\n\nStable version:\n\n\n$ pip install parglare\n\n\n\n\n\n\nDevelopment version:\n\n\n$ git clone git@github.com:igordejanovic/parglare.git\n$ pip install -e parglare\n\n\n\n\n\n\nQuick intro\n\u00b6\n\n\nThis is just a small example to get the general idea. This example shows how to\nparse and evaluate expressions with 5 operations with different priority and\nassociativity. Evaluation is done using semantic/reduction actions.\n\n\nThe whole expression evaluator is done in under 30 lines of code!\n\n\nfrom parglare import Parser, Grammar\n\ngrammar = r\"\"\"\nE: E '+' E  {left, 1}\n | E '-' E  {left, 1}\n | E '*' E  {left, 2}\n | E '/' E  {left, 2}\n | E '^' E  {right, 3}\n | '(' E ')'\n | number;\nnumber: /\\d+(\\.\\d+)?/;\n\"\"\"\n\nactions = {\n    \"E\": [lambda _, nodes: nodes[0] + nodes[2],\n          lambda _, nodes: nodes[0] - nodes[2],\n          lambda _, nodes: nodes[0] * nodes[2],\n          lambda _, nodes: nodes[0] / nodes[2],\n          lambda _, nodes: nodes[0] ** nodes[2],\n          lambda _, nodes: nodes[1],\n          lambda _, nodes: nodes[0]],\n    \"number\": lambda _, value: float(value),\n}\n\ng = Grammar.from_string(grammar)\nparser = Parser(g, debug=True, actions=actions)\n\nresult = parser.parse(\"34 + 4.6 / 2 * 4^2^2 + 78\")\n\nprint(\"Result = \", result)\n\n# Output\n# -- Debuging/tracing output with detailed info about grammar, productions,\n# -- terminals and nonterminals, DFA states, parsing progress,\n# -- and at the end of the output:\n# Result = 700.8\n\n\n\n\n\n\nNote\n\n\nLR tables calculation\n\n\nparglare provides both SLR and LALR tables calculation (LALR is the default).\nLALR is modified to avoid REDUCE/REDUCE conflicts on state merging. Although\nnot proven, this should enable handling of all LR(1) grammars with reduced set\nof states and without conflicts. For grammars that are not LR(1) a GLR parsing\nis provided.\n\n\n\n\nWhat does \nparglare\n mean?\n\u00b6\n\n\nIt is an amalgam of the words \nparser\n and \nglare\n where the second word is\nchosen to contain letters GLR and to be easy for pronunciation. I also like one\nof the translations for the word - \nto be very bright and intense\n\n(by \nThe Free Dictionary\n)\n\n\nOh, and the name is non-generic and unique which make it easy to find on the\nnet. ;)\n\n\nThe project logo is my take (not very successful) on drawing the Hydra, a\ncreature with multiple heads from the Greek mythology. According to the legend,\nthe Hydra had a regeneration feature: for every head chopped off, the Hydra\nwould regrow a couple of new heads. That reminds me a lot of the GLR parsing ;)\n\n\nLicense\n\u00b6\n\n\nMIT\n\n\nPython versions\n\u00b6\n\n\nTested with 2.7, 3.3-3.6",
            "title": "Home"
        },
        {
            "location": "/#feature-highlights",
            "text": "Integrated scanner  There is no lexing as a separate phase. There is no separate lexer grammar.\nThe parser will try to reconize token during parsing at the given location.\nThis brings more parsing power as there are no lexical ambiguities\nintroduced by a separate lexing stage. You want variable names in your\nlanguage to be allowed to be like some of the keywords? No problem.    Generalized parsing - GLR  parglare gives you powerful tools to see where non-determinism in your\ngrammar lies (the notorious shift-reduce and reduce-reduce conflicts) and\ngives detailed info on why that happened. In case your language needs\nnon-deterministic parsing \u2014 either it needs additional lookahead to decide\nor your language is inherently ambiguous \u2014 you can resort to the GLR\nalgorithm by a simple change of the parser class. The grammar stays the\nsame.  In the case of non-determinism (unability for a parser to deterministically\ndecide what to do) the parser will fork and investigate each possibility.\nEventualy, parsers that decided wrong will die leaving only the right one.\nIn case there are multiple interpretation of your input you will get all the\ntrees (a.k.a. \"the parse forest\").    Declarative associativity and priority rules  These problems arise a lot when building expression languages. Even a little\narithmetic expression as  3 + 4 * 5 * 2  have multiple interpretation\ndepending on the associativity and priority of operations. In parglare it is\neasy to specify these rules in the grammar (see the quick intro bellow\nor the calc example ).    Tracing/debuging, visualization and error reporting  There is an extensive support for grammar checking, debugging, automata\nvisualization, and parse tracing. Check out  pglr command .    Parsing arbitrary list of object  parglare is not used only to parse the textual content. It can parse (create\na tree) of an arbitrary list of objects (numbers, bytes, whatever) based on\nthe common parglare grammar. For this you have to\ndefine  token recognizers  for your input stream. The\nbuilt-in recognizers are string and regex recognizers for parsing textual\ninputs. See  recognizers  parameter to grammar construction in\nthe  test_recognizers.py test .    Flexible actions calling strategies  During parsing you will want to do something when the grammar rule matches.\nThe whole point of parsing is that you want to transform your input to some\noutput. There are several options:   by default parser builds nested lists;  you can build a tree using  build_tree=True  parameter to the parser;  call user supplied actions - you write a Python function that is called\n  when the rule matches. You can do whatever you want at this place and the\n  result returned is used in parent rules/actions. There are some handy\n  build-in actions in the  parglare.actions  module.  User actions may be postponed and called on the parse tree - this is handy\n  if you want to process your tree in multiple ways, or you are using GLR\n  parsing and the actions are introducing side-effects and you would like to\n  avoid those effects created from wrong parsers/trees.   Besides calling your actions in-line - during the parsing process - you can\ndecide to build the tree first and call custom actions afterwards. This is a\ngood option if you want to evaluate your tree in a multiple ways or if you\nare using GLR and want to be sure that actions are called only for the\nsurviving tree.    Support for whitespaces/comments  Support for language comments/whitespaces is done using the special rule LAYOUT . By default whitespaces are skipped. This is controlled by  ws \nparameter to the parser constructor which is by default set to  \\t\\n . If\nset to  None  no whitespace skipping is provided. If there is a rule LAYOUT  in the grammar this rule is used instead. An additional parser with\nthe layout grammar will be built to handle whitespaces.    Error recovery  This is something that often lacks in parsing libraries. More often than not\nyou will want your parser to recover from an error, report it, and continue\nparsing. parglare has a built-in error recovery strategy which is currently\na simplistic one -- it will skip current character and try to continue --\nbut gives you possibility to provide your own. You will write a strategy\nthat will either skip input or introduce non-existing but expected tokens.    Test coverage  Test coverage is high and I'll try to keep it that way.",
            "title": "Feature highlights"
        },
        {
            "location": "/#todoplaned",
            "text": "Table caching  At the moment parser tables are constructed on-the-fly which might be slow\nfor larger grammars. This is usually a problem only for parsers which need\nto be instantiated before each parse (e.g. when called from CLI). For\nlong-running processes this is not a problem as parser instance can be kept\nand reused. In the future, tables will be recalculated only if the grammar\nhas changed and loaded from cache otherwise.    GLR performance optimization  The GLR parsing has a lot of overhead compared to LR which makes it slower.\nThere are some technique that could be used to cut on this difference in\nspeed.",
            "title": "TODO/Planed"
        },
        {
            "location": "/#install",
            "text": "Stable version:  $ pip install parglare    Development version:  $ git clone git@github.com:igordejanovic/parglare.git\n$ pip install -e parglare",
            "title": "Install"
        },
        {
            "location": "/#quick-intro",
            "text": "This is just a small example to get the general idea. This example shows how to\nparse and evaluate expressions with 5 operations with different priority and\nassociativity. Evaluation is done using semantic/reduction actions.  The whole expression evaluator is done in under 30 lines of code!  from parglare import Parser, Grammar\n\ngrammar = r\"\"\"\nE: E '+' E  {left, 1}\n | E '-' E  {left, 1}\n | E '*' E  {left, 2}\n | E '/' E  {left, 2}\n | E '^' E  {right, 3}\n | '(' E ')'\n | number;\nnumber: /\\d+(\\.\\d+)?/;\n\"\"\"\n\nactions = {\n    \"E\": [lambda _, nodes: nodes[0] + nodes[2],\n          lambda _, nodes: nodes[0] - nodes[2],\n          lambda _, nodes: nodes[0] * nodes[2],\n          lambda _, nodes: nodes[0] / nodes[2],\n          lambda _, nodes: nodes[0] ** nodes[2],\n          lambda _, nodes: nodes[1],\n          lambda _, nodes: nodes[0]],\n    \"number\": lambda _, value: float(value),\n}\n\ng = Grammar.from_string(grammar)\nparser = Parser(g, debug=True, actions=actions)\n\nresult = parser.parse(\"34 + 4.6 / 2 * 4^2^2 + 78\")\n\nprint(\"Result = \", result)\n\n# Output\n# -- Debuging/tracing output with detailed info about grammar, productions,\n# -- terminals and nonterminals, DFA states, parsing progress,\n# -- and at the end of the output:\n# Result = 700.8   Note  LR tables calculation  parglare provides both SLR and LALR tables calculation (LALR is the default).\nLALR is modified to avoid REDUCE/REDUCE conflicts on state merging. Although\nnot proven, this should enable handling of all LR(1) grammars with reduced set\nof states and without conflicts. For grammars that are not LR(1) a GLR parsing\nis provided.",
            "title": "Quick intro"
        },
        {
            "location": "/#what-does-parglare-mean",
            "text": "It is an amalgam of the words  parser  and  glare  where the second word is\nchosen to contain letters GLR and to be easy for pronunciation. I also like one\nof the translations for the word -  to be very bright and intense \n(by  The Free Dictionary )  Oh, and the name is non-generic and unique which make it easy to find on the\nnet. ;)  The project logo is my take (not very successful) on drawing the Hydra, a\ncreature with multiple heads from the Greek mythology. According to the legend,\nthe Hydra had a regeneration feature: for every head chopped off, the Hydra\nwould regrow a couple of new heads. That reminds me a lot of the GLR parsing ;)",
            "title": "What does parglare mean?"
        },
        {
            "location": "/#license",
            "text": "MIT",
            "title": "License"
        },
        {
            "location": "/#python-versions",
            "text": "Tested with 2.7, 3.3-3.6",
            "title": "Python versions"
        },
        {
            "location": "/getting_started/",
            "text": "Getting started\n\u00b6\n\n\nThe first thing to do is to write your language grammar using\nthe \nparglare grammar language\n. You write the grammar\neither as a Python string in your source code or as a separate file. In case you\nare writing a grammar of a complex language I would suggest the separate file\napproach. Although not mandatory, the convention is that parglare grammar files\nhave \n.pg\n extension.\n\n\nThe next step is to create the instance of the \nGrammar\n class. This is achieved\nby importing the \nGrammar\n class and calling either \nfrom_file\n or \nfrom_str\n\nmethods supplying the file name for the former and the Python string for the\nlater call.\n\n\nfrom parglare import Grammar\n\nfile_name = .....\ngrammar = Grammar.from_file(file_name)\n\n\n\n\nIf there is no errors in the grammar you now have the grammar instance. For more\ninformation see the \nsection about \nGrammar\n class\n.\n\n\n\n\nNote\n\n\nThere is also a handy \npglr command line tool\n that can be\nused for grammar checking, visualization and debugging.\n\n\n\n\nThe next step is to create an instance of the parser. There are two options. If\nyou want to use LR parser instantiate \nParser\n class. For GLR instantiate\n\nGLRParser\n class.\n\n\nfrom parglare import Parser\nparser = Parser(grammar)\n\n\n\n\nor\n\n\nfrom parglare import GLRParser\nparser = GLRParser(grammar)\n\n\n\n\nYou can provide additional \nparser parameters\n during instantiation.\n\n\n\n\nNote\n\n\nLR parser is faster as the GLR machinery brings a significant overhead. So,\nthe general advice is to stick to the LR parsing until you are sure that you\nneed additional power of GLR, i.e. either you need more than one token of\nlookahead or your language is inherently ambiguous. pglr tool will help you in\ninvestigating why you have LR conflicts in your grammar and there are some\nnice \ndisambiguation features\n in parglare\nthat will help you resolve some of those conflicts.\n\n\n\n\nNow parse your input calling \nparse\n method on the parser instance.\n\n\nresult = parser.parse(input_str)\n\n\n\n\nDepending on whether you have configured \nactions\n and what\nparameters you used for parser instance you will\nget either:\n\n\n\n\na nested lists if no actions are used,\n\n\na parse tree if \nbuild_tree\n parser param\n is set to\n  \nTrue\n,\n\n\nsome other representation of your input if custom actions are used.\n\n\n\n\nIn case of the GLR parser you will get a list of all possible results (a.k.a.\n\nthe parse forest\n).\n\n\nWhere to go next?\n\u00b6\n\n\nYou can investigate various topics in the docs.\nThe \nexamples\n\nand\nthe \ntests\n\nare also a good source of information.",
            "title": "Getting started"
        },
        {
            "location": "/getting_started/#getting-started",
            "text": "The first thing to do is to write your language grammar using\nthe  parglare grammar language . You write the grammar\neither as a Python string in your source code or as a separate file. In case you\nare writing a grammar of a complex language I would suggest the separate file\napproach. Although not mandatory, the convention is that parglare grammar files\nhave  .pg  extension.  The next step is to create the instance of the  Grammar  class. This is achieved\nby importing the  Grammar  class and calling either  from_file  or  from_str \nmethods supplying the file name for the former and the Python string for the\nlater call.  from parglare import Grammar\n\nfile_name = .....\ngrammar = Grammar.from_file(file_name)  If there is no errors in the grammar you now have the grammar instance. For more\ninformation see the  section about  Grammar  class .   Note  There is also a handy  pglr command line tool  that can be\nused for grammar checking, visualization and debugging.   The next step is to create an instance of the parser. There are two options. If\nyou want to use LR parser instantiate  Parser  class. For GLR instantiate GLRParser  class.  from parglare import Parser\nparser = Parser(grammar)  or  from parglare import GLRParser\nparser = GLRParser(grammar)  You can provide additional  parser parameters  during instantiation.   Note  LR parser is faster as the GLR machinery brings a significant overhead. So,\nthe general advice is to stick to the LR parsing until you are sure that you\nneed additional power of GLR, i.e. either you need more than one token of\nlookahead or your language is inherently ambiguous. pglr tool will help you in\ninvestigating why you have LR conflicts in your grammar and there are some\nnice  disambiguation features  in parglare\nthat will help you resolve some of those conflicts.   Now parse your input calling  parse  method on the parser instance.  result = parser.parse(input_str)  Depending on whether you have configured  actions  and what\nparameters you used for parser instance you will\nget either:   a nested lists if no actions are used,  a parse tree if  build_tree  parser param  is set to\n   True ,  some other representation of your input if custom actions are used.   In case of the GLR parser you will get a list of all possible results (a.k.a. the parse forest ).",
            "title": "Getting started"
        },
        {
            "location": "/getting_started/#where-to-go-next",
            "text": "You can investigate various topics in the docs.\nThe  examples \nand\nthe  tests \nare also a good source of information.",
            "title": "Where to go next?"
        },
        {
            "location": "/grammar_language/",
            "text": "The parglare grammar language\n\u00b6\n\n\nThe parglare grammar specification language is based\non \nBNF\n\nwith \nsyntactic sugar extensions\n which are\noptional and builds on top of a pure BNF. parglare is based\non\n\nContext-Free Grammars (CFGs)\n and\na grammar is written declaratively. You don't have to think about the parsing\nprocess like in\ne.g. \nPEGs\n.\nAmbiguities are dealt with explicitly (see\nthe \nsection on conflicts\n).\n\n\nGrammar consists of a set of derivation rules where each rule is of the form:\n\n\n<symbol>: <expression> ;\n\n\n\n\nwhere \n<symbol>\n is grammar non-terminal and \n<expression>\n is a sequence of\nterminals and non-terminals separated by choice operator \n|\n.\n\n\nFor example:\n\n\nFields: Field | Fields \",\" Field;\n\n\n\n\nHere \nFields\n is a non-terminal grammar symbol and it is defined as either a\nsingle \nField\n or, recursively, as \nFields\n followed by a string terminal \n,\n\nand than by another \nField\n. It is not given here but \nField\n could also be\ndefined as a non-terminal. For example:\n\n\nField: QuotedField | FieldContent;\n\n\n\n\nOr it could be defined as a terminal:\n\n\nField: /[A-Z]*/;\n\n\n\n\nThis terminal definition uses regular expression recognizer.\n\n\nTerminals\n\u00b6\n\n\nTerminal symbols of the grammar define the fundamental or atomic elements of\nyour language -- tokens or lexemes (e.g. keywords, numbers). In parglare a\nterminal is connected to the recognizer which is an object used to recognize\ntoken of a particular type in the input. Most of the time you will do parsing of\ntextual content and you will need textual recognizers. These recognizers are\nbuilt-in and there are two type of textual recognizers:\n\n\n\n\nstring recognizer\n\n\nregular expression recognizer\n\n\n\n\nString recognizer\n\u00b6\n\n\nString recognizer is defined as a plain string inside of double quotes:\n\n\nmy_rule: \"start\" other_rule \"end\";\n\n\n\nIn this example \n\"start\"\n and \n\"end\"\n will be terminals with string recognizers\nthat match exactly the words \nstart\n and \nend\n.\n\n\nYou can write string recognizing terminal directly in the rule expression or you\ncan define terminal separately and reference it by name, like:\n\n\nmy_rule: start other_rule end;\nstart: \"start\";\nend: \"end\";\n\n\n\nEither way it will be the same terminal. You will usually write it as a separate\nterminal if the terminal is used at multiple places in the grammar.\n\n\nRegular expression recognizer\n\u00b6\n\n\nOr regex recognizer for short is a regex pattern written inside slashes\n(\n/.../\n).\n\n\nFor example:\n\n\n number: /\\d+/;\n\n\n\nThis rule defines terminal symbol \nnumber\n which has a regex recognizer and will\nrecognize one or more digits as a number.\n\n\nCustom recognizers\n\u00b6\n\n\nIf you are parsing arbitrary input (non-textual) you'll have to provide your own\nrecognizers. In the grammar, you just have to provide terminal symbol without\nbody, i.e. without string or regex recognizer. You will provide missing\nrecognizers during grammar instantiation from Python. Although you don't supply\nbody of the terminal you can define \ndisambiguation rules\n\nas usual.\n\n\nLets say that we have a list of integers (real list of Python ints, not a text\nwith numbers) and we have some weird requirement to break those numbers\naccording to the following grammar:\n\n\n  Numbers: all_less_than_five  ascending  all_less_than_five EOF;\n  all_less_than_five: all_less_than_five  int_less_than_five\n                    | int_less_than_five;\n  int_less_than_five: ;  // <--- This terminal has no recognizer in the grammar\n\n\n\nSo, we should first match all numbers less than five and collect those, than we\nshould match a list of ascending numbers and than list of less than five again.\n\nint_less_than_five\n and \nascending\n are terminals/recognizers that will be\ndefined in Python and passed to grammar construction. \nint_less_than_five\n will\nrecognize Python integer that is, well, less than five. \nascending\n will\nrecognize a sublist of integers in ascending order.\n\n\nFor more details on the usage\nsee\n\nthis test\n.\n\n\nMore on this topic can be found in \na separate section\n.\n\n\n\n\nNote\n\n\nYou can directly write regex or string recognizer at the place of terminal:\n\n\nsome_rule: \"a\" aterm \"a\";\naterm: \"a\";\n\n\n\nWritting \n\"a\"\n in \nsome_rule\n is equivalent to writing terminal reference\n\naterm\n. Rule \naterm\n is terminal definition rule. All occurences of \n\"a\"\n\nas well as \naterm\n will result in the same terminal in the grammar.\n\n\n\n\nUsual patterns\n\u00b6\n\n\nThis section explains how some common grammar patterns can be written using just\na plain BNF notation.\n\n\nOne or more\n\u00b6\n\n\n// sections rule bellow will match one or more section.\nsections: sections section | section;\n\n\n\nIn this example \nsections\n will match one or more \nsection\n. Notice the\nrecursive definition of the rule. You can read this as \nsections\n is either a\nsingle section or \nsections\n and a \nsection\n.\n\n\n\n\nNote\n\n\nPlease note that you could do the same with this rule:\n\n\nsections: section sections | section;\n\n\n\nwhich will give you similar result but the resulting tree will be different.\nNotice the recursive reference is now at the and of the first production.\nPrevious example will reduce sections early and than add another section to it,\nthus the tree will be expanding to the left. The example in this note will\ncollect all the sections and than start reducing from the end, thus building a\ntree expanding to the right. These are subtle differences that are important\nwhen you start writing your semantic actions. Most of the time you don't care\nabout this so use the first version as it is more efficient and parglare\nprovides built-in actions for these common cases.\n\n\n\n\nZero or more\n\u00b6\n\n\n// sections rule bellow will match zero or more section.\nsections: sections section | section | EMPTY;\n\n\n\nIn this example \nsections\n will match zero or more \nsection\n. Notice the\naddition of the \nEMPTY\n choice at the end. This means that matching nothing is a\nvalid \nsections\n non-terminal.\n\n\nSame note from above applies here to.\n\n\nOptional\n\u00b6\n\n\ndocument: optheader body;\noptheader: header | EMPTY;\n\n\n\nIn this example \noptheader\n is either a header or nothing.\n\n\nSyntactic sugar - BNF extensions\n\u00b6\n\n\nPrevious section gives the overview of the basic BNF syntax. If you got to use\nvarious BNF extensions\n(like \nKleene star\n) you might find\nwriting patterns in the previous section awkward. Since some of the patterns are\nused frequently in the grammars (zero-or-more, one-or-more etc.) parglare\nprovides syntactic sugar for this common idioms using a well known regular\nexpression syntax.\n\n\nOptional\n\u00b6\n\n\nOptional\n can be specified using \n?\n. For example:\n\n\nS: \"2\" b? \"3\"?;\nb: \"1\";\n\n\n\nHere, after \n2\n we might have terminal \nb\n but it is optional, as well as \n3\n\nthat follows.\n\n\nLets see what the parser will return for various inputs (the \ngrammar\n variable\nis a string holding grammar from above):\n\n\ng = Grammar.from_string(grammar)\np = Parser(g)\n\ninput_str = '2 1 3'\nresult = p.parse(input_str)\nassert result == [\"2\", \"1\", \"3\"]\n\ninput_str = '2 3'\nresult = p.parse(input_str)\nassert result == [\"2\", None, \"3\"]\n\n\n\n\n\nNote\n\n\nSyntax equivalence for \noptional\n operator:\n\n\nS: b?;\nb: \"1\";\n\n\n\nis equivalent to:\n\n\nS: b_opt;\nb_opt: b | EMPTY;\nb: \"1\";\n\n\n\nBehind the scenes parglare will create \nb_opt\n rule.\nAll syntactic sugar additions operate by creating additional rules in the\ngrammar during table construction.\n\n\n\n\nOne or more\n\u00b6\n\n\nOne or more\n match is specified using \n+\n operator. For example:\n\n\nS: \"2\" c+;\nc: \"c\";\n\n\n\nAfter \n2\n we expect to see one or more \nc\n terminals.\n\n\nLets see what the parser will return for various inputs (the \ngrammar\n variable\nis a string holding grammar from above):\n\n\ng = Grammar.from_string(grammar)\np = Parser(g)\n\ninput_str = '2 c c c'\nresult = p.parse(input_str)\nassert result == [\"2\", [\"c\", \"c\", \"c\"]]\n\ninput_str = '2 c'\nresult = p.parse(input_str)\nassert result == [\"2\", [\"c\"]]\n\n\n\nSo the sub-expression on the second position (\nc+\n sub-rule) will by default\nproduce a list of matched \nc\n terminals. If \nc\n is missing\na \nparse error\n will be raised.\n\n\n\n\nNote\n\n\nSyntax equivalence for \none or more\n:\n\n\nS: a+;\na: \"a\";\n\n\n\nis equivalent to:\n\n\nS: a_1;\n@collect\na_1: a_1 a | a;\na: \"a\";\n\n\n\n\n\n+\n operator allows repetition modifier for separators. For example:\n\n\nS: \"2\" c+[comma];\nc: \"c\";\ncomma: \",\";\n\n\n\nc+[comma]\n will match one or more \nc\n terminals separated by whatever is\nmatched by the \ncomma\n rule.\n\n\nLets see what the parser will return for various inputs (the \ngrammar\n variable\nis a string holding grammar from above):\n\n\ng = Grammar.from_string(grammar)\np = Parser(g)\n\ninput_str = '2 c, c,  c'\nresult = p.parse(input_str)\nassert result == [\"2\", [\"c\", \"c\", \"c\"]]\n\ninput_str = '2 c'\nresult = p.parse(input_str)\nassert result == [\"2\", [\"c\"]]\n\n\n\nAs you can see giving a separator modifier allows us to parse a list of items\nseparated by the whatever is matched by the rule given inside \n[]\n.\n\n\n\n\nNote\n\n\nSyntax equivalence \none or more with separator\n:\n\n\nS: a+[comma];\na: \"a\";\ncomma: \",\";\n\n\n\nis equivalent to:\n\n\nS: a_1_comma;\n@collect_sep\na_1_comma: a_1_comma comma a | a;\na: \"a\";\ncomma: \",\";\n\n\n\nMaking the name of the separator rule a suffix of the additional rule\nname makes sure that only one additional rule will be added to the\ngrammar for all instances of \na+[comma]\n, i.e. same base rule with the\nsame separator.\n\n\n\n\nZero or more\n\u00b6\n\n\nZero or more\n match is specified using \n*\n operator. For example:\n\n\nS: \"2\" c*;\nc: \"c\";\n\n\n\nThis syntactic addition is similar to \n+\n except that it doesn't require rule to\nmatch at least once. If there is no match, resulting sub-expression will be an\nempty list. For example:\n\n\ng = Grammar.from_string(grammar)\np = Parser(g)\n\ninput_str = '2 c c c'\nresult = p.parse(input_str)\nassert result == [\"2\", [\"c\", \"c\", \"c\"]]\n\ninput_str = '2'\nresult = p.parse(input_str)\nassert result == [\"2\", []]\n\n\n\n\n\nNote\n\n\nSyntax equivalence \nzero of more\n:\n\n\nS: a*;\na: \"a\";\n\n\n\nis equivalent to:\n\n\nS: a_0;\na_0: a_1 | EMPTY;\n@collect\na_1: a_1 a | a;\na: \"a\";\n\n\n\nSo using of \n*\n creates both \na_0\n and \na_1\n rules.\nAction attached to \na_0\n returns a list of matched \na\n and empty list\nif no match is found.\n\n\n\n\nSame as \none or more\n this operator may use separator modifiers.\n\n\n\n\nNote\n\n\nSyntax equivalence \nzero or more with separator\n:\n\n\nS: a*[comma];\na: \"a\";\ncomma: \",\";\n\n\n\nis equivalent to:\n\n\nS: a_0_comma;\na_0_comma: a_1_comma | EMPTY;\n@collect_sep\na_1_comma: a_1_comma comma a | a;\na: \"a\";\n\n\n\nwhere action is attached to \na_0_comma\n to provide returning a list of\nmatched \na\n and empty list if no match is found.\n\n\n\n\nNamed matches (\nassignments\n)\n\u00b6\n\n\nIn section on \nactions\n you can see that semantic action (Python\ncallable) connected to a rule will be called with two parameters: a context and\na list of sub-expressions evaluation results. This require you to use positional\naccess in the list of sub-expressions.\n\n\nNamed matches\n (a.k.a \nassignments\n) enable giving a name to the sub-expression\ndirectly in the grammar.\n\n\nFor example:\n\n\nS: first=a second=digit+[comma];\na: \"a\";\ndigit: /\\d+/;\n\n\n\nIn this example root rule matches one \na\n and then one or more digit separated\nby a comma. You can see that the first sub-expression (\na\n match) is assigned to\n\nfirst\n while the second sub-expression \ndigit+[comma]\n is assigned to \nsecond\n.\n\n\nfirst\n and \nsecond\n will now be an additional keyword parameters passed to the\nsemantic action. The values passed in using these parameters will be the results\nof evaluation of the rules referenced by the assignments.\n\n\nThere are two kind of assignments:\n\n\n\n\nplain assignment (\n=\n) -- will collect RHS and pass it to the action under the\n  names given by LHS,\n\n\nbool assignment (\n?=\n) -- will pass \nTrue\n if the match return non-empty\n  result. If the result of RHS is empty the assignment will result in \nFalse\n\n  being passed to the action.\n\n\n\n\nEach rule using named matches result in a dynamically created Python class named\nafter the rule. These classes are kept in a dictionary \ngrammar.classes\n and\nused to instantiate Python objects during parsing by an implicitly\nset \nbuilt-in \nobj\n action\n.\n\n\nThus, for rules using named matches, default action is to create object with\nattributes whose names are those of LHS of the assignments and values are from\nRHS of the assignments (or boolean values for \nbool\n assignments). Each object\nis an instance of corresponding dynamically created Python class.\n\n\nEffectively, using named matches enables automatic creation of a nice AST.\n\n\n\n\nNote\n\n\nYou can, of course, override default action either in the grammar\nusing \n@\n syntax or using rule name in \nactions\n dict given to the parser.\nSee the next section.\n\n\n\n\nReferencing semantic actions from a grammar\n\u00b6\n\n\nBy default \naction\n with the name same as the rule name will be\nsearched in the \nactions\n dict\n. You can override this by\nspecifying action name for the rule directly in the grammar using \n@\n syntax.\n\n\nFor example:\n\n\n@myaction\nsome_rule: first second;\n\n\n\n\nFor rule \nsome_rule\n action with the name \nmyaction\n will be searched in the\n\nactions\n dict or \nbuilt-in actions\n provided by the\n\nparglare.actions\n module. This is helpful if you have some common action that\ncan be used for multiple rules in your grammar. Also this can be used to specify\nbuilt-in action to be used for a rule directly in the grammar.\n\n\nGrammar comments\n\u00b6\n\n\nIn parglare grammar, comments are available as both line comments and block\ncomments:\n\n\n// This is a line comment. Everything from the '//' to the end of line is a comment.\n\n/*\n  This is a block comment.\n  Everything in between `/*`  and '*/' is a comment.\n*/\n\n\n\nHandling whitespaces and comments in your language\n\u00b6\n\n\nBy default parser will skip whitespaces. Whitespace skipping is controlled\nby \nws\n parameter to the parser\n which is by default set to\n\n'\\n\\t '\n.\n\n\nIf you need more control of the layout, i.e. handling of not only whitespaces\nbut comments also, you can use a special rule \nLAYOUT\n:\n\n\n  LAYOUT: LayoutItem | LAYOUT LayoutItem;\n  LayoutItem: WS | Comment | EMPTY;\n  WS: /\\s+/;\n  Comment: /\\/\\/.*/;\n\n\n\nThis will form a separate layout parser that will parse in-between each matched\ntokens. In this example whitespaces and line-comments will be consumed by the\nlayout parser.\n\n\nIf this special rule is found in the grammar \nws\n parser parameter is ignored.\n\n\nHere is another example that gives support for both line comments and block\ncomments like the one used in the grammar language itself:\n\n\n  LAYOUT: LayoutItem | LAYOUT LayoutItem;\n  LayoutItem: WS | Comment | EMPTY;\n  WS: /\\s+/;\n  Comment: '/*' CorNCs '*/' | /\\/\\/.*/;\n  CorNCs: CorNC | CorNCs CorNC | EMPTY;\n  CorNC: Comment | NotComment | WS;\n  NotComment: /((\\*[^\\/])|[^\\s*\\/]|\\/[^\\*])+/;\n\n\n\nHandling keywords in your language\n\u00b6\n\n\nBy default parser will match given string recognizer even if it is part of some\nlarger word, i.e. it will not require matching on the word boundary. This is not\nthe desired behaviour for language keywords.\n\n\nFor example, lets take this little grammar:\n\n\nS: \"for\" name=ID \"=\" from=INT \"to\" to=INT EOF;\nID: /\\w+/;\nINT: /\\d+/;\n\n\n\nThis grammar is intended to match statement like this one:\n\n\nfor a=10 to 20\n\n\n\nBut it will also match:\n\n\nfora=10 to20\n\n\n\nwhich is not what we wanted.\n\n\nparglare allows the definition of a special terminal rule \nKEYWORD\n. This rule\nmust define a \nregular expression recognizer\n.\nAny string recognizer in the grammar that can be recognized by the \nKEYWORD\n\nrecognizer is treated as a keyword and is changed during grammar construction to\nmatch only on word boundary.\n\n\nFor example:\n\n\nS: \"for\" name=ID \"=\" from=INT \"to\" to=INT EOF;\nID: /\\w+/;\nINT: /\\d+/;\nKEYWORD: /\\w+/;\n\n\n\nNow,\n\n\nfora=10 to20\n\n\n\nwill not be recognized as the words \nfor\n and \nto\n are recognized to be keywords\n(they can be matched by the \nKEYWORD\n rule).\n\n\nThis will be parsed correctly:\n\n\nfor a=10 to 20\n\n\n\nAs \n=\n is not matched by the \nKEYWORD\n rule and thus doesn't require to be\nseparated from the surrounding tokens.\n\n\n\n\nNote\n\n\nparglare uses integrated scanner so this example:\n\n\nfor for=10 to 20\n\n\n\nwill be correctly parsed. \nfor\n in \nfor=10\n will be recognized as \nID\n and\nnot as a keyword \nfor\n, i.e. there is no lexical ambiguity due to tokenizer\nseparation.",
            "title": "Grammar language"
        },
        {
            "location": "/grammar_language/#the-parglare-grammar-language",
            "text": "The parglare grammar specification language is based\non  BNF \nwith  syntactic sugar extensions  which are\noptional and builds on top of a pure BNF. parglare is based\non Context-Free Grammars (CFGs)  and\na grammar is written declaratively. You don't have to think about the parsing\nprocess like in\ne.g.  PEGs .\nAmbiguities are dealt with explicitly (see\nthe  section on conflicts ).  Grammar consists of a set of derivation rules where each rule is of the form:  <symbol>: <expression> ;  where  <symbol>  is grammar non-terminal and  <expression>  is a sequence of\nterminals and non-terminals separated by choice operator  | .  For example:  Fields: Field | Fields \",\" Field;  Here  Fields  is a non-terminal grammar symbol and it is defined as either a\nsingle  Field  or, recursively, as  Fields  followed by a string terminal  , \nand than by another  Field . It is not given here but  Field  could also be\ndefined as a non-terminal. For example:  Field: QuotedField | FieldContent;  Or it could be defined as a terminal:  Field: /[A-Z]*/;  This terminal definition uses regular expression recognizer.",
            "title": "The parglare grammar language"
        },
        {
            "location": "/grammar_language/#terminals",
            "text": "Terminal symbols of the grammar define the fundamental or atomic elements of\nyour language -- tokens or lexemes (e.g. keywords, numbers). In parglare a\nterminal is connected to the recognizer which is an object used to recognize\ntoken of a particular type in the input. Most of the time you will do parsing of\ntextual content and you will need textual recognizers. These recognizers are\nbuilt-in and there are two type of textual recognizers:   string recognizer  regular expression recognizer",
            "title": "Terminals"
        },
        {
            "location": "/grammar_language/#string-recognizer",
            "text": "String recognizer is defined as a plain string inside of double quotes:  my_rule: \"start\" other_rule \"end\";  In this example  \"start\"  and  \"end\"  will be terminals with string recognizers\nthat match exactly the words  start  and  end .  You can write string recognizing terminal directly in the rule expression or you\ncan define terminal separately and reference it by name, like:  my_rule: start other_rule end;\nstart: \"start\";\nend: \"end\";  Either way it will be the same terminal. You will usually write it as a separate\nterminal if the terminal is used at multiple places in the grammar.",
            "title": "String recognizer"
        },
        {
            "location": "/grammar_language/#regular-expression-recognizer",
            "text": "Or regex recognizer for short is a regex pattern written inside slashes\n( /.../ ).  For example:   number: /\\d+/;  This rule defines terminal symbol  number  which has a regex recognizer and will\nrecognize one or more digits as a number.",
            "title": "Regular expression recognizer"
        },
        {
            "location": "/grammar_language/#custom-recognizers",
            "text": "If you are parsing arbitrary input (non-textual) you'll have to provide your own\nrecognizers. In the grammar, you just have to provide terminal symbol without\nbody, i.e. without string or regex recognizer. You will provide missing\nrecognizers during grammar instantiation from Python. Although you don't supply\nbody of the terminal you can define  disambiguation rules \nas usual.  Lets say that we have a list of integers (real list of Python ints, not a text\nwith numbers) and we have some weird requirement to break those numbers\naccording to the following grammar:    Numbers: all_less_than_five  ascending  all_less_than_five EOF;\n  all_less_than_five: all_less_than_five  int_less_than_five\n                    | int_less_than_five;\n  int_less_than_five: ;  // <--- This terminal has no recognizer in the grammar  So, we should first match all numbers less than five and collect those, than we\nshould match a list of ascending numbers and than list of less than five again. int_less_than_five  and  ascending  are terminals/recognizers that will be\ndefined in Python and passed to grammar construction.  int_less_than_five  will\nrecognize Python integer that is, well, less than five.  ascending  will\nrecognize a sublist of integers in ascending order.  For more details on the usage\nsee this test .  More on this topic can be found in  a separate section .   Note  You can directly write regex or string recognizer at the place of terminal:  some_rule: \"a\" aterm \"a\";\naterm: \"a\";  Writting  \"a\"  in  some_rule  is equivalent to writing terminal reference aterm . Rule  aterm  is terminal definition rule. All occurences of  \"a\" \nas well as  aterm  will result in the same terminal in the grammar.",
            "title": "Custom recognizers"
        },
        {
            "location": "/grammar_language/#usual-patterns",
            "text": "This section explains how some common grammar patterns can be written using just\na plain BNF notation.",
            "title": "Usual patterns"
        },
        {
            "location": "/grammar_language/#one-or-more",
            "text": "// sections rule bellow will match one or more section.\nsections: sections section | section;  In this example  sections  will match one or more  section . Notice the\nrecursive definition of the rule. You can read this as  sections  is either a\nsingle section or  sections  and a  section .   Note  Please note that you could do the same with this rule:  sections: section sections | section;  which will give you similar result but the resulting tree will be different.\nNotice the recursive reference is now at the and of the first production.\nPrevious example will reduce sections early and than add another section to it,\nthus the tree will be expanding to the left. The example in this note will\ncollect all the sections and than start reducing from the end, thus building a\ntree expanding to the right. These are subtle differences that are important\nwhen you start writing your semantic actions. Most of the time you don't care\nabout this so use the first version as it is more efficient and parglare\nprovides built-in actions for these common cases.",
            "title": "One or more"
        },
        {
            "location": "/grammar_language/#zero-or-more",
            "text": "// sections rule bellow will match zero or more section.\nsections: sections section | section | EMPTY;  In this example  sections  will match zero or more  section . Notice the\naddition of the  EMPTY  choice at the end. This means that matching nothing is a\nvalid  sections  non-terminal.  Same note from above applies here to.",
            "title": "Zero or more"
        },
        {
            "location": "/grammar_language/#optional",
            "text": "document: optheader body;\noptheader: header | EMPTY;  In this example  optheader  is either a header or nothing.",
            "title": "Optional"
        },
        {
            "location": "/grammar_language/#syntactic-sugar-bnf-extensions",
            "text": "Previous section gives the overview of the basic BNF syntax. If you got to use\nvarious BNF extensions\n(like  Kleene star ) you might find\nwriting patterns in the previous section awkward. Since some of the patterns are\nused frequently in the grammars (zero-or-more, one-or-more etc.) parglare\nprovides syntactic sugar for this common idioms using a well known regular\nexpression syntax.",
            "title": "Syntactic sugar - BNF extensions"
        },
        {
            "location": "/grammar_language/#optional_1",
            "text": "Optional  can be specified using  ? . For example:  S: \"2\" b? \"3\"?;\nb: \"1\";  Here, after  2  we might have terminal  b  but it is optional, as well as  3 \nthat follows.  Lets see what the parser will return for various inputs (the  grammar  variable\nis a string holding grammar from above):  g = Grammar.from_string(grammar)\np = Parser(g)\n\ninput_str = '2 1 3'\nresult = p.parse(input_str)\nassert result == [\"2\", \"1\", \"3\"]\n\ninput_str = '2 3'\nresult = p.parse(input_str)\nassert result == [\"2\", None, \"3\"]   Note  Syntax equivalence for  optional  operator:  S: b?;\nb: \"1\";  is equivalent to:  S: b_opt;\nb_opt: b | EMPTY;\nb: \"1\";  Behind the scenes parglare will create  b_opt  rule.\nAll syntactic sugar additions operate by creating additional rules in the\ngrammar during table construction.",
            "title": "Optional"
        },
        {
            "location": "/grammar_language/#one-or-more_1",
            "text": "One or more  match is specified using  +  operator. For example:  S: \"2\" c+;\nc: \"c\";  After  2  we expect to see one or more  c  terminals.  Lets see what the parser will return for various inputs (the  grammar  variable\nis a string holding grammar from above):  g = Grammar.from_string(grammar)\np = Parser(g)\n\ninput_str = '2 c c c'\nresult = p.parse(input_str)\nassert result == [\"2\", [\"c\", \"c\", \"c\"]]\n\ninput_str = '2 c'\nresult = p.parse(input_str)\nassert result == [\"2\", [\"c\"]]  So the sub-expression on the second position ( c+  sub-rule) will by default\nproduce a list of matched  c  terminals. If  c  is missing\na  parse error  will be raised.   Note  Syntax equivalence for  one or more :  S: a+;\na: \"a\";  is equivalent to:  S: a_1;\n@collect\na_1: a_1 a | a;\na: \"a\";   +  operator allows repetition modifier for separators. For example:  S: \"2\" c+[comma];\nc: \"c\";\ncomma: \",\";  c+[comma]  will match one or more  c  terminals separated by whatever is\nmatched by the  comma  rule.  Lets see what the parser will return for various inputs (the  grammar  variable\nis a string holding grammar from above):  g = Grammar.from_string(grammar)\np = Parser(g)\n\ninput_str = '2 c, c,  c'\nresult = p.parse(input_str)\nassert result == [\"2\", [\"c\", \"c\", \"c\"]]\n\ninput_str = '2 c'\nresult = p.parse(input_str)\nassert result == [\"2\", [\"c\"]]  As you can see giving a separator modifier allows us to parse a list of items\nseparated by the whatever is matched by the rule given inside  [] .   Note  Syntax equivalence  one or more with separator :  S: a+[comma];\na: \"a\";\ncomma: \",\";  is equivalent to:  S: a_1_comma;\n@collect_sep\na_1_comma: a_1_comma comma a | a;\na: \"a\";\ncomma: \",\";  Making the name of the separator rule a suffix of the additional rule\nname makes sure that only one additional rule will be added to the\ngrammar for all instances of  a+[comma] , i.e. same base rule with the\nsame separator.",
            "title": "One or more"
        },
        {
            "location": "/grammar_language/#zero-or-more_1",
            "text": "Zero or more  match is specified using  *  operator. For example:  S: \"2\" c*;\nc: \"c\";  This syntactic addition is similar to  +  except that it doesn't require rule to\nmatch at least once. If there is no match, resulting sub-expression will be an\nempty list. For example:  g = Grammar.from_string(grammar)\np = Parser(g)\n\ninput_str = '2 c c c'\nresult = p.parse(input_str)\nassert result == [\"2\", [\"c\", \"c\", \"c\"]]\n\ninput_str = '2'\nresult = p.parse(input_str)\nassert result == [\"2\", []]   Note  Syntax equivalence  zero of more :  S: a*;\na: \"a\";  is equivalent to:  S: a_0;\na_0: a_1 | EMPTY;\n@collect\na_1: a_1 a | a;\na: \"a\";  So using of  *  creates both  a_0  and  a_1  rules.\nAction attached to  a_0  returns a list of matched  a  and empty list\nif no match is found.   Same as  one or more  this operator may use separator modifiers.   Note  Syntax equivalence  zero or more with separator :  S: a*[comma];\na: \"a\";\ncomma: \",\";  is equivalent to:  S: a_0_comma;\na_0_comma: a_1_comma | EMPTY;\n@collect_sep\na_1_comma: a_1_comma comma a | a;\na: \"a\";  where action is attached to  a_0_comma  to provide returning a list of\nmatched  a  and empty list if no match is found.",
            "title": "Zero or more"
        },
        {
            "location": "/grammar_language/#named-matches-assignments",
            "text": "In section on  actions  you can see that semantic action (Python\ncallable) connected to a rule will be called with two parameters: a context and\na list of sub-expressions evaluation results. This require you to use positional\naccess in the list of sub-expressions.  Named matches  (a.k.a  assignments ) enable giving a name to the sub-expression\ndirectly in the grammar.  For example:  S: first=a second=digit+[comma];\na: \"a\";\ndigit: /\\d+/;  In this example root rule matches one  a  and then one or more digit separated\nby a comma. You can see that the first sub-expression ( a  match) is assigned to first  while the second sub-expression  digit+[comma]  is assigned to  second .  first  and  second  will now be an additional keyword parameters passed to the\nsemantic action. The values passed in using these parameters will be the results\nof evaluation of the rules referenced by the assignments.  There are two kind of assignments:   plain assignment ( = ) -- will collect RHS and pass it to the action under the\n  names given by LHS,  bool assignment ( ?= ) -- will pass  True  if the match return non-empty\n  result. If the result of RHS is empty the assignment will result in  False \n  being passed to the action.   Each rule using named matches result in a dynamically created Python class named\nafter the rule. These classes are kept in a dictionary  grammar.classes  and\nused to instantiate Python objects during parsing by an implicitly\nset  built-in  obj  action .  Thus, for rules using named matches, default action is to create object with\nattributes whose names are those of LHS of the assignments and values are from\nRHS of the assignments (or boolean values for  bool  assignments). Each object\nis an instance of corresponding dynamically created Python class.  Effectively, using named matches enables automatic creation of a nice AST.   Note  You can, of course, override default action either in the grammar\nusing  @  syntax or using rule name in  actions  dict given to the parser.\nSee the next section.",
            "title": "Named matches (assignments)"
        },
        {
            "location": "/grammar_language/#referencing-semantic-actions-from-a-grammar",
            "text": "By default  action  with the name same as the rule name will be\nsearched in the  actions  dict . You can override this by\nspecifying action name for the rule directly in the grammar using  @  syntax.  For example:  @myaction\nsome_rule: first second;  For rule  some_rule  action with the name  myaction  will be searched in the actions  dict or  built-in actions  provided by the parglare.actions  module. This is helpful if you have some common action that\ncan be used for multiple rules in your grammar. Also this can be used to specify\nbuilt-in action to be used for a rule directly in the grammar.",
            "title": "Referencing semantic actions from a grammar"
        },
        {
            "location": "/grammar_language/#grammar-comments",
            "text": "In parglare grammar, comments are available as both line comments and block\ncomments:  // This is a line comment. Everything from the '//' to the end of line is a comment.\n\n/*\n  This is a block comment.\n  Everything in between `/*`  and '*/' is a comment.\n*/",
            "title": "Grammar comments"
        },
        {
            "location": "/grammar_language/#handling-whitespaces-and-comments-in-your-language",
            "text": "By default parser will skip whitespaces. Whitespace skipping is controlled\nby  ws  parameter to the parser  which is by default set to '\\n\\t ' .  If you need more control of the layout, i.e. handling of not only whitespaces\nbut comments also, you can use a special rule  LAYOUT :    LAYOUT: LayoutItem | LAYOUT LayoutItem;\n  LayoutItem: WS | Comment | EMPTY;\n  WS: /\\s+/;\n  Comment: /\\/\\/.*/;  This will form a separate layout parser that will parse in-between each matched\ntokens. In this example whitespaces and line-comments will be consumed by the\nlayout parser.  If this special rule is found in the grammar  ws  parser parameter is ignored.  Here is another example that gives support for both line comments and block\ncomments like the one used in the grammar language itself:    LAYOUT: LayoutItem | LAYOUT LayoutItem;\n  LayoutItem: WS | Comment | EMPTY;\n  WS: /\\s+/;\n  Comment: '/*' CorNCs '*/' | /\\/\\/.*/;\n  CorNCs: CorNC | CorNCs CorNC | EMPTY;\n  CorNC: Comment | NotComment | WS;\n  NotComment: /((\\*[^\\/])|[^\\s*\\/]|\\/[^\\*])+/;",
            "title": "Handling whitespaces and comments in your language"
        },
        {
            "location": "/grammar_language/#handling-keywords-in-your-language",
            "text": "By default parser will match given string recognizer even if it is part of some\nlarger word, i.e. it will not require matching on the word boundary. This is not\nthe desired behaviour for language keywords.  For example, lets take this little grammar:  S: \"for\" name=ID \"=\" from=INT \"to\" to=INT EOF;\nID: /\\w+/;\nINT: /\\d+/;  This grammar is intended to match statement like this one:  for a=10 to 20  But it will also match:  fora=10 to20  which is not what we wanted.  parglare allows the definition of a special terminal rule  KEYWORD . This rule\nmust define a  regular expression recognizer .\nAny string recognizer in the grammar that can be recognized by the  KEYWORD \nrecognizer is treated as a keyword and is changed during grammar construction to\nmatch only on word boundary.  For example:  S: \"for\" name=ID \"=\" from=INT \"to\" to=INT EOF;\nID: /\\w+/;\nINT: /\\d+/;\nKEYWORD: /\\w+/;  Now,  fora=10 to20  will not be recognized as the words  for  and  to  are recognized to be keywords\n(they can be matched by the  KEYWORD  rule).  This will be parsed correctly:  for a=10 to 20  As  =  is not matched by the  KEYWORD  rule and thus doesn't require to be\nseparated from the surrounding tokens.   Note  parglare uses integrated scanner so this example:  for for=10 to 20  will be correctly parsed.  for  in  for=10  will be recognized as  ID  and\nnot as a keyword  for , i.e. there is no lexical ambiguity due to tokenizer\nseparation.",
            "title": "Handling keywords in your language"
        },
        {
            "location": "/grammar/",
            "text": "The grammar class and related APIs\n\u00b6\n\n\nAfter you write your grammar either as a Python string or as a separate file the\nnext step is to instantiate the grammar object that will be used to create the\nparser.\n\n\nThere are two factory methods defined on the \nGrammar\n class for creating the\n\nGrammar\n instance:\n\n\n\n\nGrammar.from_string(grammar_string)\n - if your grammar is given as a Python\n  string,\n\n\nGrammar.from_file(file_path)\n - if the grammar is given as a separate file.\n\n\n\n\nBoth methods return initialized \nGrammar\n object that is passed as the first and\nthe only mandatory parameter for the \nParser/GLRParser\n constructor.\n\n\nGrammar factory methods additional parameters\n\u00b6\n\n\nBoth methods \nfrom_string\n and \nfrom_file\n accept additional optional\nparameters:\n\n\n\n\n\n\nrecognizers\n - a dict of custom recognizers. These recognizers are mandatory\n  if a non-textual content is being parsed and the grammar terminals don't\n  provide recognizers. See \nrecognizers section\n for more\n  information.\n\n\n\n\n\n\ndebug\n - set to \nTrue\n to put the grammar in debug/trace mode. \nFalse\n by\n  default. See \ndebugging section\n for more information.\n\n\n\n\n\n\ndebug_parse\n - set to \nTrue\n to debug/trace grammar file/string parsing.\n  \nFalse\n by default.\n\n\n\n\n\n\ndebug_colors\n - set to \nTrue\n to enable terminal colors in debug/trace\n  output. \nFalse\n by default.\n\n\n\n\n\n\nre_flags\n - regex flags used for regex recognizers. See Python \nre\n module.\n  By default flags is set to \nre.MULTILINE\n.\n\n\n\n\n\n\nignore_case\n - By default parsing is case sensitive. Set this param to\n  \nTrue\n for case-insensitive parsing.\n\n\n\n\n\n\nGrammar class\n\u00b6\n\n\nAttributes\n\u00b6\n\n\n\n\n\n\nterminals\n - a set of terminals (instances of \nTerminal\n);\n\n\n\n\n\n\nnonterminals\n - a set of non-terminal (instances\n  of \nNonTerminal\n);\n\n\n\n\n\n\nroot_symbol\n - a grammar symbol of the start/root rule. By default this is\n  the first rule in the grammar;\n\n\n\n\n\n\nproductions\n - a list of productions (\nProduction\n\n  instances);\n\n\n\n\n\n\nrecognizers\n - a dict of \nuser supplied recognizers\n\n  keyed by the terminal rule name;\n\n\n\n\n\n\nclasses\n - a dict of Python classes dynamically created for rules\n  using \nnamed matches\n keyed\n  by the rule name.\n\n\n\n\n\n\nMethods\n\u00b6\n\n\n\n\n\n\nprint_debug()\n - prints detailed debug/trace info;\n\n\n\n\n\n\nget_terminal(name)\n - gets the terminal by the given name or \nNone\n if\n  not found;\n\n\n\n\n\n\nget_nonterminal(name)\n - gets the non-terminal by the given name or \nNone\n\n  if not found;\n\n\n\n\n\n\nget_symbol(name)\n - gets either a terminal or non-terminal by the given\n  name or \nNone\n if not found.\n\n\n\n\n\n\nGrammarSymbol class\n\u00b6\n\n\nThis is a base class for \nTerminal\n and \nNonTerminal\n.\n\n\nAttributes\n\u00b6\n\n\n\n\n\n\nname\n - the name of the grammar symbol,\n\n\n\n\n\n\naction_name\n - the action name assigned for the symbol. This is given in\n  the grammar using the \n@\n syntax\n.\n\n\n\n\n\n\naction\n - resolved reference to the action given by the user using\n  \nactions\n parameter of the parser. Overrides grammar action if provided. If\n  not given will be the same as \ngrammar_action\n.\n\n\n\n\n\n\ngrammar_action\n - resolved reference to the action specified in the\n  grammar. Not used if \naction\n attribute is defined, i.e. \naction\n overrides\n  \ngrammar_action\n.\n\n\n\n\n\n\nTerminal class\n\u00b6\n\n\nAttributes\n\u00b6\n\n\n\n\n\n\nprior (int)\n - a priority used in disambiguation,\n\n\n\n\n\n\nrecognizer (callable)\n - a callable in charge of recognition of this terminal\n  in the input stream,\n\n\n\n\n\n\nprefer (bool)\n - If \nTrue\n this recognizer/terminal is preferred in case of\n  conflict where multiple recognizer match at the same place and implicit\n  disambiguation doesn't resolve the conflict.\n\n\n\n\n\n\ndynamic (bool)\n - \nTrue\n if disambiguation should\n  be \nresolved dynamically\n.\n\n\n\n\n\n\nNonTerminal class\n\u00b6\n\n\nOnly inherited from \nGrammarSymbol\n.\n\n\nProduction class\n\u00b6\n\n\nAttributes\n\u00b6\n\n\n\n\n\n\nsymbol (GrammarSymbol)\n - LHS of the production,\n\n\n\n\n\n\nrhs (ProductionRHS)\n - RHS of this production,\n\n\n\n\n\n\nassignments (dict)\n - \nAssignment\n instances keyed by match name. Created\n  by \nnamed matches\n,\n\n\n\n\n\n\nassoc (int)\n - associativity of the production. See\n  \nparglare.grammar.ASSOC_{NONE|LEFT|RIGHT}\n\n\n\n\n\n\nprior (int)\n - integer defining priority of this production. Default\n  priority is 10.\n\n\n\n\n\n\ndynamic (bool)\n - \nTrue\n if this production disambiguation should\n  be \nresolved dynamically\n.\n\n\n\n\n\n\nprod_id (int)\n - ordinal number of the production in the grammar,\n\n\n\n\n\n\nprod_symbol_id\n - zero-based ordinal of the production for the \nsymbol\n\n  grammar symbol, i.e. the ordinal for the alternative choice for this symbol.",
            "title": "Grammar class"
        },
        {
            "location": "/grammar/#the-grammar-class-and-related-apis",
            "text": "After you write your grammar either as a Python string or as a separate file the\nnext step is to instantiate the grammar object that will be used to create the\nparser.  There are two factory methods defined on the  Grammar  class for creating the Grammar  instance:   Grammar.from_string(grammar_string)  - if your grammar is given as a Python\n  string,  Grammar.from_file(file_path)  - if the grammar is given as a separate file.   Both methods return initialized  Grammar  object that is passed as the first and\nthe only mandatory parameter for the  Parser/GLRParser  constructor.",
            "title": "The grammar class and related APIs"
        },
        {
            "location": "/grammar/#grammar-factory-methods-additional-parameters",
            "text": "Both methods  from_string  and  from_file  accept additional optional\nparameters:    recognizers  - a dict of custom recognizers. These recognizers are mandatory\n  if a non-textual content is being parsed and the grammar terminals don't\n  provide recognizers. See  recognizers section  for more\n  information.    debug  - set to  True  to put the grammar in debug/trace mode.  False  by\n  default. See  debugging section  for more information.    debug_parse  - set to  True  to debug/trace grammar file/string parsing.\n   False  by default.    debug_colors  - set to  True  to enable terminal colors in debug/trace\n  output.  False  by default.    re_flags  - regex flags used for regex recognizers. See Python  re  module.\n  By default flags is set to  re.MULTILINE .    ignore_case  - By default parsing is case sensitive. Set this param to\n   True  for case-insensitive parsing.",
            "title": "Grammar factory methods additional parameters"
        },
        {
            "location": "/grammar/#grammar-class",
            "text": "",
            "title": "Grammar class"
        },
        {
            "location": "/grammar/#attributes",
            "text": "terminals  - a set of terminals (instances of  Terminal );    nonterminals  - a set of non-terminal (instances\n  of  NonTerminal );    root_symbol  - a grammar symbol of the start/root rule. By default this is\n  the first rule in the grammar;    productions  - a list of productions ( Production \n  instances);    recognizers  - a dict of  user supplied recognizers \n  keyed by the terminal rule name;    classes  - a dict of Python classes dynamically created for rules\n  using  named matches  keyed\n  by the rule name.",
            "title": "Attributes"
        },
        {
            "location": "/grammar/#methods",
            "text": "print_debug()  - prints detailed debug/trace info;    get_terminal(name)  - gets the terminal by the given name or  None  if\n  not found;    get_nonterminal(name)  - gets the non-terminal by the given name or  None \n  if not found;    get_symbol(name)  - gets either a terminal or non-terminal by the given\n  name or  None  if not found.",
            "title": "Methods"
        },
        {
            "location": "/grammar/#grammarsymbol-class",
            "text": "This is a base class for  Terminal  and  NonTerminal .",
            "title": "GrammarSymbol class"
        },
        {
            "location": "/grammar/#attributes_1",
            "text": "name  - the name of the grammar symbol,    action_name  - the action name assigned for the symbol. This is given in\n  the grammar using the  @  syntax .    action  - resolved reference to the action given by the user using\n   actions  parameter of the parser. Overrides grammar action if provided. If\n  not given will be the same as  grammar_action .    grammar_action  - resolved reference to the action specified in the\n  grammar. Not used if  action  attribute is defined, i.e.  action  overrides\n   grammar_action .",
            "title": "Attributes"
        },
        {
            "location": "/grammar/#terminal-class",
            "text": "",
            "title": "Terminal class"
        },
        {
            "location": "/grammar/#attributes_2",
            "text": "prior (int)  - a priority used in disambiguation,    recognizer (callable)  - a callable in charge of recognition of this terminal\n  in the input stream,    prefer (bool)  - If  True  this recognizer/terminal is preferred in case of\n  conflict where multiple recognizer match at the same place and implicit\n  disambiguation doesn't resolve the conflict.    dynamic (bool)  -  True  if disambiguation should\n  be  resolved dynamically .",
            "title": "Attributes"
        },
        {
            "location": "/grammar/#nonterminal-class",
            "text": "Only inherited from  GrammarSymbol .",
            "title": "NonTerminal class"
        },
        {
            "location": "/grammar/#production-class",
            "text": "",
            "title": "Production class"
        },
        {
            "location": "/grammar/#attributes_3",
            "text": "symbol (GrammarSymbol)  - LHS of the production,    rhs (ProductionRHS)  - RHS of this production,    assignments (dict)  -  Assignment  instances keyed by match name. Created\n  by  named matches ,    assoc (int)  - associativity of the production. See\n   parglare.grammar.ASSOC_{NONE|LEFT|RIGHT}    prior (int)  - integer defining priority of this production. Default\n  priority is 10.    dynamic (bool)  -  True  if this production disambiguation should\n  be  resolved dynamically .    prod_id (int)  - ordinal number of the production in the grammar,    prod_symbol_id  - zero-based ordinal of the production for the  symbol \n  grammar symbol, i.e. the ordinal for the alternative choice for this symbol.",
            "title": "Attributes"
        },
        {
            "location": "/parser/",
            "text": "Parser parameters\n\u00b6\n\n\nThere are several parameters you can pass during the parser construction. The\nmandatory first parameter is the \nGrammar\n instance. Other parameters are\nexplained in the rest of this section.\n\n\nAll parameters described here work for both \nparglare.Parser\n and\n\nparglare.GLRParser\n classes.\n\n\nactions\n\u00b6\n\n\nThis parameters is a dict of \nactions\n keyed by the name of the\ngrammar rule.\n\n\nlayout_actions\n\u00b6\n\n\nThis parameter is used to specify actions called when the rules\nof\n\nlayout sub-grammar\n are\nreduced. This is rarely needed but there are times when you would like to\nprocess matched layout (e.g. whitespaces, comments).\n\n\nIt is given in the same format as \nactions\n parameter, a dict of callables keyed\nby grammar rule names.\n\n\nws\n\u00b6\n\n\nThis parameter specifies a string whose characters are considered to be\nwhitespace. By default its value is \n'\\n\\t '\n. It is used\nif\n\nlayout sub-grammar\n (\nLAYOUT\n\ngrammar rule) is not defined. If \nLAYOUT\n rule is given in the grammar it is\nused instead and this parameter is ignored.\n\n\nbuild_tree\n\u00b6\n\n\nA boolean whose default value is \nFalse\n. If set to \nTrue\n parser will call\nactions that will build the \nparse tree\n.\n\n\nprefer_shifts\n\u00b6\n\n\nBy default set to \nFalse\n. In case of \nshift/reduce conflicts\n\nthis strategy would favor shift over reduce.\n\n\n\n\nNote\n\n\nDo not use \nprefer_shifts\n if you don't understand the implications. Try to\nunderstand \nconflicts\n and\n\nresolution strategies\n.\n\n\n\n\nerror_recovery\n\u00b6\n\n\nBy default set to \nFalse\n. If set to \nTrue\n default error recovery will be used.\nIf set to a Python function, the function will be called to recover from errors.\nFor more information see \nError recovery\n.\n\n\nstart_production\n\u00b6\n\n\nBy default the first rule of the grammar is the start rule. If you want to\nchange this default behavior \u2014 for example you want to create multiple parsers\nfrom the same grammar with different start production \u2014 you can use this\nparameter. The parameter accepts the \nid\n of the grammar production. To get the\n\nid\n from the rule name use the \nget_production_id(rule_name)\n method of the\ngrammar.\n\n\ndebug/debug_layout\n\u00b6\n\n\nThis parameter if set to \nTrue\n will put the parser in debug mode. In this mode\nparser will print a detailed information of its actions to the standard output.\nTo put layout subparser in the debug mode use the \ndebug_layout\n parameter. Both\nparameters are set to \nFalse\n by default.\n\n\nFor more information see \nDebugging\n\n\ndebug_colors\n\u00b6\n\n\nSet this to \nTrue\n to enable terminal colors in debug/trace output. \nFalse\n by\ndefault.\n\n\ntables\n\u00b6\n\n\nThe value of this parameter is either \nparglare.LALR\n or \nparglare.SLR\n and it\nis used to chose the type of LR tables to create. By default \nLALR\n tables are\nused with a slight twist to avoid Reduce/Reduce conflicts that may happen with\npure LALR tables. This parameter should not be used in normal circumstances but\nis provided more for experimentation purposes.\n\n\nparse\n and \nparse_file\n calls\n\u00b6\n\n\nparse\n call is used to parse input string or list of objects. For parsing of\ntextual file \nparse_file\n is used.\n\n\nThese two calls accepts the following parameters:\n\n\n\n\n\n\ninput_str\n - first positional and mandatory parameter only for \nparse\n call -\n  the input string/list of objects.\n\n\n\n\n\n\nposition\n - the start position to parse from. By default 0.\n\n\n\n\n\n\ncontext\n - the \ncontext object\n to use. By\n  default \nNone\n - context object is created by the parser.\n\n\n\n\n\n\nfile_name\n - first positional and mandatory parameter only for \nparse_file\n\n  call - the name/path of the file to parse.\n\n\n\n\n\n\nToken\n\u00b6\n\n\nThis class from \nparglare.parser\n is used to represent lookahead tokens. Token\nis a concrete matched terminal from the input stream.\n\n\nAttributes\n\u00b6\n\n\n\n\n\n\nsymbol\n (\nTerminal\n) - terminal grammar symbol represented by this token,\n\n\n\n\n\n\nvalue\n (\nlist\n or \nstr\n) - matched part of input stream,\n\n\n\n\n\n\nlength\n (\nint\n) - length of matched input.",
            "title": "Parser"
        },
        {
            "location": "/parser/#parser-parameters",
            "text": "There are several parameters you can pass during the parser construction. The\nmandatory first parameter is the  Grammar  instance. Other parameters are\nexplained in the rest of this section.  All parameters described here work for both  parglare.Parser  and parglare.GLRParser  classes.",
            "title": "Parser parameters"
        },
        {
            "location": "/parser/#actions",
            "text": "This parameters is a dict of  actions  keyed by the name of the\ngrammar rule.",
            "title": "actions"
        },
        {
            "location": "/parser/#layout_actions",
            "text": "This parameter is used to specify actions called when the rules\nof layout sub-grammar  are\nreduced. This is rarely needed but there are times when you would like to\nprocess matched layout (e.g. whitespaces, comments).  It is given in the same format as  actions  parameter, a dict of callables keyed\nby grammar rule names.",
            "title": "layout_actions"
        },
        {
            "location": "/parser/#ws",
            "text": "This parameter specifies a string whose characters are considered to be\nwhitespace. By default its value is  '\\n\\t ' . It is used\nif layout sub-grammar  ( LAYOUT \ngrammar rule) is not defined. If  LAYOUT  rule is given in the grammar it is\nused instead and this parameter is ignored.",
            "title": "ws"
        },
        {
            "location": "/parser/#build_tree",
            "text": "A boolean whose default value is  False . If set to  True  parser will call\nactions that will build the  parse tree .",
            "title": "build_tree"
        },
        {
            "location": "/parser/#prefer_shifts",
            "text": "By default set to  False . In case of  shift/reduce conflicts \nthis strategy would favor shift over reduce.   Note  Do not use  prefer_shifts  if you don't understand the implications. Try to\nunderstand  conflicts  and resolution strategies .",
            "title": "prefer_shifts"
        },
        {
            "location": "/parser/#error_recovery",
            "text": "By default set to  False . If set to  True  default error recovery will be used.\nIf set to a Python function, the function will be called to recover from errors.\nFor more information see  Error recovery .",
            "title": "error_recovery"
        },
        {
            "location": "/parser/#start_production",
            "text": "By default the first rule of the grammar is the start rule. If you want to\nchange this default behavior \u2014 for example you want to create multiple parsers\nfrom the same grammar with different start production \u2014 you can use this\nparameter. The parameter accepts the  id  of the grammar production. To get the id  from the rule name use the  get_production_id(rule_name)  method of the\ngrammar.",
            "title": "start_production"
        },
        {
            "location": "/parser/#debugdebug_layout",
            "text": "This parameter if set to  True  will put the parser in debug mode. In this mode\nparser will print a detailed information of its actions to the standard output.\nTo put layout subparser in the debug mode use the  debug_layout  parameter. Both\nparameters are set to  False  by default.  For more information see  Debugging",
            "title": "debug/debug_layout"
        },
        {
            "location": "/parser/#debug_colors",
            "text": "Set this to  True  to enable terminal colors in debug/trace output.  False  by\ndefault.",
            "title": "debug_colors"
        },
        {
            "location": "/parser/#tables",
            "text": "The value of this parameter is either  parglare.LALR  or  parglare.SLR  and it\nis used to chose the type of LR tables to create. By default  LALR  tables are\nused with a slight twist to avoid Reduce/Reduce conflicts that may happen with\npure LALR tables. This parameter should not be used in normal circumstances but\nis provided more for experimentation purposes.",
            "title": "tables"
        },
        {
            "location": "/parser/#parse-and-parse_file-calls",
            "text": "parse  call is used to parse input string or list of objects. For parsing of\ntextual file  parse_file  is used.  These two calls accepts the following parameters:    input_str  - first positional and mandatory parameter only for  parse  call -\n  the input string/list of objects.    position  - the start position to parse from. By default 0.    context  - the  context object  to use. By\n  default  None  - context object is created by the parser.    file_name  - first positional and mandatory parameter only for  parse_file \n  call - the name/path of the file to parse.",
            "title": "parse and parse_file calls"
        },
        {
            "location": "/parser/#token",
            "text": "This class from  parglare.parser  is used to represent lookahead tokens. Token\nis a concrete matched terminal from the input stream.",
            "title": "Token"
        },
        {
            "location": "/parser/#attributes",
            "text": "symbol  ( Terminal ) - terminal grammar symbol represented by this token,    value  ( list  or  str ) - matched part of input stream,    length  ( int ) - length of matched input.",
            "title": "Attributes"
        },
        {
            "location": "/actions/",
            "text": "Actions\n\u00b6\n\n\nActions (a.k.a. \nsemantic actions\n or \nreductions actions\n) are Python callables\n(functions or lambdas mostly) that get called to reduce the recognized pattern\nto some higher concept. E.g. in the calc example actions are called to calculate\nsub-expressions.\n\n\nThere are two consideration to think of:\n\n\n\n\nWhich actions are called?\n\n\nWhen actions are called?\n\n\n\n\nCustom actions and built-in actions\n\u00b6\n\n\nIf you don't provide actions of your own the parser will return nested list\ncorresponding to your grammar. Each non-terminal result in a list of evaluated\nsub-expression while each terminal result in the matched string. If the parser\nparameter \nbuild_tree\n is set to \nTrue\n the parser will\nbuild \na parse tree\n.\n\n\nCustom actions are provided to the parser during parser instantiation as\n\nactions\n parameter which must be a Python dict where the keys are the names of\nthe rules from the grammar and values are the action callables or a list of\ncallables if the rule has more than one production/choice. You can provide\nadditional actions that are not named after the grammar rule names, these\nactions may be referenced from the grammar\nusing\n\n@\n syntax for action specification\n.\n\n\nLets take a closer look at the quick intro example:\n\n\ngrammar = r\"\"\"\nE: E '+' E  {left, 1}\n | E '-' E  {left, 1}\n | E '*' E  {left, 2}\n | E '/' E  {left, 2}\n | E '^' E  {right, 3}\n | '(' E ')'\n | number;\nnumber: /\\d+(\\.\\d+)?/;\n\"\"\"\n\nactions = {\n    \"E\": [lambda _, nodes: nodes[0] + nodes[2],\n          lambda _, nodes: nodes[0] - nodes[2],\n          lambda _, nodes: nodes[0] * nodes[2],\n          lambda _, nodes: nodes[0] / nodes[2],\n          lambda _, nodes: nodes[0] ** nodes[2],\n          lambda _, nodes: nodes[1],\n          lambda _, nodes: nodes[0]],\n    \"number\": lambda _, value: float(value),\n}\n\ng = Grammar.from_string(grammar)\nparser = Parser(g, actions=actions)\nresult = parser.parse(\"34 + 4.6 / 2 * 4^2^2 + 78\")\n\n\n\nHere you can see that for rule \nE\n we provide a list of lambdas, one lambda for\neach operation. The first element of the list corresponds to the first\nproduction of the \nE\n rule (\nE '+' E {left, 1}\n), the second to the second and\nso on. For \nnumber\n rule there is only a single lambda which converts the\nmatched string to the Python \nfloat\n type, because \nnumber\n has only a single\nproduction. Actually, \nnumber\n is a terminal definition and thus the second\nparameter in action call will not be a list but a matched value itself. At the\nend we instantiate the parser and pass in our \nactions\n using the parameter.\n\n\nEach action callable receive two parameters. The first is the context object\nwhich gives \nparsing context information\n (like the start\nand end position where the match occured, the parser instance etc.). The second\nparameters \nnodes\n is a list of actual results of sub-expressions given in the\norder defined in the grammar.\n\n\nFor example:\n\n\nlambda _, nodes: nodes[0] * nodes[2],\n\n\n\nIn this line we don't care about context thus giving it the \n_\n name. \nnodes[0]\n\nwill cary the value of the left sub-expression while \nnodes[2]\n will carry the\nresult of the right sub-expression. \nnodes[1]\n must be \n*\n and we don't need to\ncheck that as the parser already did that for us.\n\n\nThe result of the parsing will be the evaluated expression as the actions will\nget called along the way and the result of each actions will be used as an\nelement of the \nnodes\n parameter in calling actions higher in the hierarchy.\n\n\nIf we don't provide \nactions\n, by default parglare will return a matched string\nfor each terminal and a list of sub-expressions for each non-terminal\neffectively producing nested lists. If we set \nbuild_tree\n parameter of the\nparser to \nTrue\n the parser will produce a \nparse tree\n whose\nelements are instances of \nNodeNonTerm\n and \nNodeTerm\n classes representing a\nnon-terminals and terminals respectively.\n\n\nTime of actions call\n\u00b6\n\n\nIn parglare actions can be called during parsing (i.e. on the fly) which you\ncould use if you want to transform input immediately without building the parse\ntree. But there are times when you would like to build a tree first and call\nactions afterwards. For example, a very good reason is if you are using GLR and\nyou want to be sure that actions are called only on the final tree.\n\n\n\n\nNote\n\n\nIf you are using GLR be sure that your actions has no side-effects, as the\ndying parsers will left those side-effects behind leading to unpredictable\nbehaviour. In case of doubt create trees first, choose the right one and\ncall actions afterwards with the \ncall_actions\n parser method.\n\n\n\n\nTo get the tree and call actions afterwards you supply \nactions\n parameter to\nthe parser as usual and set \nbuild_tree\n to \nTrue\n. When the parser finishes\nsuccessfully it will return the parse tree which you pass to the \ncall_actions\n\nmethod of the parser object to execute actions. For example:\n\n\nparser = Parser(g, actions=actions)\ntree = parser.parse(\"34 + 4.6 / 2 * 4^2^2 + 78\")\nresult = parser.call_actions(tree)\n\n\n\nThe Context object\n\u00b6\n\n\nThe first parameter passed to the action function is the Context object. This\nobject provides us with the parsing context of where the match occurred.\n\n\nFollowing attributes are available on the context object:\n\n\n\n\n\n\nstart_position/end_position\n - the beginning and the end in the input\n  stream where the match occured. \nstart_position\n is the location of the first\n  element/character in the input while the \nend_position\n is one past the last\n  element/character of the match. Thus \nend_position - start_position\n will give\n  the lenght of the match including the layout. You can use\n  \nparglare.pos_to_line_col(input, position)\n function to get line and column of\n  the position. This function returns a tuple \n(line, column)\n.\n\n\n\n\n\n\nfile_name\n - the name/path of the file being parsed. \nNone\n if Python\n  string is parsed.\n\n\n\n\n\n\ninput_str\n - the input string (or list of objects) that is being parsed.\n\n\n\n\n\n\nlayout_content\n - is the layout (whitespaces, comments etc.) that are\n  collected from the previous non-layout match. Default actions for building the\n  parse tree will attach these layouts to the tree nodes.\n\n\n\n\n\n\nsymbol\n - the grammar symbol (instance of either \nTerminal\n or \nNonTerminal\n\n  which inherits \nGrammarSymbol\n) this match is for.\n\n\n\n\n\n\nproduction\n - an instance of \nparglare.grammar.Production\n class available\n  only on reduction actions (not on shifts). Represents the grammar production.\n\n\n\n\n\n\nnode\n - this is available only if the actions are called over the parse tree\n  using \ncall_actions\n. It represens the instance of \nNodeNonTerm\n or \nNodeTerm\n\n  classes from the parse tree where the actions is executed.\n\n\n\n\n\n\nparser\n - is the reference to the parser instance. You should use this only\n  to investigate parser configuration not to alter its state.\n\n\n\n\n\n\nYou can also use context object to pass information between lower level and\nupper level actions. You can attach any attribute you like, the context object\nis shared between action calls. It is shared with the internal layout parser\ntoo.\n\n\nBuilt-in actions\n\u00b6\n\n\nparglare provides some common actions in the module \nparglare.actions\n. You\ncan\n\nreference these actions directly from the grammar\n.\nBuilt-in actions are used implicitly by parglare as default actions in\nparticular case (e.g.\nfor \nsyntactic sugar\n) but\nyou might want to reference some of these actions directly.\n\n\nFollowing are parglare built-in actions from the \nparglare.actions\n module:\n\n\n\n\n\n\npass_none\n - returns \nNone\n;\n\n\n\n\n\n\npass_nochange\n - returns second parameter of action callable (\nvalue\n or\n  \nnodes\n) unchanged;\n\n\n\n\n\n\npass_empty\n - returns an empty list \n[]\n;\n\n\n\n\n\n\npass_single\n - returns \nnodes[0]\n. Used implicitly by rules where all\n  productions have only a single rule reference on the RHS;\n\n\n\n\n\n\npass_inner\n - returns \nnodes[1]\n. Handy to extract sub-expression value for\n  values in parentheses;\n\n\n\n\n\n\ncollect\n - Used for rules of the form \nElements: Elements Element |\n  Element;\n. Implicitly used for \n+\n operator. Returns list;\n\n\n\n\n\n\ncollect_sep\n - Used for rules of the form \nElements: Elements separator\n  Element | Element;\n. Implicitly used for \n+\n with separator. Returns list;\n\n\n\n\n\n\ncollect_optional\n - Can be used for rules of the form \nElements: Elements\n  Element | Element | EMPTY;\n. Returns list;\n\n\n\n\n\n\ncollect_sep_optional\n - Can be used for rules of the form \nElements: Elements\n  separator Element | Element | EMPTY;\n. Returns list;\n\n\n\n\n\n\ncollect_right\n - Can be used for rules of the form \nElements: Element\n  Elements | Element;\n. Returns list;\n\n\n\n\n\n\ncollect_right_sep\n - Can be used for rules of the form \nElements: Element\n  separator Elements | Element;\n. Returns list;\n\n\n\n\n\n\ncollect_right_optional\n - Can be used for rules of the form \nElements:\n  Element Elements | Element | EMPTY;\n. Returns list;\n\n\n\n\n\n\ncollect_right_sep_optional\n - Can be used for rules of the form \nElements:\n  Element separator Elements | Element | EMPTY;\n. Returns list;\n\n\n\n\n\n\noptional\n - Used for rules of the form \nOptionalElement: Element | EMPTY;\n.\n  Implicitly used for \n?\n operator. Returns either a sub-expression value or\n  \nNone\n if empty match.\n\n\n\n\n\n\nobj\n - Used implicitly by rules\n  using \nnamed matches\n.\n  Creates Python object with attributes derived from named matches. Objects\n  created this way have additional attributes\n  \n_pg_start_position\n/\n_pg_end_position\n with start/end position in the input\n  stream where the object is found.\n\n\n\n\n\n\nActions for rules using named matches\n\u00b6\n\n\nIf \nnamed matches\n are used in\nthe grammar rule, action will be called with additional keyword parameters named\nby the name of LHS of rule assignments. If no action is specified for the rule a\nbuilt-in action \nobj\n is called and will produce instance of dynamically created\nPython class corresponding to the grammar rule. See more in the section\non \nnamed matches\n.\n\n\nIf for some reason you want to override default behavior that create Python\nobject you can create action like this:\n\n\nS: first=a second=digit+[comma];\na: \"a\";\ndigit: /\\d+/;\n\n\n\nnow create action function that accepts additional params:\n\n\ndef s_action(context, nodes, first, second):\n   ... do some transformation and return the result of S evaluation\n   ... nodes will contain subexpression results by position while\n   ... first and second will contain the values of corresponding\n   ... sub-expressions\n\n\n\nregister action on \nParser\n instance as usual:\n\n\nparser = Parser(grammar, actions={\"S\": s_action})",
            "title": "Actions"
        },
        {
            "location": "/actions/#actions",
            "text": "Actions (a.k.a.  semantic actions  or  reductions actions ) are Python callables\n(functions or lambdas mostly) that get called to reduce the recognized pattern\nto some higher concept. E.g. in the calc example actions are called to calculate\nsub-expressions.  There are two consideration to think of:   Which actions are called?  When actions are called?",
            "title": "Actions"
        },
        {
            "location": "/actions/#custom-actions-and-built-in-actions",
            "text": "If you don't provide actions of your own the parser will return nested list\ncorresponding to your grammar. Each non-terminal result in a list of evaluated\nsub-expression while each terminal result in the matched string. If the parser\nparameter  build_tree  is set to  True  the parser will\nbuild  a parse tree .  Custom actions are provided to the parser during parser instantiation as actions  parameter which must be a Python dict where the keys are the names of\nthe rules from the grammar and values are the action callables or a list of\ncallables if the rule has more than one production/choice. You can provide\nadditional actions that are not named after the grammar rule names, these\nactions may be referenced from the grammar\nusing @  syntax for action specification .  Lets take a closer look at the quick intro example:  grammar = r\"\"\"\nE: E '+' E  {left, 1}\n | E '-' E  {left, 1}\n | E '*' E  {left, 2}\n | E '/' E  {left, 2}\n | E '^' E  {right, 3}\n | '(' E ')'\n | number;\nnumber: /\\d+(\\.\\d+)?/;\n\"\"\"\n\nactions = {\n    \"E\": [lambda _, nodes: nodes[0] + nodes[2],\n          lambda _, nodes: nodes[0] - nodes[2],\n          lambda _, nodes: nodes[0] * nodes[2],\n          lambda _, nodes: nodes[0] / nodes[2],\n          lambda _, nodes: nodes[0] ** nodes[2],\n          lambda _, nodes: nodes[1],\n          lambda _, nodes: nodes[0]],\n    \"number\": lambda _, value: float(value),\n}\n\ng = Grammar.from_string(grammar)\nparser = Parser(g, actions=actions)\nresult = parser.parse(\"34 + 4.6 / 2 * 4^2^2 + 78\")  Here you can see that for rule  E  we provide a list of lambdas, one lambda for\neach operation. The first element of the list corresponds to the first\nproduction of the  E  rule ( E '+' E {left, 1} ), the second to the second and\nso on. For  number  rule there is only a single lambda which converts the\nmatched string to the Python  float  type, because  number  has only a single\nproduction. Actually,  number  is a terminal definition and thus the second\nparameter in action call will not be a list but a matched value itself. At the\nend we instantiate the parser and pass in our  actions  using the parameter.  Each action callable receive two parameters. The first is the context object\nwhich gives  parsing context information  (like the start\nand end position where the match occured, the parser instance etc.). The second\nparameters  nodes  is a list of actual results of sub-expressions given in the\norder defined in the grammar.  For example:  lambda _, nodes: nodes[0] * nodes[2],  In this line we don't care about context thus giving it the  _  name.  nodes[0] \nwill cary the value of the left sub-expression while  nodes[2]  will carry the\nresult of the right sub-expression.  nodes[1]  must be  *  and we don't need to\ncheck that as the parser already did that for us.  The result of the parsing will be the evaluated expression as the actions will\nget called along the way and the result of each actions will be used as an\nelement of the  nodes  parameter in calling actions higher in the hierarchy.  If we don't provide  actions , by default parglare will return a matched string\nfor each terminal and a list of sub-expressions for each non-terminal\neffectively producing nested lists. If we set  build_tree  parameter of the\nparser to  True  the parser will produce a  parse tree  whose\nelements are instances of  NodeNonTerm  and  NodeTerm  classes representing a\nnon-terminals and terminals respectively.",
            "title": "Custom actions and built-in actions"
        },
        {
            "location": "/actions/#time-of-actions-call",
            "text": "In parglare actions can be called during parsing (i.e. on the fly) which you\ncould use if you want to transform input immediately without building the parse\ntree. But there are times when you would like to build a tree first and call\nactions afterwards. For example, a very good reason is if you are using GLR and\nyou want to be sure that actions are called only on the final tree.   Note  If you are using GLR be sure that your actions has no side-effects, as the\ndying parsers will left those side-effects behind leading to unpredictable\nbehaviour. In case of doubt create trees first, choose the right one and\ncall actions afterwards with the  call_actions  parser method.   To get the tree and call actions afterwards you supply  actions  parameter to\nthe parser as usual and set  build_tree  to  True . When the parser finishes\nsuccessfully it will return the parse tree which you pass to the  call_actions \nmethod of the parser object to execute actions. For example:  parser = Parser(g, actions=actions)\ntree = parser.parse(\"34 + 4.6 / 2 * 4^2^2 + 78\")\nresult = parser.call_actions(tree)",
            "title": "Time of actions call"
        },
        {
            "location": "/actions/#the-context-object",
            "text": "The first parameter passed to the action function is the Context object. This\nobject provides us with the parsing context of where the match occurred.  Following attributes are available on the context object:    start_position/end_position  - the beginning and the end in the input\n  stream where the match occured.  start_position  is the location of the first\n  element/character in the input while the  end_position  is one past the last\n  element/character of the match. Thus  end_position - start_position  will give\n  the lenght of the match including the layout. You can use\n   parglare.pos_to_line_col(input, position)  function to get line and column of\n  the position. This function returns a tuple  (line, column) .    file_name  - the name/path of the file being parsed.  None  if Python\n  string is parsed.    input_str  - the input string (or list of objects) that is being parsed.    layout_content  - is the layout (whitespaces, comments etc.) that are\n  collected from the previous non-layout match. Default actions for building the\n  parse tree will attach these layouts to the tree nodes.    symbol  - the grammar symbol (instance of either  Terminal  or  NonTerminal \n  which inherits  GrammarSymbol ) this match is for.    production  - an instance of  parglare.grammar.Production  class available\n  only on reduction actions (not on shifts). Represents the grammar production.    node  - this is available only if the actions are called over the parse tree\n  using  call_actions . It represens the instance of  NodeNonTerm  or  NodeTerm \n  classes from the parse tree where the actions is executed.    parser  - is the reference to the parser instance. You should use this only\n  to investigate parser configuration not to alter its state.    You can also use context object to pass information between lower level and\nupper level actions. You can attach any attribute you like, the context object\nis shared between action calls. It is shared with the internal layout parser\ntoo.",
            "title": "The Context object"
        },
        {
            "location": "/actions/#built-in-actions",
            "text": "parglare provides some common actions in the module  parglare.actions . You\ncan reference these actions directly from the grammar .\nBuilt-in actions are used implicitly by parglare as default actions in\nparticular case (e.g.\nfor  syntactic sugar ) but\nyou might want to reference some of these actions directly.  Following are parglare built-in actions from the  parglare.actions  module:    pass_none  - returns  None ;    pass_nochange  - returns second parameter of action callable ( value  or\n   nodes ) unchanged;    pass_empty  - returns an empty list  [] ;    pass_single  - returns  nodes[0] . Used implicitly by rules where all\n  productions have only a single rule reference on the RHS;    pass_inner  - returns  nodes[1] . Handy to extract sub-expression value for\n  values in parentheses;    collect  - Used for rules of the form  Elements: Elements Element |\n  Element; . Implicitly used for  +  operator. Returns list;    collect_sep  - Used for rules of the form  Elements: Elements separator\n  Element | Element; . Implicitly used for  +  with separator. Returns list;    collect_optional  - Can be used for rules of the form  Elements: Elements\n  Element | Element | EMPTY; . Returns list;    collect_sep_optional  - Can be used for rules of the form  Elements: Elements\n  separator Element | Element | EMPTY; . Returns list;    collect_right  - Can be used for rules of the form  Elements: Element\n  Elements | Element; . Returns list;    collect_right_sep  - Can be used for rules of the form  Elements: Element\n  separator Elements | Element; . Returns list;    collect_right_optional  - Can be used for rules of the form  Elements:\n  Element Elements | Element | EMPTY; . Returns list;    collect_right_sep_optional  - Can be used for rules of the form  Elements:\n  Element separator Elements | Element | EMPTY; . Returns list;    optional  - Used for rules of the form  OptionalElement: Element | EMPTY; .\n  Implicitly used for  ?  operator. Returns either a sub-expression value or\n   None  if empty match.    obj  - Used implicitly by rules\n  using  named matches .\n  Creates Python object with attributes derived from named matches. Objects\n  created this way have additional attributes\n   _pg_start_position / _pg_end_position  with start/end position in the input\n  stream where the object is found.",
            "title": "Built-in actions"
        },
        {
            "location": "/actions/#actions-for-rules-using-named-matches",
            "text": "If  named matches  are used in\nthe grammar rule, action will be called with additional keyword parameters named\nby the name of LHS of rule assignments. If no action is specified for the rule a\nbuilt-in action  obj  is called and will produce instance of dynamically created\nPython class corresponding to the grammar rule. See more in the section\non  named matches .  If for some reason you want to override default behavior that create Python\nobject you can create action like this:  S: first=a second=digit+[comma];\na: \"a\";\ndigit: /\\d+/;  now create action function that accepts additional params:  def s_action(context, nodes, first, second):\n   ... do some transformation and return the result of S evaluation\n   ... nodes will contain subexpression results by position while\n   ... first and second will contain the values of corresponding\n   ... sub-expressions  register action on  Parser  instance as usual:  parser = Parser(grammar, actions={\"S\": s_action})",
            "title": "Actions for rules using named matches"
        },
        {
            "location": "/recognizers/",
            "text": "Recognizers\n\u00b6\n\n\nParglare uses scannerless parsing. Actually, scanner is integrated in the\nparser. Each token is created/recognized in the input during parsing using so\ncalled \nrecognizer\n which is connected to the grammar terminal symbol.\n\n\nThis gives a great flexibility to the parglare.\n\n\nFirst, recognizing tokens during parsing eliminate lexical ambiguities that\narise in separate scanning due to the lack of parsing context.\n\n\nSecond, having a separate recognizers for grammar terminal symbols allows us to\nparse not only text but a stream of anything as parsing is nothing more by\nconstructing a tree (or some other form) out of a flat list of objects. Those\nobjects are characters if text is parsed, but don't have to be.\n\n\nParglare has two built-in recognizers for textual parsing that can be specified\nin \nthe grammar directly\n. Those are\nusually enough if text is parsed, but if non-textual content is parsed you will\nhave to supply your own recognizers that are able to recognize tokens in the\ninput stream of objects.\n\n\nRecognizers are Python callables of the following form:\n\n\ndef some_recognizer(input, pos):\n   ...\n   ...\n   return part of input starting at pos\n\n\n\nFor example if we have an input stream of objects that are comparable (e.g.\nnumbers) and we want to recognize the ascending elements starting at the given\nposition but such that the recognized token must have at least two object from\nthe input. We could write following:\n\n\ndef ascending_nosingle(input, pos):\n    \"Match sublist of ascending elements. Matches at least two.\"\n    last = pos + 1\n    while last < len(input) and input[last] > input[last-1]:\n        last += 1\n    if last - pos >= 2:\n        return input[pos:last]\n\n\n\nWe register our recognizers during grammar contstruction. All references in the\ngrammar rules that don't exists in the grammar (i.e. they are not the rule\nthemself) must be resolved as recognizers or the exception will be thrown during\ngrammar construction.\n\n\nIn order to do that, create a Python dict where the key will be a rule name used\nin the grammar references and the value will be recognizer callable.\n\n\nrecognizers = {\n   'ascending': ascending_nosingle\n}\n\ngrammar = Grammar.from_file('mygrammar.pg', recognizers=recognizers)\n\n\n\nNow, in the grammar in file \nmygrammar.pg\n you can reference terminal rule\n\nascending\n (see the key in \nrecognizers\n dict) although it is not defined in\nthe grammar itself. This terminal rule will match ascending sublist of objects\nfrom the input.\n\n\n\n\nNote\n\n\nIf you want more information you could investigate\n\ntest_recognizers.py\n test.",
            "title": "Recognizers"
        },
        {
            "location": "/recognizers/#recognizers",
            "text": "Parglare uses scannerless parsing. Actually, scanner is integrated in the\nparser. Each token is created/recognized in the input during parsing using so\ncalled  recognizer  which is connected to the grammar terminal symbol.  This gives a great flexibility to the parglare.  First, recognizing tokens during parsing eliminate lexical ambiguities that\narise in separate scanning due to the lack of parsing context.  Second, having a separate recognizers for grammar terminal symbols allows us to\nparse not only text but a stream of anything as parsing is nothing more by\nconstructing a tree (or some other form) out of a flat list of objects. Those\nobjects are characters if text is parsed, but don't have to be.  Parglare has two built-in recognizers for textual parsing that can be specified\nin  the grammar directly . Those are\nusually enough if text is parsed, but if non-textual content is parsed you will\nhave to supply your own recognizers that are able to recognize tokens in the\ninput stream of objects.  Recognizers are Python callables of the following form:  def some_recognizer(input, pos):\n   ...\n   ...\n   return part of input starting at pos  For example if we have an input stream of objects that are comparable (e.g.\nnumbers) and we want to recognize the ascending elements starting at the given\nposition but such that the recognized token must have at least two object from\nthe input. We could write following:  def ascending_nosingle(input, pos):\n    \"Match sublist of ascending elements. Matches at least two.\"\n    last = pos + 1\n    while last < len(input) and input[last] > input[last-1]:\n        last += 1\n    if last - pos >= 2:\n        return input[pos:last]  We register our recognizers during grammar contstruction. All references in the\ngrammar rules that don't exists in the grammar (i.e. they are not the rule\nthemself) must be resolved as recognizers or the exception will be thrown during\ngrammar construction.  In order to do that, create a Python dict where the key will be a rule name used\nin the grammar references and the value will be recognizer callable.  recognizers = {\n   'ascending': ascending_nosingle\n}\n\ngrammar = Grammar.from_file('mygrammar.pg', recognizers=recognizers)  Now, in the grammar in file  mygrammar.pg  you can reference terminal rule ascending  (see the key in  recognizers  dict) although it is not defined in\nthe grammar itself. This terminal rule will match ascending sublist of objects\nfrom the input.   Note  If you want more information you could investigate test_recognizers.py  test.",
            "title": "Recognizers"
        },
        {
            "location": "/lr_parsing/",
            "text": "LR parsing, ambiguities and conflicts resolving\n\u00b6\n\n\nLR parser operates as a deterministic PDA (Push-down automata). It is a state\nmachine which is always in some state during parsing. The state machine must\ndeterministically decide what is the next state just based on its current state\nand one token of lookahead. This decision is given by LR tables which are\nprecalculated from the grammar before parsing even begins.\n\n\nFor example, let's see what happens if we have a simple expression grammar:\n\n\nE: E '+' E\n | E '*' E\n | number;\nnumber: /\\d+/;\n\n\n\nand we want to parse the following input:\n\n\n 1 + 2 * 3\n\n\n\nLanguage defined by this grammar is ambiguous as the expression can be\ninterpreted either as:\n\n\n ((1 + 2) * 3)\n\n\n\nor:\n\n\n (1 + (2 * 3))\n\n\n\nIn the parsing process, parser starts in state 0 and it sees token \n1\n ahead\n(one lookahead is used - LR(1)).\n\n\nThe only thing a parser can do in this case is to shift, i.e. to consume the\ntoken and advance the position. This operation transition the automata to some\nother state. From each state there is only one valid transition that can be\ntaken or the PDA won't be deterministic, i.e. we could simultaneously follow\ndifferent paths (that is exactly what the \nGLRParser\n does).\n\n\nCurrent position would be (the dot represents the position):\n\n\n 1 . + 2 * 3\n\n\n\nNow the parser sees \n+\n token ahead and the tables will tell him to reduce the\nnumber he just saw to \nE\n (a number is an expression according to the grammar).\nThus, on the stack the parser will have an expression \nE\n (actually LR states\nare kept on stack but that's not important for this little analysis). This\nreduction will advace PDA to some other state again. Each shift/reduce operation\nchange state so I'll not repeat that anymore.\n\n\n\n\nNote\n\n\nSee \npglr command\n which can be used to visualize PDA. Try to\nvisualize automata for this grammar.\n\n\n\n\nAfter reduction parser will do shift of \n+\n token. There is nothing to reduce as\nthe sub-expresison on stack is \nE +\n which can't be reduced as it's not complete.\nSo, the only thing we can do is to shift \n2\n token.\n\n\nNow, the position is:\n\n\n  1 + 2 . * 3\n\n\n\nAnd the stack is:\n\n\n  E + 2\n\n\n\nAnd this is a place where the parser can't decide what to do. It can either\nreduce the sum on the stack or shift \n*\n and \n3\n and reduce multiplication\nfirst and the sumation afterwards.\n\n\nIf the sum is reduced first and \n*\n shifted afterwards we would get the\nfollowing result:\n\n\n (1 + 2) * 3\n\n\n\nIf the shift of \n*\n and \n3\n is done instead of reducing, the reduction would\nfirst reduce multiplication and then sum (reduction is always done on the top of\nthe stack). We will have the following result:\n\n\n1 + (2 * 3)\n\n\n\nFrom the point of view of arithmetic priorities, preferred solution is the last\none but the parser don't know arithmetic rules.\n\n\nIf you analyze this grammar using \npglr command\n you will see that\nthe LR tables have Shift/Reduce conflicts as there is a state in which parser\ncan't decide whether to shift or to reduce (we just saw that situation).\n\n\nparglare gives you various tools to be more explicit with your grammar and to\nresolve these conflicts.\n\n\nThere are two situations when conflicts can't be resolved:\n\n\n\n\nyou need more than one lookahead to disambiguate,\n\n\nyour language is inherently ambiguous.\n\n\n\n\nIf you end up in one of these situations you must use GLR parsing, which will\nfork the parser at each state which has multiple paths, and explore all\npossibilities.\n\n\nResolving conflicts\n\u00b6\n\n\nWhen we run:\n\n\n$ pglr -d check expr.pg\n\n\n\nwhere in \nexpr.pg\n we have the above grammar, we get the following output at the\nend:\n\n\n*** S/R conflicts ***\nThere are 4 S/R conflicts\n\nState 6\n        1: E = E + E .   {+, *, STOP}\n        1: E = E . + E   {+, *, STOP}\n        2: E = E . * E   {+, *, STOP}\n\nIn state 6 and input symbol '+' can't decide whether to shift or\nreduce by production(s) '1: E = E + E'.\n\nState 6\n        1: E = E + E .   {+, *, STOP}\n        1: E = E . + E   {+, *, STOP}\n        2: E = E . * E   {+, *, STOP}\n\nIn state 6 and input symbol '*' can't decide whether to shift or\nreduce by production(s) '1: E = E + E'.\n\nState 7\n        2: E = E * E .   {+, *, STOP}\n        1: E = E . + E   {+, *, STOP}\n        2: E = E . * E   {+, *, STOP}\n\nIn state 7 and input symbol '+' can't decide whether to shift or\nreduce by production(s) '2: E = E * E'.\n\nState 7\n        2: E = E * E .   {+, *, STOP}\n        1: E = E . + E   {+, *, STOP}\n        2: E = E . * E   {+, *, STOP}\n\nIn state 7 and input symbol '*' can't decide whether to shift or\nreduce by production(s) '2: E = E * E'.\nGrammar OK.\nThere are 4 Shift/Reduce conflicts. Either use 'prefer_shifts' parser mode,\ntry to resolve manually or use GLR parsing.\n\n\n\nAs we can see this grammar has 4 Shift/Reduce conflicts. At the end of the\noutput we get an advice to either use \nprefer_shifts\n strategy that will always\nprefer shift over reduce. In this case that's not what we want.\n\n\nIf we look closely at the output we see that parglare gives us an informative\nexplanation why there are conflicts in our grammar.\n\n\nThe first conflict:\n\n\nState 6\n        1: E = E + E .   {+, *, STOP}\n        1: E = E . + E   {+, *, STOP}\n        2: E = E . * E   {+, *, STOP}\n\nIn state 6 and input symbol '+' can't decide whether to shift or\nreduce by production(s) '1: E = E + E'.\n\n\n\nTell us that when the parser saw addition \u2014 the dot in the above productions\nrepresents all possible positions of the parser in the input stream in this\nstate \u2014 and there is \n+\n ahead, it doesn't know should it reduce the addition or\nshift the \n+\n token.\n\n\nThis means that if we have an expression: \n1 + 2 + 3\n should we calculate it as\n\n(1 + 2) + 3\n or as \n1 + (2 + 3)\n. Of course, the result in this case would be\nthe same, but imagine what would happen if we had subtraction operation instead\nof addition. In arithmetic, this is defined by association which says that\naddition if left associative, thus the operation is executed from left to right.\n\n\nParglare enables you to define associativity for you productions by specifying\n\n{left}\n or \n{right}\n at the end of production. You can think of \nleft\n\nassociativity as telling the parser to prefer reduce over shift for this\nproduction and the \nright\n associativity for preferring shifts over reduces.\n\n\nLet's see the second conflict:\n\n\nState 6\n        1: E = E + E .   {+, *, STOP}\n        1: E = E . + E   {+, *, STOP}\n        2: E = E . * E   {+, *, STOP}\n\nIn state 6 and input symbol '*' can't decide whether to shift or\nreduce by production(s) '1: E = E + E'.\n\n\n\nIn the same state, when we saw addition and have \n*\n ahead parser can't decide.\n\n\nThis means that if we have an expression: \n1 + 2 * 3\n should we calculate it as\n\n(1 + 2) * 3\n or \n1 + (2 * 3)\n. In arithmetic this is handled by operation\npriority. We want multiplication to be executed first, so we should raise the\npriority of multiplication (or lower the priority of addition).\n\n\nE: E '+' E  {left, 1}\n | E '*' E  {left, 2}\n | number;\nnumber: /\\d+/;\n\n\n\nWe have augmented our grammar to state that both operation are left associative,\nthus the parser will know what to do in the case of \n1 + 2 + 3\n or \n1 * 2 * 3\n\nit will reduce from left to right, i.e. prefer reduce over shifts. We also\nspecified that addition has a priority of 1 and multiplication has a priority of\n2, thus the parser will know what to do in case of \n1 + 2 * 3\n, it will shift\n\n*\n instead of reducing addition as the multiplication should be\nreduced/calculated first.\n\n\n\n\nNote\n\n\nThe default priority for rules is 10.\n\n\n\n\nThis change in the grammar resolves all ambiguities and our grammar is now\nLR(1).\n\n\nSee the section on \ndisambiguation strategies\n for more.",
            "title": "LR parsing and conflicts"
        },
        {
            "location": "/lr_parsing/#lr-parsing-ambiguities-and-conflicts-resolving",
            "text": "LR parser operates as a deterministic PDA (Push-down automata). It is a state\nmachine which is always in some state during parsing. The state machine must\ndeterministically decide what is the next state just based on its current state\nand one token of lookahead. This decision is given by LR tables which are\nprecalculated from the grammar before parsing even begins.  For example, let's see what happens if we have a simple expression grammar:  E: E '+' E\n | E '*' E\n | number;\nnumber: /\\d+/;  and we want to parse the following input:   1 + 2 * 3  Language defined by this grammar is ambiguous as the expression can be\ninterpreted either as:   ((1 + 2) * 3)  or:   (1 + (2 * 3))  In the parsing process, parser starts in state 0 and it sees token  1  ahead\n(one lookahead is used - LR(1)).  The only thing a parser can do in this case is to shift, i.e. to consume the\ntoken and advance the position. This operation transition the automata to some\nother state. From each state there is only one valid transition that can be\ntaken or the PDA won't be deterministic, i.e. we could simultaneously follow\ndifferent paths (that is exactly what the  GLRParser  does).  Current position would be (the dot represents the position):   1 . + 2 * 3  Now the parser sees  +  token ahead and the tables will tell him to reduce the\nnumber he just saw to  E  (a number is an expression according to the grammar).\nThus, on the stack the parser will have an expression  E  (actually LR states\nare kept on stack but that's not important for this little analysis). This\nreduction will advace PDA to some other state again. Each shift/reduce operation\nchange state so I'll not repeat that anymore.   Note  See  pglr command  which can be used to visualize PDA. Try to\nvisualize automata for this grammar.   After reduction parser will do shift of  +  token. There is nothing to reduce as\nthe sub-expresison on stack is  E +  which can't be reduced as it's not complete.\nSo, the only thing we can do is to shift  2  token.  Now, the position is:    1 + 2 . * 3  And the stack is:    E + 2  And this is a place where the parser can't decide what to do. It can either\nreduce the sum on the stack or shift  *  and  3  and reduce multiplication\nfirst and the sumation afterwards.  If the sum is reduced first and  *  shifted afterwards we would get the\nfollowing result:   (1 + 2) * 3  If the shift of  *  and  3  is done instead of reducing, the reduction would\nfirst reduce multiplication and then sum (reduction is always done on the top of\nthe stack). We will have the following result:  1 + (2 * 3)  From the point of view of arithmetic priorities, preferred solution is the last\none but the parser don't know arithmetic rules.  If you analyze this grammar using  pglr command  you will see that\nthe LR tables have Shift/Reduce conflicts as there is a state in which parser\ncan't decide whether to shift or to reduce (we just saw that situation).  parglare gives you various tools to be more explicit with your grammar and to\nresolve these conflicts.  There are two situations when conflicts can't be resolved:   you need more than one lookahead to disambiguate,  your language is inherently ambiguous.   If you end up in one of these situations you must use GLR parsing, which will\nfork the parser at each state which has multiple paths, and explore all\npossibilities.",
            "title": "LR parsing, ambiguities and conflicts resolving"
        },
        {
            "location": "/lr_parsing/#resolving-conflicts",
            "text": "When we run:  $ pglr -d check expr.pg  where in  expr.pg  we have the above grammar, we get the following output at the\nend:  *** S/R conflicts ***\nThere are 4 S/R conflicts\n\nState 6\n        1: E = E + E .   {+, *, STOP}\n        1: E = E . + E   {+, *, STOP}\n        2: E = E . * E   {+, *, STOP}\n\nIn state 6 and input symbol '+' can't decide whether to shift or\nreduce by production(s) '1: E = E + E'.\n\nState 6\n        1: E = E + E .   {+, *, STOP}\n        1: E = E . + E   {+, *, STOP}\n        2: E = E . * E   {+, *, STOP}\n\nIn state 6 and input symbol '*' can't decide whether to shift or\nreduce by production(s) '1: E = E + E'.\n\nState 7\n        2: E = E * E .   {+, *, STOP}\n        1: E = E . + E   {+, *, STOP}\n        2: E = E . * E   {+, *, STOP}\n\nIn state 7 and input symbol '+' can't decide whether to shift or\nreduce by production(s) '2: E = E * E'.\n\nState 7\n        2: E = E * E .   {+, *, STOP}\n        1: E = E . + E   {+, *, STOP}\n        2: E = E . * E   {+, *, STOP}\n\nIn state 7 and input symbol '*' can't decide whether to shift or\nreduce by production(s) '2: E = E * E'.\nGrammar OK.\nThere are 4 Shift/Reduce conflicts. Either use 'prefer_shifts' parser mode,\ntry to resolve manually or use GLR parsing.  As we can see this grammar has 4 Shift/Reduce conflicts. At the end of the\noutput we get an advice to either use  prefer_shifts  strategy that will always\nprefer shift over reduce. In this case that's not what we want.  If we look closely at the output we see that parglare gives us an informative\nexplanation why there are conflicts in our grammar.  The first conflict:  State 6\n        1: E = E + E .   {+, *, STOP}\n        1: E = E . + E   {+, *, STOP}\n        2: E = E . * E   {+, *, STOP}\n\nIn state 6 and input symbol '+' can't decide whether to shift or\nreduce by production(s) '1: E = E + E'.  Tell us that when the parser saw addition \u2014 the dot in the above productions\nrepresents all possible positions of the parser in the input stream in this\nstate \u2014 and there is  +  ahead, it doesn't know should it reduce the addition or\nshift the  +  token.  This means that if we have an expression:  1 + 2 + 3  should we calculate it as (1 + 2) + 3  or as  1 + (2 + 3) . Of course, the result in this case would be\nthe same, but imagine what would happen if we had subtraction operation instead\nof addition. In arithmetic, this is defined by association which says that\naddition if left associative, thus the operation is executed from left to right.  Parglare enables you to define associativity for you productions by specifying {left}  or  {right}  at the end of production. You can think of  left \nassociativity as telling the parser to prefer reduce over shift for this\nproduction and the  right  associativity for preferring shifts over reduces.  Let's see the second conflict:  State 6\n        1: E = E + E .   {+, *, STOP}\n        1: E = E . + E   {+, *, STOP}\n        2: E = E . * E   {+, *, STOP}\n\nIn state 6 and input symbol '*' can't decide whether to shift or\nreduce by production(s) '1: E = E + E'.  In the same state, when we saw addition and have  *  ahead parser can't decide.  This means that if we have an expression:  1 + 2 * 3  should we calculate it as (1 + 2) * 3  or  1 + (2 * 3) . In arithmetic this is handled by operation\npriority. We want multiplication to be executed first, so we should raise the\npriority of multiplication (or lower the priority of addition).  E: E '+' E  {left, 1}\n | E '*' E  {left, 2}\n | number;\nnumber: /\\d+/;  We have augmented our grammar to state that both operation are left associative,\nthus the parser will know what to do in the case of  1 + 2 + 3  or  1 * 2 * 3 \nit will reduce from left to right, i.e. prefer reduce over shifts. We also\nspecified that addition has a priority of 1 and multiplication has a priority of\n2, thus the parser will know what to do in case of  1 + 2 * 3 , it will shift *  instead of reducing addition as the multiplication should be\nreduced/calculated first.   Note  The default priority for rules is 10.   This change in the grammar resolves all ambiguities and our grammar is now\nLR(1).  See the section on  disambiguation strategies  for more.",
            "title": "Resolving conflicts"
        },
        {
            "location": "/disambiguation/",
            "text": "Disambiguation\n\u00b6\n\n\nAt each step LR parser has to decide which operation to execute: SHIFT (to\nconsume the next token) or REDUCE (to reduce what it saw previously to some\nhigher level concept).\nSee \nsection on LR parsing and conflicts\n.\n\n\nDefining language by CFG alone often leads to ambiguous languages. Sometimes\nthis is what we want, i.e. our inputs indeed have multiple interpretation and we\nwant all of them. This is usually true for natural languages. But when we deal\nwith computer languages we want to avoid ambiguity as there should be only one\ninterpretation for each valid input. We want to define unambiguous language.\n\n\nTo constrain our grammar and make it define unambiguous language we use so\ncalled \ndisambiguation filters\n. These filters are in charge of choosing the\nright interpretation/tree when there is ambiguity in the our grammar.\n\n\nEven in the simple expression grammar there is ambiguity. For example,\n\n2 + 3 * 4\n expression \u2014 in case we know nothing about priorities of arithmetic\noperations \u2014 can be interpreted in two different ways: \n(2 + 3) * 4\n and\n\n2 + (3 * 4)\n. Without priorities we would be obliged to use parentheses\neverywhere to specify the right interpretation.\n\n\nAmbiguity is a one source of conflicts in the LR grammars. The other is limited\nlookahead. Whatever is the source of conflicts \nGLRParser\n can cope with it. In\ncase of ambiguity the parser will return all interpretations possible. In case\nof a limited lookahead the parser will investigate all possible paths and\nresolve to the correct interpretation further down the input stream.\n\n\nIf our grammar is ambiguous and our language is not that means that we need to\nconstrain our grammar using disambiguation filters to better describe our\nlanguage. Ideally, we strive for a grammar that describe all valid sentences in\nour language with a single interpretation for each of them and nothing more.\n\n\nStatic disambiguation filters\n\u00b6\n\n\nStatic disambiguation filters are given in the grammar at the end of the\nproduction using \n{}\n syntax. There is also\na \ndynamic disambiguation filter\n that is most\npowerful and is specified as a Python function.\n\n\npriority\n\u00b6\n\n\nPriority is probably the simplest form of disambiguation. It is also the\nstrongest in parglare as it's first checked. It is given as a numeric value\nwhere the default is 10. When the parser can't decide what operation to use it\nwill favor the one associated with the production with a higher priority.\n\n\nFor example:\n\n\nE: E \"*\" E {2};\nE: E \"+\" E {1};\n\n\n\n\nThis gives priority of \n2\n to the production \nE \"*\" E\n and \n1\n to the production\n\nE \"+\" E\n. When parglare needs to decide, e.g. between shifting \n+\n or reducing\n\n*\n it saw, it will choose reduce as the multiplication production has higher\npriority.\n\n\nPriority can also be given to terminal productions.\n\n\nassociativity\n\u00b6\n\n\nAssociativity is used for disambiguation between productions of the same\npriority. In the grammar fragment above we still have ambiguity for expression:\n\n\n2 + 3 + 5\n\n\n\n\nThere are two interpretations \n(2 + 3) + 5\n and \n2 + (3 + 5)\n. Of course, with\narithmentic \n+\n operation the result will be the same but that's not true for\neach operation. Anyway, parse trees will be different so some choice has to be\nmade.\n\n\nIn this situation associativity is used. Both \n+\n and \n*\n in arithmentic are\nleft associative (i.e. the operation is evaluated from left to right).\n\n\nE: E \"*\" E {2, left};\nE: E \"+\" E {1, left};\n\n\n\n\nNow, the expression above is not ambiguous anymore. It is interpreted as \n(2 +\n3) + 5\n.\n\n\nThe associativity given in the grammar is either \nleft\n or \nright\n. Default is\nno associativity, i.e. associativity is not used for disambiguation decision.\n\n\nprefer\n\u00b6\n\n\nThis disambiguation filter is applicable to terminal productions only. It will\nbe used to choose the right recognizer/terminal in case\nof \nlexical ambiguity\n.\n\n\nFor example:\n\n\nINT = /[-+]?[0-9]+\\b/ {prefer};\nFLOAT = /[-+]?[0-9]*\\.?[0-9]+([eE][-+]?[0-9]+)?\\b/;\n\n\n\n\nIf in the grammar we have a possibility that both recognizers are tried, both\nwill succeed for input \n23\n, but we want \nINT\n to be choosen in this case.\n\n\nDynamic disambiguation filter\n\u00b6\n\n\nAll previously described filters are of static nature, i.e. they are compiled\nduring LR table calculation (by removing erroneous automata transitions) and\nthey don't depend on the parsed input.\n\n\nThere are sometimes situations when parsing decision depends on the input.\n\n\nFor example, lets say that we need to parse arithmetic expression but our\noperation priority increase for operations that are introduced later in the\ninput.\n\n\n1 + 2 * 3 - 4 + 5\n\n\n\n\nShould be parsed as:\n\n\n((1 + (2 * (3 - 4))) + 5)\n\n\n\n\nWhile\n\n\n1 - 2 + 3 * 4 + 5\n\n\n\n\nshould be parsed as:\n\n\n(1 - ((2 + (3 * 4)) + 5))\n\n\n\n\nAs you can see, operations that appears later in the input are of higher priority.\n\n\nIn parglare you can implement this dynamic behavior in two steps:\n\n\nFirst, mark productions in your grammar by \ndynamic\n rule:\n\n\nE: E op_sum E {dynamic}\n | E op_mul E {dynamic}\n | /\\d+/;\nop_sum: '+' {dynamic};\nop_mul: '*' {dynamic};\n\n\n\n\nThis tells parglare that those production are candidates for dynamic ambiguity\nresolution.\n\n\nSecond step is to register a function, during parser construction, that will be\nused for resolution. This function operates as a filter for actions in a given\nstate and lookahead token. It receives the current action, a token ahead,\nproduction (for REDUCE action), sub-results (for REDUCE action), and current LR\nautomata state and returns either \nTrue\n if the action is acceptable or \nFalse\n\notherwise. This function sometimes need to maintain some kind of state. To\ninitialize its state at the beginning it is called with \nNone\n as parameters.\n\n\nparser = Parser(grammar, dynamic_filter=custom_disambiguation_filter)\n\n\n\n\nWhere resolution function is of the following form:\n\n\ndef custom_disambiguation_filter(action, token, production, subresults, state):\n    \"\"\"Make first operation that appears in the input as lower priority.\n    This demonstrates how priority rule can change dynamically depending\n    on the input.\n    \"\"\"\n    global operations\n\n    # At the start of parsing this function is called with actions set to\n    # None to give a chance for the strategy to initialize.\n    if action is None:\n        operations = []\n        return\n\n    actions = state.actions[token.symbol]\n\n    # Lookahead operation\n    shift_op = token.symbol\n\n    if action is SHIFT:\n        if shift_op not in operations:\n            operations.append(shift_op)\n        if len(actions) == 1:\n            return True\n        red_op = [a for a in actions if a.action is REDUCE][0].prod.rhs[1]\n        return operations.index(shift_op) > operations.index(red_op)\n\n    elif action is REDUCE:\n\n        # Current reduction operation\n        red_op = production.rhs[1]\n        if red_op not in operations:\n            operations.append(red_op)\n\n        if len(actions) == 1:\n            return True\n\n        # If lookahead operation is not processed yet is is of higer priority\n        # so do not reduce.\n        # If lookahead is in operation and its index is higher do not reduce.\n        return (shift_op in operations\n                and (operations.index(shift_op)\n                     <= operations.index(red_op)))\n\n\n\n\nThis function is a predicate that will be called for each action for productions\nmarked with \ndynamic\n (SHIFT action for dynamic terminal production and REDUCE\naction for dynamic non-terminal productions). You are provided with enough\ninformation to make a custom decision whether to perform or reject the\noperation.\n\n\nParameters are:\n\n\n\n\n\n\naction\n - either SHIFT or REDUCE constant from \nparglare\n module,\n\n\n\n\n\n\ntoken (Token)\n - a \nlookahead token\n,\n\n\n\n\n\n\nproduction (Production)\n - a \nproduction\n to be reduced. Valid only for\n  REDUCE.\n\n\n\n\n\n\nsubresults (list)\n - a sub-results for the reduction. Valid only for\n  REDUCE. The length of this list is equal to \nlen(production.rhs)\n.\n\n\n\n\n\n\nstate (LRState)\n - current LR parser state.\n\n\n\n\n\n\nFor details see \ntest_dynamic_disambiguation_filters.py\n.\n\n\nLexical ambiguities\n\u00b6\n\n\nThere is another source of ambiguities.\n\n\nParglare uses integrated scanner, thus tokens are determined on the fly. This\ngives greater lexical disambiguation power but lexical ambiguities might arise\nnevertheless. Lexical ambiguity is a situation when at some place in the input\nmore than one recognizer match successfully.\n\n\nFor example, if in the input we have \n3.4\n and we expect at this place either an\ninteger or a float. Both of these recognizer can match the input. The integer\nrecognizer would match \n3\n while the float recognizer would match \n3.4\n. What\nshould we use?\n\n\nparglare has implicit lexical disambiguation strategy that will:\n\n\n\n\nUse priorities first.\n\n\nString recognizers are preferred over regexes (i.e. the most specific match).\n\n\nIf priorities are the same and we have no string recognizers use\n   longest-match strategy.\n\n\nIf more recognizers still match use \nprefer\n rule if given.\n\n\nIf all else fails raise an exception. In case of GLR, ambiguity will be\n   handled by parser forking, i.e. you will end up with all solutions/trees.\n\n\n\n\nThus, in terminal definition rules we can use priorities to favor some of the\nrecognizers, or we can use \nprefer\n to favor recognizer if there are multiple\nmatches of the same length.\n\n\nFor example:\n\n\n  number = /\\d+/ {15};\n\n\n\nor:\n\n\n  number = /\\d+/ {prefer};\n\n\n\nIn addition, you can also specify terminal to take a part in dynamic\ndisambiguation:\n\n\n  number = /\\d+/ {dynamic};\n\n\n\nCustom lexical disambiguation\n\u00b6\n\n\nIn the previous section is explained built-in parglare lexical disambiguation\nstrategy. There are use-cases when this strategy is not sufficient. For example,\nif we want to do fuzzy match of tokens and choose the most similar token at the\nposition.\n\n\nparglare solves this problem by enabling you to register a callable during\nparser instantiation that will, during parsing, get all the symbols expected at\nthe current location and return a list of tokens (instances\nof \nToken\n class\n) or \nNone\n or empty list if no symbol is\nfound at the location.\n\n\nThis callable is registered during parser instantiation as the parameter\n\ncustom_lexical_disambiguation\n.\n\n\nparser = Parser(\n    grammar, custom_lexical_disambiguation=custom_lexical_disambiguation)\n\n\n\n\nThe callable accepts:\n\n\n\n\nsymbols\n - a list of terminals expected at the current position,\n\n\ninput_str\n - input string,\n\n\nposition\n - current position in the input string.\n\n\nget_tokens\n - a callable used to get the tokens recognized using the\n  default strategy. Called without parameters. Custom disambiguation might\n  decide to return this list if no change is necessary, reduce the list, or\n  extend it with new tokens. See the example bellow how to return list with a\n  token only if the default recognition doesn't succeed.\n\n\n\n\nReturns:\n a list of \nToken\n class instances or \nNone\n/empty list if no token\nis found.\n\n\nTo instantiate \nToken\n pass in the symbol and the value of the token. Value of\nthe token is usually a sub-string of the input string.\n\n\nIn the following test \nBar\n and \nBaz\n non-terminals are fuzzy matched. The\nnon-terminal with the higher score wins but only if the score is above 0.7.\n\n\ngrammar = \"\"\"\nS: Element+ EOF;\nElement: Bar | Baz | Number;\nBar: /Bar. \\d+/;\nBaz: /Baz. \\d+/;\nNumber: /\\d+/;\n\"\"\"\n\ng = Grammar.from_string(grammar)\ngrammar = [g]\n\ndef custom_lexical_disambiguation(symbols, input_str, position, get_tokens):\n    \"\"\"\n    Lexical disambiguation should return a single token that is\n    recognized at the given place in the input string.\n    \"\"\"\n    # Call default token recognition.\n    tokens = get_tokens()\n\n    if tokens:\n        # If default recognition succeeds use the result.\n        return tokens\n    else:\n        # If no tokens are found do the fuzzy match.\n        matchers = [\n            lambda x: difflib.SequenceMatcher(None, 'bar.', x.lower()),\n            lambda x: difflib.SequenceMatcher(None, 'baz.', x.lower())\n        ]\n        symbols = [\n            grammar[0].get_terminal('Bar'),\n            grammar[0].get_terminal('Baz'),\n        ]\n        # Try to do fuzzy match at the position\n        elem = input_str[position:position+4]\n        elem_num = input_str[position:]\n        number_matcher = re.compile('[^\\d]*(\\d+)')\n        number_match = number_matcher.match(elem_num)\n        ratios = []\n        for matcher in matchers:\n            ratios.append(matcher(elem).ratio())\n\n        max_ratio_index = ratios.index(max(ratios))\n        if ratios[max_ratio_index] > 0.7 and number_match.group(1):\n            return [Token(symbols[max_ratio_index], number_match.group())]\n\n\nparser = Parser(\n    g, custom_lexical_disambiguation=custom_lexical_disambiguation)\n\n# Bar and Baz will be recognized by a fuzzy match\nresult = parser.parse('bar. 56 Baz 12')\nassert result == [['bar. 56', 'Baz 12'], None]\n\nresult = parser.parse('Buz. 34 bar 56')\nassert result == [['Buz. 34', 'bar 56'], None]\n\nresult = parser.parse('Ba. 34 baz 56')\nassert result == [['Ba. 34', 'baz 56'], None]\n\n# But if Bar/Baz are too different from the correct pattern\n# we get ParseError. In this case `bza` score is bellow 0.7\n# for both Bar and Baz symbols.\nwith pytest.raises(ParseError):\n    parser.parse('Bar. 34 bza 56')",
            "title": "Disambiguation"
        },
        {
            "location": "/disambiguation/#disambiguation",
            "text": "At each step LR parser has to decide which operation to execute: SHIFT (to\nconsume the next token) or REDUCE (to reduce what it saw previously to some\nhigher level concept).\nSee  section on LR parsing and conflicts .  Defining language by CFG alone often leads to ambiguous languages. Sometimes\nthis is what we want, i.e. our inputs indeed have multiple interpretation and we\nwant all of them. This is usually true for natural languages. But when we deal\nwith computer languages we want to avoid ambiguity as there should be only one\ninterpretation for each valid input. We want to define unambiguous language.  To constrain our grammar and make it define unambiguous language we use so\ncalled  disambiguation filters . These filters are in charge of choosing the\nright interpretation/tree when there is ambiguity in the our grammar.  Even in the simple expression grammar there is ambiguity. For example, 2 + 3 * 4  expression \u2014 in case we know nothing about priorities of arithmetic\noperations \u2014 can be interpreted in two different ways:  (2 + 3) * 4  and 2 + (3 * 4) . Without priorities we would be obliged to use parentheses\neverywhere to specify the right interpretation.  Ambiguity is a one source of conflicts in the LR grammars. The other is limited\nlookahead. Whatever is the source of conflicts  GLRParser  can cope with it. In\ncase of ambiguity the parser will return all interpretations possible. In case\nof a limited lookahead the parser will investigate all possible paths and\nresolve to the correct interpretation further down the input stream.  If our grammar is ambiguous and our language is not that means that we need to\nconstrain our grammar using disambiguation filters to better describe our\nlanguage. Ideally, we strive for a grammar that describe all valid sentences in\nour language with a single interpretation for each of them and nothing more.",
            "title": "Disambiguation"
        },
        {
            "location": "/disambiguation/#static-disambiguation-filters",
            "text": "Static disambiguation filters are given in the grammar at the end of the\nproduction using  {}  syntax. There is also\na  dynamic disambiguation filter  that is most\npowerful and is specified as a Python function.",
            "title": "Static disambiguation filters"
        },
        {
            "location": "/disambiguation/#priority",
            "text": "Priority is probably the simplest form of disambiguation. It is also the\nstrongest in parglare as it's first checked. It is given as a numeric value\nwhere the default is 10. When the parser can't decide what operation to use it\nwill favor the one associated with the production with a higher priority.  For example:  E: E \"*\" E {2};\nE: E \"+\" E {1};  This gives priority of  2  to the production  E \"*\" E  and  1  to the production E \"+\" E . When parglare needs to decide, e.g. between shifting  +  or reducing *  it saw, it will choose reduce as the multiplication production has higher\npriority.  Priority can also be given to terminal productions.",
            "title": "priority"
        },
        {
            "location": "/disambiguation/#associativity",
            "text": "Associativity is used for disambiguation between productions of the same\npriority. In the grammar fragment above we still have ambiguity for expression:  2 + 3 + 5  There are two interpretations  (2 + 3) + 5  and  2 + (3 + 5) . Of course, with\narithmentic  +  operation the result will be the same but that's not true for\neach operation. Anyway, parse trees will be different so some choice has to be\nmade.  In this situation associativity is used. Both  +  and  *  in arithmentic are\nleft associative (i.e. the operation is evaluated from left to right).  E: E \"*\" E {2, left};\nE: E \"+\" E {1, left};  Now, the expression above is not ambiguous anymore. It is interpreted as  (2 +\n3) + 5 .  The associativity given in the grammar is either  left  or  right . Default is\nno associativity, i.e. associativity is not used for disambiguation decision.",
            "title": "associativity"
        },
        {
            "location": "/disambiguation/#prefer",
            "text": "This disambiguation filter is applicable to terminal productions only. It will\nbe used to choose the right recognizer/terminal in case\nof  lexical ambiguity .  For example:  INT = /[-+]?[0-9]+\\b/ {prefer};\nFLOAT = /[-+]?[0-9]*\\.?[0-9]+([eE][-+]?[0-9]+)?\\b/;  If in the grammar we have a possibility that both recognizers are tried, both\nwill succeed for input  23 , but we want  INT  to be choosen in this case.",
            "title": "prefer"
        },
        {
            "location": "/disambiguation/#dynamic-disambiguation-filter",
            "text": "All previously described filters are of static nature, i.e. they are compiled\nduring LR table calculation (by removing erroneous automata transitions) and\nthey don't depend on the parsed input.  There are sometimes situations when parsing decision depends on the input.  For example, lets say that we need to parse arithmetic expression but our\noperation priority increase for operations that are introduced later in the\ninput.  1 + 2 * 3 - 4 + 5  Should be parsed as:  ((1 + (2 * (3 - 4))) + 5)  While  1 - 2 + 3 * 4 + 5  should be parsed as:  (1 - ((2 + (3 * 4)) + 5))  As you can see, operations that appears later in the input are of higher priority.  In parglare you can implement this dynamic behavior in two steps:  First, mark productions in your grammar by  dynamic  rule:  E: E op_sum E {dynamic}\n | E op_mul E {dynamic}\n | /\\d+/;\nop_sum: '+' {dynamic};\nop_mul: '*' {dynamic};  This tells parglare that those production are candidates for dynamic ambiguity\nresolution.  Second step is to register a function, during parser construction, that will be\nused for resolution. This function operates as a filter for actions in a given\nstate and lookahead token. It receives the current action, a token ahead,\nproduction (for REDUCE action), sub-results (for REDUCE action), and current LR\nautomata state and returns either  True  if the action is acceptable or  False \notherwise. This function sometimes need to maintain some kind of state. To\ninitialize its state at the beginning it is called with  None  as parameters.  parser = Parser(grammar, dynamic_filter=custom_disambiguation_filter)  Where resolution function is of the following form:  def custom_disambiguation_filter(action, token, production, subresults, state):\n    \"\"\"Make first operation that appears in the input as lower priority.\n    This demonstrates how priority rule can change dynamically depending\n    on the input.\n    \"\"\"\n    global operations\n\n    # At the start of parsing this function is called with actions set to\n    # None to give a chance for the strategy to initialize.\n    if action is None:\n        operations = []\n        return\n\n    actions = state.actions[token.symbol]\n\n    # Lookahead operation\n    shift_op = token.symbol\n\n    if action is SHIFT:\n        if shift_op not in operations:\n            operations.append(shift_op)\n        if len(actions) == 1:\n            return True\n        red_op = [a for a in actions if a.action is REDUCE][0].prod.rhs[1]\n        return operations.index(shift_op) > operations.index(red_op)\n\n    elif action is REDUCE:\n\n        # Current reduction operation\n        red_op = production.rhs[1]\n        if red_op not in operations:\n            operations.append(red_op)\n\n        if len(actions) == 1:\n            return True\n\n        # If lookahead operation is not processed yet is is of higer priority\n        # so do not reduce.\n        # If lookahead is in operation and its index is higher do not reduce.\n        return (shift_op in operations\n                and (operations.index(shift_op)\n                     <= operations.index(red_op)))  This function is a predicate that will be called for each action for productions\nmarked with  dynamic  (SHIFT action for dynamic terminal production and REDUCE\naction for dynamic non-terminal productions). You are provided with enough\ninformation to make a custom decision whether to perform or reject the\noperation.  Parameters are:    action  - either SHIFT or REDUCE constant from  parglare  module,    token (Token)  - a  lookahead token ,    production (Production)  - a  production  to be reduced. Valid only for\n  REDUCE.    subresults (list)  - a sub-results for the reduction. Valid only for\n  REDUCE. The length of this list is equal to  len(production.rhs) .    state (LRState)  - current LR parser state.    For details see  test_dynamic_disambiguation_filters.py .",
            "title": "Dynamic disambiguation filter"
        },
        {
            "location": "/disambiguation/#lexical-ambiguities",
            "text": "There is another source of ambiguities.  Parglare uses integrated scanner, thus tokens are determined on the fly. This\ngives greater lexical disambiguation power but lexical ambiguities might arise\nnevertheless. Lexical ambiguity is a situation when at some place in the input\nmore than one recognizer match successfully.  For example, if in the input we have  3.4  and we expect at this place either an\ninteger or a float. Both of these recognizer can match the input. The integer\nrecognizer would match  3  while the float recognizer would match  3.4 . What\nshould we use?  parglare has implicit lexical disambiguation strategy that will:   Use priorities first.  String recognizers are preferred over regexes (i.e. the most specific match).  If priorities are the same and we have no string recognizers use\n   longest-match strategy.  If more recognizers still match use  prefer  rule if given.  If all else fails raise an exception. In case of GLR, ambiguity will be\n   handled by parser forking, i.e. you will end up with all solutions/trees.   Thus, in terminal definition rules we can use priorities to favor some of the\nrecognizers, or we can use  prefer  to favor recognizer if there are multiple\nmatches of the same length.  For example:    number = /\\d+/ {15};  or:    number = /\\d+/ {prefer};  In addition, you can also specify terminal to take a part in dynamic\ndisambiguation:    number = /\\d+/ {dynamic};",
            "title": "Lexical ambiguities"
        },
        {
            "location": "/disambiguation/#custom-lexical-disambiguation",
            "text": "In the previous section is explained built-in parglare lexical disambiguation\nstrategy. There are use-cases when this strategy is not sufficient. For example,\nif we want to do fuzzy match of tokens and choose the most similar token at the\nposition.  parglare solves this problem by enabling you to register a callable during\nparser instantiation that will, during parsing, get all the symbols expected at\nthe current location and return a list of tokens (instances\nof  Token  class ) or  None  or empty list if no symbol is\nfound at the location.  This callable is registered during parser instantiation as the parameter custom_lexical_disambiguation .  parser = Parser(\n    grammar, custom_lexical_disambiguation=custom_lexical_disambiguation)  The callable accepts:   symbols  - a list of terminals expected at the current position,  input_str  - input string,  position  - current position in the input string.  get_tokens  - a callable used to get the tokens recognized using the\n  default strategy. Called without parameters. Custom disambiguation might\n  decide to return this list if no change is necessary, reduce the list, or\n  extend it with new tokens. See the example bellow how to return list with a\n  token only if the default recognition doesn't succeed.   Returns:  a list of  Token  class instances or  None /empty list if no token\nis found.  To instantiate  Token  pass in the symbol and the value of the token. Value of\nthe token is usually a sub-string of the input string.  In the following test  Bar  and  Baz  non-terminals are fuzzy matched. The\nnon-terminal with the higher score wins but only if the score is above 0.7.  grammar = \"\"\"\nS: Element+ EOF;\nElement: Bar | Baz | Number;\nBar: /Bar. \\d+/;\nBaz: /Baz. \\d+/;\nNumber: /\\d+/;\n\"\"\"\n\ng = Grammar.from_string(grammar)\ngrammar = [g]\n\ndef custom_lexical_disambiguation(symbols, input_str, position, get_tokens):\n    \"\"\"\n    Lexical disambiguation should return a single token that is\n    recognized at the given place in the input string.\n    \"\"\"\n    # Call default token recognition.\n    tokens = get_tokens()\n\n    if tokens:\n        # If default recognition succeeds use the result.\n        return tokens\n    else:\n        # If no tokens are found do the fuzzy match.\n        matchers = [\n            lambda x: difflib.SequenceMatcher(None, 'bar.', x.lower()),\n            lambda x: difflib.SequenceMatcher(None, 'baz.', x.lower())\n        ]\n        symbols = [\n            grammar[0].get_terminal('Bar'),\n            grammar[0].get_terminal('Baz'),\n        ]\n        # Try to do fuzzy match at the position\n        elem = input_str[position:position+4]\n        elem_num = input_str[position:]\n        number_matcher = re.compile('[^\\d]*(\\d+)')\n        number_match = number_matcher.match(elem_num)\n        ratios = []\n        for matcher in matchers:\n            ratios.append(matcher(elem).ratio())\n\n        max_ratio_index = ratios.index(max(ratios))\n        if ratios[max_ratio_index] > 0.7 and number_match.group(1):\n            return [Token(symbols[max_ratio_index], number_match.group())]\n\n\nparser = Parser(\n    g, custom_lexical_disambiguation=custom_lexical_disambiguation)\n\n# Bar and Baz will be recognized by a fuzzy match\nresult = parser.parse('bar. 56 Baz 12')\nassert result == [['bar. 56', 'Baz 12'], None]\n\nresult = parser.parse('Buz. 34 bar 56')\nassert result == [['Buz. 34', 'bar 56'], None]\n\nresult = parser.parse('Ba. 34 baz 56')\nassert result == [['Ba. 34', 'baz 56'], None]\n\n# But if Bar/Baz are too different from the correct pattern\n# we get ParseError. In this case `bza` score is bellow 0.7\n# for both Bar and Baz symbols.\nwith pytest.raises(ParseError):\n    parser.parse('Bar. 34 bza 56')",
            "title": "Custom lexical disambiguation"
        },
        {
            "location": "/parse_trees/",
            "text": "Parse trees\n\u00b6\n\n\nDuring shift/reduce operations parser will call \nactions\n. If\n\nbuild_tree\n parser constructor parameter is set to \nTrue\n the default actions\nfor building parse tree nodes will be called. In the case of GLR parser multiple\ntrees can be built simultaneously (the parse forest).\n\n\nThe nodes of parse trees are instances of either \nNodeTerm\n for terminal nodes\n(leafs of the tree) or \nNodeNonTerm\n for non-terminal nodes (intermediate\nnodes).\n\n\nEach node of the tree has following attributes:\n\n\n\n\n\n\nstart_position/end_position\n - the start and end position in the input\n  stream where the node starts/ends. It is given in absolute 0-based offset. To\n  convert to line/column format for textual inputs you can use\n  \nparglare.pos_to_line_col(input_str, position)\n function which returns tuple\n  \n(line, column)\n. Of course, this call doesn't make any sense if you are\n  parsing a non-textual content.\n\n\n\n\n\n\nlayout_content\n -\n  the\n  \nlayout\n that\n  preceeds the given tree node. The layout consists of whitespaces/comments.\n\n\n\n\n\n\nsymbol\n - a grammar symbol this node is created for.\n\n\n\n\n\n\nAdditionally, each \nNodeTerm\n has:\n\n\n\n\nvalue\n - the value (a part of input_str) which this terminal represents. It\n  is equivalent to \ninput_str[start_position:end_position]\n.\n\n\n\n\nAdditionally, each \nNodeNonTerm\n has:\n\n\n\n\n\n\nchildren\n - sub-nodes which are also of \nNodeNonTerm\n/\nNodeTerm\n type.\n  \nNodeNonTerm\n is iterable. Iterating over it will iterate over its children.\n\n\n\n\n\n\nproduction\n - a grammar production whose reduction created this node.\n\n\n\n\n\n\nEach node has a \ntree_str()\n method which will return a string representation of\nthe sub-tree starting from the given node. If called on a root node it will\nreturn the string representation of the whole tree.\n\n\nFor example, parsing the input \n1 + 2 * 3 -1\n with the expression grammar from\nthe quick start will look like this if printed\nwith \ntree_str()\n:\n\n\nE[0]\nE[0]\n  E[0]\n    number[0, 1]\n  +[2, +]\n  E[4]\n    E[4]\n      number[4, 2]\n    *[6, *]\n    E[8]\n      number[8, 3]\n-[10, -]\nE[11]\n  number[11, 1]",
            "title": "Parse trees"
        },
        {
            "location": "/parse_trees/#parse-trees",
            "text": "During shift/reduce operations parser will call  actions . If build_tree  parser constructor parameter is set to  True  the default actions\nfor building parse tree nodes will be called. In the case of GLR parser multiple\ntrees can be built simultaneously (the parse forest).  The nodes of parse trees are instances of either  NodeTerm  for terminal nodes\n(leafs of the tree) or  NodeNonTerm  for non-terminal nodes (intermediate\nnodes).  Each node of the tree has following attributes:    start_position/end_position  - the start and end position in the input\n  stream where the node starts/ends. It is given in absolute 0-based offset. To\n  convert to line/column format for textual inputs you can use\n   parglare.pos_to_line_col(input_str, position)  function which returns tuple\n   (line, column) . Of course, this call doesn't make any sense if you are\n  parsing a non-textual content.    layout_content  -\n  the\n   layout  that\n  preceeds the given tree node. The layout consists of whitespaces/comments.    symbol  - a grammar symbol this node is created for.    Additionally, each  NodeTerm  has:   value  - the value (a part of input_str) which this terminal represents. It\n  is equivalent to  input_str[start_position:end_position] .   Additionally, each  NodeNonTerm  has:    children  - sub-nodes which are also of  NodeNonTerm / NodeTerm  type.\n   NodeNonTerm  is iterable. Iterating over it will iterate over its children.    production  - a grammar production whose reduction created this node.    Each node has a  tree_str()  method which will return a string representation of\nthe sub-tree starting from the given node. If called on a root node it will\nreturn the string representation of the whole tree.  For example, parsing the input  1 + 2 * 3 -1  with the expression grammar from\nthe quick start will look like this if printed\nwith  tree_str() :  E[0]\nE[0]\n  E[0]\n    number[0, 1]\n  +[2, +]\n  E[4]\n    E[4]\n      number[4, 2]\n    *[6, *]\n    E[8]\n      number[8, 3]\n-[10, -]\nE[11]\n  number[11, 1]",
            "title": "Parse trees"
        },
        {
            "location": "/handling_errors/",
            "text": "Handling errors\n\u00b6\n\n\nWhen parglare encounter a situation in which no SHIFT or REDUCE operation could\nbe performed it will report an error by raising an instance of\n\nparglare.ParseError\n class.\n\n\nParseError\n has the following attributes:\n\n\n\n\n\n\nfile_name\n - the name of the file being parsed (\nNone\n if string is\n  parsed),\n\n\n\n\n\n\nposition\n - an absolute position in the input where the error occured,\n\n\n\n\n\n\nline\n/\ncolumn\n - line and column where the error occured.\n\n\n\n\n\n\nIf there is an error in the grammar itself parglare will raise\n\nparglare.GrammarError\n exception.\n\n\nError recovery\n\u00b6\n\n\nThere are a lot of situations where you would want parser to report all the\nerrors in one go. To do this, parser has to recover from errors, i.e. get to\nthe valid state and continue.\n\n\nTo enable error recovery set\n\nerror_recovery\n \nparameter of parser construction\n\nto \nTrue\n. This will enable implicit error recovery strategy that will simply\ndrop characther/object at the place of the error and try to continue. All errors\nwill be collected as an \nerrors\n list on the parser instance.\n\n\nEach error is an instance of \nparglare.Error\n class. This class has the\nfollowing attributes:\n\n\n\n\nposition\n - an absolute position of the error,\n\n\nlength\n - the length of erroneous part of input,\n\n\nmessage\n - the error message,\n\n\ninput_str\n - the input string/list of objects,\n\n\nexpected_symbols\n - A set of grammar symbols expected at the location.\n\n\n\n\nwhich are supplied to the \nError\n constructor to build a new instance. Either a\n\nmessage\n or \ninput_str\n with \nexpected_symbols\n should be provided. If\n\nmessage\n is given it is used as a string representation of the error, otherwise\nthe message is constructed based on other parameters.\n\n\nposition\n can be converted to \nline, column\n by calling\n\nparglare.pos_to_line_col(input, position)\n.\n\n\nCustom recovery strategy\n\u00b6\n\n\nTo provide a custom strategy for error recovery set \nerror_recovery\n to a Python\nfunction. This function should have the following signature:\n\n\ndef error_recovery_strategy(parser, input, position, expected_symbols):\n    ...\n\n\n\n\n\nparser\n - instance of the parser, should be used only for querying the\n   configuration, not for altering the state,\n\n\ninput\n - the input string/list,\n\n\nposition\n - the position in the input where the error has been found,\n\n\nexpected_symbols\n - a set grammar symbols that are expected at this\n   position.\n\n\n\n\nThe recovery function should return the tuple \n(error, position, token)\n.\n\nerror\n should be an instance of \nparglare.Error\n class or \nNone\n if no error\nshould be reported. \nposition\n should be a new position where the parser should\ncontinue or \nNone\n if a new token is introduced. \ntoken\n should be a new token\nintroduced at the given position or \nNone\n if position is advanced.\n\n\nSo, either position should be advanced if input is skipped, or new token should\nbe returned if a new token is introduced at the position. You can't both\nadvance position and introduce new token at the same time.\n\n\nThe \ntoken\n symbol should be from the \nexpected_symbols\n for the parser to\nrecover successfully. \ntoken\n should be an instance\nof \nparser.Token\n. This class constructor accepts \nsymbol\n,\n\nvalue\n and \nlength\n. \nsymbol\n should a grammar symbol from \nexpected_symbols\n,\n\nvalue\n should be a matched part of the input (actually, in the context of error\nrecovery this value is a made-up value) and length is the length of the token.\nFor introduced tokens, length should be set to 0.",
            "title": "Handling errors"
        },
        {
            "location": "/handling_errors/#handling-errors",
            "text": "When parglare encounter a situation in which no SHIFT or REDUCE operation could\nbe performed it will report an error by raising an instance of parglare.ParseError  class.  ParseError  has the following attributes:    file_name  - the name of the file being parsed ( None  if string is\n  parsed),    position  - an absolute position in the input where the error occured,    line / column  - line and column where the error occured.    If there is an error in the grammar itself parglare will raise parglare.GrammarError  exception.",
            "title": "Handling errors"
        },
        {
            "location": "/handling_errors/#error-recovery",
            "text": "There are a lot of situations where you would want parser to report all the\nerrors in one go. To do this, parser has to recover from errors, i.e. get to\nthe valid state and continue.  To enable error recovery set error_recovery   parameter of parser construction \nto  True . This will enable implicit error recovery strategy that will simply\ndrop characther/object at the place of the error and try to continue. All errors\nwill be collected as an  errors  list on the parser instance.  Each error is an instance of  parglare.Error  class. This class has the\nfollowing attributes:   position  - an absolute position of the error,  length  - the length of erroneous part of input,  message  - the error message,  input_str  - the input string/list of objects,  expected_symbols  - A set of grammar symbols expected at the location.   which are supplied to the  Error  constructor to build a new instance. Either a message  or  input_str  with  expected_symbols  should be provided. If message  is given it is used as a string representation of the error, otherwise\nthe message is constructed based on other parameters.  position  can be converted to  line, column  by calling parglare.pos_to_line_col(input, position) .",
            "title": "Error recovery"
        },
        {
            "location": "/handling_errors/#custom-recovery-strategy",
            "text": "To provide a custom strategy for error recovery set  error_recovery  to a Python\nfunction. This function should have the following signature:  def error_recovery_strategy(parser, input, position, expected_symbols):\n    ...   parser  - instance of the parser, should be used only for querying the\n   configuration, not for altering the state,  input  - the input string/list,  position  - the position in the input where the error has been found,  expected_symbols  - a set grammar symbols that are expected at this\n   position.   The recovery function should return the tuple  (error, position, token) . error  should be an instance of  parglare.Error  class or  None  if no error\nshould be reported.  position  should be a new position where the parser should\ncontinue or  None  if a new token is introduced.  token  should be a new token\nintroduced at the given position or  None  if position is advanced.  So, either position should be advanced if input is skipped, or new token should\nbe returned if a new token is introduced at the position. You can't both\nadvance position and introduce new token at the same time.  The  token  symbol should be from the  expected_symbols  for the parser to\nrecover successfully.  token  should be an instance\nof  parser.Token . This class constructor accepts  symbol , value  and  length .  symbol  should a grammar symbol from  expected_symbols , value  should be a matched part of the input (actually, in the context of error\nrecovery this value is a made-up value) and length is the length of the token.\nFor introduced tokens, length should be set to 0.",
            "title": "Custom recovery strategy"
        },
        {
            "location": "/pglr/",
            "text": "The \npglr\n command\n\u00b6\n\n\npglr\n CLI command is available when parglare is installed. This command is used\nto debug the grammar, visualize the LR automata and make a visual trace of the\nGLR parsing.\n\n\nTo get the help on the command run:\n\n\n$ pglr --help\nUsage: pglr [OPTIONS] COMMAND [ARGS]...\n\n  Command line interface for working with parglare grammars.\n\nOptions:\n  --debug / --no-debug    Debug/trace output\n  --colors / --no-colors  Output coloring\n  --help                  Show this message and exit.\n\nCommands:\n  check\n  trace\n  viz\n\n\n\nChecking the grammar\n\u00b6\n\n\ncheck\n command is used for grammar checking. To get help on the command run:\n\n\n$ pglr check --help\nUsage: pglr check [OPTIONS] GRAMMAR_FILE\n\nOptions:\n  --help  Show this message and exit.\n\n\n\nTo check your grammar run:\n\n\n$ pglr check <grammar_file>\n\n\n\nwhere \n<grammar_file>\n is the path to your grammar file.\n\n\nIf there is no error in the grammar you will get \nGrammar OK.\n message. In case\nof error you will get error message with the information what is the error and\nwhere it is in the grammar.\n\n\nFor example:\n\n\n$ pglr check calc.pg\nError in the grammar file.\nError in file \"calc.pg\" at position 4,16 => \"/' E  left*, 2}\\n | E \".\nExpected: { or | or ; or Name or RegExTerm or StrTerm\n\n\n\nGetting detailed information\n\u00b6\n\n\nTo get the detailed information on the grammar run \npglr\n command in the debug\nmode.\n\n\n$ pglr --debug check calc.pg\n\n\n*** GRAMMAR ***\nTerminals:\nnumber STOP + - ^ EMPTY ) \\d+(\\.\\d+)? ( EOF / *\nNonTerminals:\nS' E\nProductions:\n0: S' = E STOP\n1: E = E + E\n2: E = E - E\n3: E = E * E\n4: E = E / E\n5: E = E ^ E\n6: E = ( E )\n7: E = number\n\n\n*** STATES ***\n\nState 0\n        0: S' = . E STOP   {}\n        1: E = . E + E   {STOP, -, +, ^, ), /, *}\n        2: E = . E - E   {STOP, -, +, ^, ), /, *}\n        3: E = . E * E   {STOP, -, +, ^, ), /, *}\n        4: E = . E / E   {STOP, -, +, ^, ), /, *}\n        5: E = . E ^ E   {STOP, -, +, ^, ), /, *}\n        6: E = . ( E )   {STOP, -, +, ^, ), /, *}\n        7: E = . number   {STOP, -, +, ^, ), /, *}\n\n\n\n    GOTO:\n     E->1\n\n    ACTIONS:\n     (->SHIFT:2, number->SHIFT:3\n\n...\n\n\n\nThis will give enumerated all the productions of your grammars and all the\nstates. For each state you get the LR items with lookahead, elements of GOTO\ntable and elements of ACTIONS table. In the previous example state 0 will have a\ntransition to state 1 when \nE\n is reduced, transition to state 2 if \n(\n can\nbe shifted and transition to state 3 if \nnumber\n can be shifted.\n\n\nIn addition you will get a detailed information on all Shift/Reduce and\nReduce/Reduce conflicts which makes much easier to see the exact cause of\nambiguity and to use \ndisambiguation rules\n\nto resolve the conflicts or to go with GLR if the grammar is not LR(1).\n\n\n\n\nNote\n\n\nYou can use \n--debug\n option with any \npglr\n command to put the parser\nin the debug mode and get a detailed output.\n\n\n\n\nVisualizing LR automata\n\u00b6\n\n\nTo visualize your automata with all the states and possible transitions run the\ncommand:\n\n\n$ pglr viz calc.pg\nGrammar OK.\nGenerating 'calc.pg.dot' file for the grammar PDA.\nUse dot viewer (e.g. xdot) or convert to pdf by running 'dot -Tpdf -O calc.pg.dot'\n\n\n\nAs given in the output you will get a \ndot\n file which represents LR automata\nvisualization. You can see this diagram using dot viewers\n(e.g. \nxdot\n) or you can transform it to\nother file formats using the \ndot\n tool (you'll have to install Graphviz\nsoftware for that).\n\n\nThis is an example of LR automata visualization for the \ncalc\n grammar from the\nquick intro (click on the image to enlarge):\n\n\n\n\nTracing GLR parsing\n\u00b6\n\n\nGLR parser uses a graph-like stack (\nGraph-Structured Stack - GSS\n) and to\nunderstand what's going on during GLR operation GLR parser and \npglr\n command\nprovide a way to trace the GSS.\n\n\nTo get a help on the \ntrace\n command run:\n\n\n$ pglr trace --help\nUsage: pglr trace [OPTIONS] GRAMMAR_FILE\n\nOptions:\n  -f, --input-file PATH  Input file for tracing\n  -i, --input TEXT       Input string for tracing\n  --help                 Show this message and exit.\n\n\n\nYou either give your input using file (\n-f\n) or using string provided in the\ncommand (\n-i\n), but not both.\n\n\nTo run the GLR trace for the calc grammar and some input:\n\n\n$ pglr trace calc.pg -i \"2 + 3 * 5\"\n\n\n\nThe \n-i\n switch tells the command to treat the last parameter as the input\nstring to parse.\n\n\n\n\nNote\n\n\nSince the GSS can be quite large and complex for larger inputs the advice is\nto use a minimal input that will exibit the intended behaviour for a\nvisualization to be usable.\n\n\n\n\nThe \ntrace\n sub-command implies \n--debug\n switch so the parser will run in the\ndebug mode and will produce the detailed output on the grammar, LR automata and\nthe parsing process.\n\n\nAdditionally, a \ndot\n file will be created, with the name \nparglare_trace.dot\n\nif input is given on command line or \n<input_file_name>_trace.dot\n if input is\ngiven as a file. The \ndot\n file can be visualized using dot viewers or\ntransformed to other file formats using the \ndot\n tool.\n\n\nFor the command above, GLR trace visualization will be (click on the image to\nenlarge):\n\n\n\n\nDotted red arrows represent each step in the parsing process. They are numbered\nconsecutively. After the ordinal number is the action (either \nS\n-Shift or\n\nR\n-reduce). For shift action a grammar symbol and the shifted value is given.\nFor reduction a production is given and the resulting head will have a parent\nnode closer to the beginning.\n\n\nBlack solid arrows are the links to the parent node in the GSS.\n\n\nThere are also dotted orange arrows (not shown in this example) that shows dropped\nempty reductions. Dropping happens when parser has found a better solution (i.e. a\nsolution with fewer empty reductions).\n\n\n\n\nNote\n\n\nTo produce GLR parser visual trace from code your must \nput the parser in\ndebug mode\n by setting \ndebug\n to \nTrue\n and enable visual\ntracing by setting \ndebug_trace\n to \nTrue\n.",
            "title": "pglr command"
        },
        {
            "location": "/pglr/#the-pglr-command",
            "text": "pglr  CLI command is available when parglare is installed. This command is used\nto debug the grammar, visualize the LR automata and make a visual trace of the\nGLR parsing.  To get the help on the command run:  $ pglr --help\nUsage: pglr [OPTIONS] COMMAND [ARGS]...\n\n  Command line interface for working with parglare grammars.\n\nOptions:\n  --debug / --no-debug    Debug/trace output\n  --colors / --no-colors  Output coloring\n  --help                  Show this message and exit.\n\nCommands:\n  check\n  trace\n  viz",
            "title": "The pglr command"
        },
        {
            "location": "/pglr/#checking-the-grammar",
            "text": "check  command is used for grammar checking. To get help on the command run:  $ pglr check --help\nUsage: pglr check [OPTIONS] GRAMMAR_FILE\n\nOptions:\n  --help  Show this message and exit.  To check your grammar run:  $ pglr check <grammar_file>  where  <grammar_file>  is the path to your grammar file.  If there is no error in the grammar you will get  Grammar OK.  message. In case\nof error you will get error message with the information what is the error and\nwhere it is in the grammar.  For example:  $ pglr check calc.pg\nError in the grammar file.\nError in file \"calc.pg\" at position 4,16 => \"/' E  left*, 2}\\n | E \".\nExpected: { or | or ; or Name or RegExTerm or StrTerm",
            "title": "Checking the grammar"
        },
        {
            "location": "/pglr/#getting-detailed-information",
            "text": "To get the detailed information on the grammar run  pglr  command in the debug\nmode.  $ pglr --debug check calc.pg  *** GRAMMAR ***\nTerminals:\nnumber STOP + - ^ EMPTY ) \\d+(\\.\\d+)? ( EOF / *\nNonTerminals:\nS' E\nProductions:\n0: S' = E STOP\n1: E = E + E\n2: E = E - E\n3: E = E * E\n4: E = E / E\n5: E = E ^ E\n6: E = ( E )\n7: E = number\n\n\n*** STATES ***\n\nState 0\n        0: S' = . E STOP   {}\n        1: E = . E + E   {STOP, -, +, ^, ), /, *}\n        2: E = . E - E   {STOP, -, +, ^, ), /, *}\n        3: E = . E * E   {STOP, -, +, ^, ), /, *}\n        4: E = . E / E   {STOP, -, +, ^, ), /, *}\n        5: E = . E ^ E   {STOP, -, +, ^, ), /, *}\n        6: E = . ( E )   {STOP, -, +, ^, ), /, *}\n        7: E = . number   {STOP, -, +, ^, ), /, *}\n\n\n\n    GOTO:\n     E->1\n\n    ACTIONS:\n     (->SHIFT:2, number->SHIFT:3\n\n...  This will give enumerated all the productions of your grammars and all the\nstates. For each state you get the LR items with lookahead, elements of GOTO\ntable and elements of ACTIONS table. In the previous example state 0 will have a\ntransition to state 1 when  E  is reduced, transition to state 2 if  (  can\nbe shifted and transition to state 3 if  number  can be shifted.  In addition you will get a detailed information on all Shift/Reduce and\nReduce/Reduce conflicts which makes much easier to see the exact cause of\nambiguity and to use  disambiguation rules \nto resolve the conflicts or to go with GLR if the grammar is not LR(1).   Note  You can use  --debug  option with any  pglr  command to put the parser\nin the debug mode and get a detailed output.",
            "title": "Getting detailed information"
        },
        {
            "location": "/pglr/#visualizing-lr-automata",
            "text": "To visualize your automata with all the states and possible transitions run the\ncommand:  $ pglr viz calc.pg\nGrammar OK.\nGenerating 'calc.pg.dot' file for the grammar PDA.\nUse dot viewer (e.g. xdot) or convert to pdf by running 'dot -Tpdf -O calc.pg.dot'  As given in the output you will get a  dot  file which represents LR automata\nvisualization. You can see this diagram using dot viewers\n(e.g.  xdot ) or you can transform it to\nother file formats using the  dot  tool (you'll have to install Graphviz\nsoftware for that).  This is an example of LR automata visualization for the  calc  grammar from the\nquick intro (click on the image to enlarge):",
            "title": "Visualizing LR automata"
        },
        {
            "location": "/pglr/#tracing-glr-parsing",
            "text": "GLR parser uses a graph-like stack ( Graph-Structured Stack - GSS ) and to\nunderstand what's going on during GLR operation GLR parser and  pglr  command\nprovide a way to trace the GSS.  To get a help on the  trace  command run:  $ pglr trace --help\nUsage: pglr trace [OPTIONS] GRAMMAR_FILE\n\nOptions:\n  -f, --input-file PATH  Input file for tracing\n  -i, --input TEXT       Input string for tracing\n  --help                 Show this message and exit.  You either give your input using file ( -f ) or using string provided in the\ncommand ( -i ), but not both.  To run the GLR trace for the calc grammar and some input:  $ pglr trace calc.pg -i \"2 + 3 * 5\"  The  -i  switch tells the command to treat the last parameter as the input\nstring to parse.   Note  Since the GSS can be quite large and complex for larger inputs the advice is\nto use a minimal input that will exibit the intended behaviour for a\nvisualization to be usable.   The  trace  sub-command implies  --debug  switch so the parser will run in the\ndebug mode and will produce the detailed output on the grammar, LR automata and\nthe parsing process.  Additionally, a  dot  file will be created, with the name  parglare_trace.dot \nif input is given on command line or  <input_file_name>_trace.dot  if input is\ngiven as a file. The  dot  file can be visualized using dot viewers or\ntransformed to other file formats using the  dot  tool.  For the command above, GLR trace visualization will be (click on the image to\nenlarge):   Dotted red arrows represent each step in the parsing process. They are numbered\nconsecutively. After the ordinal number is the action (either  S -Shift or R -reduce). For shift action a grammar symbol and the shifted value is given.\nFor reduction a production is given and the resulting head will have a parent\nnode closer to the beginning.  Black solid arrows are the links to the parent node in the GSS.  There are also dotted orange arrows (not shown in this example) that shows dropped\nempty reductions. Dropping happens when parser has found a better solution (i.e. a\nsolution with fewer empty reductions).   Note  To produce GLR parser visual trace from code your must  put the parser in\ndebug mode  by setting  debug  to  True  and enable visual\ntracing by setting  debug_trace  to  True .",
            "title": "Tracing GLR parsing"
        },
        {
            "location": "/debugging/",
            "text": "Tracing and debugging\n\u00b6\n\n\nWhen the parser doesn't work as expected there are several options. First, you\ncan use \npglr command\n to visualize LR PDA automata and GLR trace.\nThe same command can also be used to print detailed information on the grammar,\nLR states and conflicts.\n\n\nPrinting detailed debug information on grammar can be also achieved by putting\nthe grammar in the debug mode:\n\n\ngrammar = Grammar.from_file(file_name, debug=True)\n\n\n\nFor example, \ncalc\n grammar from the quick intro would give the following\noutput:\n\n\n*** GRAMMAR ***\nTerminals:\nEMPTY - * ^ + STOP ( \\d+(\\.\\d+)? number / EOF )\nNonTerminals:\nE S'\nProductions:\n0: S' = E STOP\n1: E = E + E\n2: E = E - E\n3: E = E * E\n4: E = E / E\n5: E = E ^ E\n6: E = ( E )\n7: E = number\n\n\n\nDuring grammar object construction, grammar file is parsed using the parglare\nitself and the grammar object for the new language is constructed. If you want\nto see the debug output of this process set the \nparse_debug\n parameter to\n\nTrue\n:\n\n\ngrammar = Grammar.from_file(file_name, parse_debug=True)\n\n\n\nIf you are using \ncustom recognizers\n or would like to see the\nresult of each \naction\n in debug output then you should put the\nparser in the debug mode from the code.\n\n\nTo put the parser in the debug mode do:\n\n\nparser = Parser(grammar, debug=True)\n\n\n\nTo debug \nlayout grammar\n do:\n\n\nparser = Parser(grammar, debug_layout=True)\n\n\n\nGLRParser\n can \nproduce visual trace\n. To enable\nvisual tracing set \ndebug\n and \ndebug_trace\n to \nTrue\n:\n\n\nparser = GLRParser(grammar, debug=True, debug_trace=True)\n\n\n\nDebug output and visual trace can be generated using \npglr command\n.\nFor example, parsing expression \n1 + 2 * 3\n with \nGLRParser\n in debug mode will\nproduce the following output (this output is generated by \npglr -i trace calc.pg\n\"1 + 2 * 3\"\n):\n\n\nGrammar OK.\n\n\n*** STATES ***\n\nState 0\n        0: S' = . E STOP   {}\n        1: E = . E + E   {STOP, ^, ), /, *, -, +}\n        2: E = . E - E   {STOP, ^, ), /, *, -, +}\n        3: E = . E * E   {STOP, ^, ), /, *, -, +}\n        4: E = . E / E   {STOP, ^, ), /, *, -, +}\n        5: E = . E ^ E   {STOP, ^, ), /, *, -, +}\n        6: E = . ( E )   {STOP, ^, ), /, *, -, +}\n        7: E = . number   {STOP, ^, ), /, *, -, +}\n\n\n\n        GOTO:\n        E->1\n\n        ACTIONS:\n        (->SHIFT:2, number->SHIFT:3\n\n....\n....\n\n*** PARSING STARTED\n\n        Skipping whitespaces: ''\n        New position: (1, 0)\n\n**REDUCING HEADS\nActive heads 1: [state=0:S', pos=0, endpos=0, empty=[False,True], parents=0, trees=1]\nNumber of trees = 1\n\nReducing head: state=0:S', pos=0, endpos=0, empty=[False,True], parents=0, trees=1\n        Skipping whitespaces: ''\n        New position: (1, 0)\n        Position: (1, 0)\n        Context: *1 + 2 * 3\n        Symbols expected: ['(', 'number']\n        Token(s) ahead: [<number(1)>]\n\n        New head for shifting: state=0:S', pos=0, endpos=0, token\n              ahead=<number(1)>, empty=[False,True], parents=0, trees=1.\n\n        No more reductions for this head and lookahead token <number(1)>.\n\n**SHIFTING HEADS\nActive heads 1: [state=0:S', pos=0, endpos=0, token ahead=<number(1)>,\n     empty=[False,True], parents=0, trees=1]\nNumber of trees = 1\n\nShifting head: state=0:S', pos=0, endpos=0, token ahead=<number(1)>,\n     empty=[False,True], parents=0, trees=1\n        Position: (1, 0)\n        Context: *1 + 2 * 3\n        Token(s) ahead: <number(1)>\n\n        Shift:3 \"1\" at position (1, 0)\n        Action result = type:<class 'parglare.parser.NodeTerm'>\n            value:<Term(start=0, end=1, sym=number, val=\"1\")>\n        New shifted head state=3:number, pos=0, endpos=1, empty=[False,True],\n            parents=0, trees=0.\n        Creating link   from head state=3:number, pos=0, endpos=1,\n                            empty=[False,False], parents=1, trees=1\n                  to head   state=0:S', pos=0, endpos=0, token\n                            ahead=<number(1)>, empty=[False,True],\n                            parents=0, trees=1\n ....\n ....\n\n Reducing head: state=1:E, pos=0, endpos=9, token ahead=<STOP()>,\n       empty=[False,False], parents=1, trees=1\n    Position: (1, 9)\n    Context: 1 + 2 * 3*\n    Symbols expected: ['STOP']\n    Token(s) ahead: <STOP()>\n\n    *** SUCCESS!!!!\n  *** 1 sucessful parse(s).\n  Generated file parglare_trace.dot.\n\n\n\nIn addition, a \nvisualization of GLR trace\n is\nproduced as a Graphviz dot file.",
            "title": "Debugging"
        },
        {
            "location": "/debugging/#tracing-and-debugging",
            "text": "When the parser doesn't work as expected there are several options. First, you\ncan use  pglr command  to visualize LR PDA automata and GLR trace.\nThe same command can also be used to print detailed information on the grammar,\nLR states and conflicts.  Printing detailed debug information on grammar can be also achieved by putting\nthe grammar in the debug mode:  grammar = Grammar.from_file(file_name, debug=True)  For example,  calc  grammar from the quick intro would give the following\noutput:  *** GRAMMAR ***\nTerminals:\nEMPTY - * ^ + STOP ( \\d+(\\.\\d+)? number / EOF )\nNonTerminals:\nE S'\nProductions:\n0: S' = E STOP\n1: E = E + E\n2: E = E - E\n3: E = E * E\n4: E = E / E\n5: E = E ^ E\n6: E = ( E )\n7: E = number  During grammar object construction, grammar file is parsed using the parglare\nitself and the grammar object for the new language is constructed. If you want\nto see the debug output of this process set the  parse_debug  parameter to True :  grammar = Grammar.from_file(file_name, parse_debug=True)  If you are using  custom recognizers  or would like to see the\nresult of each  action  in debug output then you should put the\nparser in the debug mode from the code.  To put the parser in the debug mode do:  parser = Parser(grammar, debug=True)  To debug  layout grammar  do:  parser = Parser(grammar, debug_layout=True)  GLRParser  can  produce visual trace . To enable\nvisual tracing set  debug  and  debug_trace  to  True :  parser = GLRParser(grammar, debug=True, debug_trace=True)  Debug output and visual trace can be generated using  pglr command .\nFor example, parsing expression  1 + 2 * 3  with  GLRParser  in debug mode will\nproduce the following output (this output is generated by  pglr -i trace calc.pg\n\"1 + 2 * 3\" ):  Grammar OK.\n\n\n*** STATES ***\n\nState 0\n        0: S' = . E STOP   {}\n        1: E = . E + E   {STOP, ^, ), /, *, -, +}\n        2: E = . E - E   {STOP, ^, ), /, *, -, +}\n        3: E = . E * E   {STOP, ^, ), /, *, -, +}\n        4: E = . E / E   {STOP, ^, ), /, *, -, +}\n        5: E = . E ^ E   {STOP, ^, ), /, *, -, +}\n        6: E = . ( E )   {STOP, ^, ), /, *, -, +}\n        7: E = . number   {STOP, ^, ), /, *, -, +}\n\n\n\n        GOTO:\n        E->1\n\n        ACTIONS:\n        (->SHIFT:2, number->SHIFT:3\n\n....\n....\n\n*** PARSING STARTED\n\n        Skipping whitespaces: ''\n        New position: (1, 0)\n\n**REDUCING HEADS\nActive heads 1: [state=0:S', pos=0, endpos=0, empty=[False,True], parents=0, trees=1]\nNumber of trees = 1\n\nReducing head: state=0:S', pos=0, endpos=0, empty=[False,True], parents=0, trees=1\n        Skipping whitespaces: ''\n        New position: (1, 0)\n        Position: (1, 0)\n        Context: *1 + 2 * 3\n        Symbols expected: ['(', 'number']\n        Token(s) ahead: [<number(1)>]\n\n        New head for shifting: state=0:S', pos=0, endpos=0, token\n              ahead=<number(1)>, empty=[False,True], parents=0, trees=1.\n\n        No more reductions for this head and lookahead token <number(1)>.\n\n**SHIFTING HEADS\nActive heads 1: [state=0:S', pos=0, endpos=0, token ahead=<number(1)>,\n     empty=[False,True], parents=0, trees=1]\nNumber of trees = 1\n\nShifting head: state=0:S', pos=0, endpos=0, token ahead=<number(1)>,\n     empty=[False,True], parents=0, trees=1\n        Position: (1, 0)\n        Context: *1 + 2 * 3\n        Token(s) ahead: <number(1)>\n\n        Shift:3 \"1\" at position (1, 0)\n        Action result = type:<class 'parglare.parser.NodeTerm'>\n            value:<Term(start=0, end=1, sym=number, val=\"1\")>\n        New shifted head state=3:number, pos=0, endpos=1, empty=[False,True],\n            parents=0, trees=0.\n        Creating link   from head state=3:number, pos=0, endpos=1,\n                            empty=[False,False], parents=1, trees=1\n                  to head   state=0:S', pos=0, endpos=0, token\n                            ahead=<number(1)>, empty=[False,True],\n                            parents=0, trees=1\n ....\n ....\n\n Reducing head: state=1:E, pos=0, endpos=9, token ahead=<STOP()>,\n       empty=[False,False], parents=1, trees=1\n    Position: (1, 9)\n    Context: 1 + 2 * 3*\n    Symbols expected: ['STOP']\n    Token(s) ahead: <STOP()>\n\n    *** SUCCESS!!!!\n  *** 1 sucessful parse(s).\n  Generated file parglare_trace.dot.  In addition, a  visualization of GLR trace  is\nproduced as a Graphviz dot file.",
            "title": "Tracing and debugging"
        },
        {
            "location": "/about/CONTRIBUTING/",
            "text": "Contributing\n\u00b6\n\n\nContributions are welcome, and they are greatly appreciated! Every\nlittle bit helps, and credit will always be given.\n\n\nYou can contribute in many ways:\n\n\nTypes of Contributions\n\u00b6\n\n\nReport Bugs\n\u00b6\n\n\nReport bugs at https://github.com/igordejanovic/parglare/issues.\n\n\nIf you are reporting a bug, please include:\n\n\n\n\nYour operating system name and version.\n\n\nAny details about your local setup that might be helpful in troubleshooting.\n\n\nDetailed steps to reproduce the bug.\n\n\n\n\nFix Bugs\n\u00b6\n\n\nLook through the GitHub issues for bugs. Anything tagged with \"bug\"\nand \"help wanted\" is open to whoever wants to implement it.\n\n\nImplement Features\n\u00b6\n\n\nLook through the GitHub issues for features. Anything tagged with \"enhancement\"\nand \"help wanted\" is open to whoever wants to implement it.\n\n\nWrite Documentation\n\u00b6\n\n\nparglare could always use more documentation, whether as part of the\nofficial parglare docs, in docstrings, or even on the web in blog posts,\narticles, and such.\n\n\nSubmit Feedback\n\u00b6\n\n\nThe best way to send feedback is to file an issue at https://github.com/igordejanovic/parglare/issues.\n\n\nIf you are proposing a feature:\n\n\n\n\nExplain in detail how it would work.\n\n\nKeep the scope as narrow as possible, to make it easier to implement.\n\n\nRemember that this is a volunteer-driven project, and that contributions\n  are welcome :)\n\n\n\n\nGet Started!\n\u00b6\n\n\nReady to contribute? Here's how to set up \nparglare\n for local development.\n\n\n\n\nFork the \nparglare\n repo on GitHub.\n\n\n\n\nClone your fork locally:\n\n\n$ git clone git@github.com:your_name_here/parglare.git\n\n\n\n\n\n\n\nInstall your local copy into a virtualenv. Assuming you have\n   virtualenvwrapper installed, this is how you set up your fork for local\n   development:\n\n\n$ mkvirtualenv parglare\n$ cd parglare/\n$ python setup.py develop\n\n\n\n\n\n\n\nCreate a branch for local development::\n\n\n$ git checkout -b name-of-your-bugfix-or-feature\n\n\n\n\n\n\n\nNow you can make your changes locally.\n\n\n\n\n\n\nWhen you're done making changes, check that your changes pass flake8 and the\n   tests, including testing other Python versions with tox:\n\n\n$ flake8 parglare tests\n$ py.test tests/func/\n\n\n\nTo run tests for all environments:\n\n\n$ tox\n\n\n\n\n\n\n\nTo get flake8 and tox, just pip install them into your virtualenv.\n\n\n\n\n\n\nCommit your changes and push your branch to GitHub:\n\n\n$ git add .\n$ git commit -m \"Your detailed description of your changes.\"\n$ git push origin name-of-your-bugfix-or-feature\n\n\n\n\n\n\n\nSubmit a pull request through the GitHub website.\n\n\n\n\n\n\nPull Request Guidelines\n\u00b6\n\n\nBefore you submit a pull request, check that it meets these guidelines:\n\n\n\n\nThe pull request should include tests.\n\n\nIf the pull request adds functionality, the docs should be updated. Put\n   your new functionality into a function with a docstring, and add the\n   feature to the list in README.rst.\n\n\nThe pull request should work for Python 2.7, 3.3-3.6, and for\n   PyPy. Check https://travis-ci.org/igordejanovic/parglare/pull_requests and\n   make sure that the tests pass for all supported Python versions.\n\n\n\n\nTips\n\u00b6\n\n\nTo run a subset of tests:\n\n\n$ py.test tests/func/mytest.py\n\n\n\n\nor a single test:\n\n\n$ py.test tests/func/mytest.py::some_test",
            "title": "Contributing"
        },
        {
            "location": "/about/CONTRIBUTING/#contributing",
            "text": "Contributions are welcome, and they are greatly appreciated! Every\nlittle bit helps, and credit will always be given.  You can contribute in many ways:",
            "title": "Contributing"
        },
        {
            "location": "/about/CONTRIBUTING/#types-of-contributions",
            "text": "",
            "title": "Types of Contributions"
        },
        {
            "location": "/about/CONTRIBUTING/#report-bugs",
            "text": "Report bugs at https://github.com/igordejanovic/parglare/issues.  If you are reporting a bug, please include:   Your operating system name and version.  Any details about your local setup that might be helpful in troubleshooting.  Detailed steps to reproduce the bug.",
            "title": "Report Bugs"
        },
        {
            "location": "/about/CONTRIBUTING/#fix-bugs",
            "text": "Look through the GitHub issues for bugs. Anything tagged with \"bug\"\nand \"help wanted\" is open to whoever wants to implement it.",
            "title": "Fix Bugs"
        },
        {
            "location": "/about/CONTRIBUTING/#implement-features",
            "text": "Look through the GitHub issues for features. Anything tagged with \"enhancement\"\nand \"help wanted\" is open to whoever wants to implement it.",
            "title": "Implement Features"
        },
        {
            "location": "/about/CONTRIBUTING/#write-documentation",
            "text": "parglare could always use more documentation, whether as part of the\nofficial parglare docs, in docstrings, or even on the web in blog posts,\narticles, and such.",
            "title": "Write Documentation"
        },
        {
            "location": "/about/CONTRIBUTING/#submit-feedback",
            "text": "The best way to send feedback is to file an issue at https://github.com/igordejanovic/parglare/issues.  If you are proposing a feature:   Explain in detail how it would work.  Keep the scope as narrow as possible, to make it easier to implement.  Remember that this is a volunteer-driven project, and that contributions\n  are welcome :)",
            "title": "Submit Feedback"
        },
        {
            "location": "/about/CONTRIBUTING/#get-started",
            "text": "Ready to contribute? Here's how to set up  parglare  for local development.   Fork the  parglare  repo on GitHub.   Clone your fork locally:  $ git clone git@github.com:your_name_here/parglare.git    Install your local copy into a virtualenv. Assuming you have\n   virtualenvwrapper installed, this is how you set up your fork for local\n   development:  $ mkvirtualenv parglare\n$ cd parglare/\n$ python setup.py develop    Create a branch for local development::  $ git checkout -b name-of-your-bugfix-or-feature    Now you can make your changes locally.    When you're done making changes, check that your changes pass flake8 and the\n   tests, including testing other Python versions with tox:  $ flake8 parglare tests\n$ py.test tests/func/  To run tests for all environments:  $ tox    To get flake8 and tox, just pip install them into your virtualenv.    Commit your changes and push your branch to GitHub:  $ git add .\n$ git commit -m \"Your detailed description of your changes.\"\n$ git push origin name-of-your-bugfix-or-feature    Submit a pull request through the GitHub website.",
            "title": "Get Started!"
        },
        {
            "location": "/about/CONTRIBUTING/#pull-request-guidelines",
            "text": "Before you submit a pull request, check that it meets these guidelines:   The pull request should include tests.  If the pull request adds functionality, the docs should be updated. Put\n   your new functionality into a function with a docstring, and add the\n   feature to the list in README.rst.  The pull request should work for Python 2.7, 3.3-3.6, and for\n   PyPy. Check https://travis-ci.org/igordejanovic/parglare/pull_requests and\n   make sure that the tests pass for all supported Python versions.",
            "title": "Pull Request Guidelines"
        },
        {
            "location": "/about/CONTRIBUTING/#tips",
            "text": "To run a subset of tests:  $ py.test tests/func/mytest.py  or a single test:  $ py.test tests/func/mytest.py::some_test",
            "title": "Tips"
        },
        {
            "location": "/about/LICENSE/",
            "text": "MIT License\n\n\nCopyright (c) 2016-2017, Igor R. Dejanovic\n\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of\nthis software and associated documentation files (the \"Software\"), to deal in\nthe Software without restriction, including without limitation the rights to\nuse, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\nthe Software, and to permit persons to whom the Software is furnished to do so,\nsubject to the following conditions:\n\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\nFOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\nCOPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\nIN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\nCONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.",
            "title": "License"
        }
    ]
}