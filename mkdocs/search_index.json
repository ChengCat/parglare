{
    "docs": [
        {
            "location": "/",
            "text": "A pure Python LR/GLR parser with integrated scanner.\n\n\n\n\nNote\n\n\nThe docs is work in progress. Stay tuned. :)\n\n\nUntil the docs is completed you can use \ntests\n and \nexamples\n as a good source\nof information on the possiblities and usage patterns.\n\n\n\n\nFeature highlights\n\u00b6\n\n\n\n\n\n\nScannerless parsing\n\n\nThere is no lexing as a separate phase. There is no separate lexer grammar.\nThe parser will try to reconize token during parsing at the given location.\nThis brings more parsing power as there are no lexical ambiguities\nintroduced by a separate lexing stage. You want variable names in your\nlanguage to be allowed to be like some of the keywords? No problem.\n\n\n\n\n\n\nGeneralized parsing - GLR\n\n\nparglare gives you powerful tools to see where non-determinism in your\ngrammar lies (the notorious shift-reduce and reduce-reduce conflicts) and\ngives detailed info on why that happened. In case your language needs\nnon-deterministic parsing \u2014 either it needs additional lookahead to decide\nor your language is inherently ambiguous \u2014 you can resort to the GLR\nalgorithm by a simple change of the parser class. The grammar stays the\nsame.\n\n\nIn the case of non-determinism (unability for a parser to deterministically\ndecide what to do) the parser will fork and investigate each possibility.\nEventualy, parsers that decided wrong will die leaving only the right one.\nIn case there are multiple interpretation of your input you will get all the\ntrees (a.k.a. \"the parse forest\").\n\n\n\n\n\n\nDeclarative associativity and priority rules\n\n\nThese problems arise a lot when building expression languages. Even a little\narithmetic expression as \n3 + 4 * 5 * 2\n have multiple interpretation\ndepending on the associativity and priority of operations. In parglare it is\neasy to specify these rules in the grammar (see the quick intro bellow\nor\n\nthe calc example\n).\n\n\n\n\n\n\nTracing/debuging, visualization and error reporting\n\n\nThere is an extensive support for grammar checking, debugging, automata\nvisualization, and parse tracing. Check out \npglr command\n.\n\n\n\n\n\n\nParsing arbitrary list of object\n\n\nparglare is not used only to parse the textual content. It can parse (create\na tree) of an arbitrary list of objects (numbers, bytes, whatever) based on\nthe common parglare grammar. For this you have to\ndefine \ntoken recognizers\n for your input stream. The\nbuilt-in recognizers are string and regex recognizers for parsing textual\ninputs. See \nrecognizers\n parameter to grammar construction in\nthe \ntest_parse_list_of_objects.py test\n.\n\n\n\n\n\n\nFlexible actions calling strategies\n\n\nDuring parsing you will want to do something when the grammar rule matches.\nThe whole point of parsing is that you want to transform your input to some\noutput. There are several options:\n- do nothing - this way your parser is a mere recognizer, it produces\n  nothing but just verifies that your input adhere to the grammar;\n- call default actions - the default actions will build a parse tree.\n- call user supplied actions - you write a Python function that is called\n  when the rule matches. You can do whatever you want at this place and the\n  result returned is used in parent rules/actions.\n\n\nBesides calling your actions in-line - during the parsing process - you can\ndecide to build the tree first and call custom actions afterwards. This is a\ngood option if you want to evaluate your tree in a multiple ways or if you\nare using GLR and want to be sure that actions are called only for the\nsurviving tree.\n\n\n\n\n\n\nSupport for whitespaces/comments\n\n\nSupport for language comments/whitespaces is done using the special rule\n\nLAYOUT\n. By default whitespaces are skipped. This is controlled by \nws\n\nparameter to the parser constructor which is by default set to \n\\t\\n\n. If\nset to \nNone\n no whitespace skipping is provided. If there is a rule\n\nLAYOUT\n in the grammar this rule is used instead. An additional parser with\nthe layout grammar will be built to handle whitespaces.\n\n\n\n\n\n\nError recovery\n\n\nThis is something that often lacks in parsing libraries. More often than not\nyou will want your parser to recover from an error, report it, and continue\nparsing. parglare has a built-in error recovery strategy which is currently\na simplistic one -- it will skip current character and try to continue --\nbut gives you possibility to provide your own. You will write a strategy\nthat will either skip input or introduce non-existing but expected tokens.\n\n\n\n\n\n\nTest coverage\n\n\nTest coverage is high and I'll try to keep it that way.\n\n\n\n\n\n\nTODO/Planed\n\u00b6\n\n\n\n\n\n\nDocs\n\n\nThis docs is currently work in progress. It should be done soon. Stay tuned.\n\n\n\n\n\n\nTable caching\n\n\nAt the moment parser tables are constructed on-the-fly which might be slow\nfor larger grammars. In the future tables will be recalculated only if the\ngrammar has changed and cached.\n\n\n\n\n\n\nSpecify common actions in the grammar\n\n\nparglare provides some commonly used custom actions. It would reduce\nboiler-plate in specification of these actions if a syntax is added to provide\nthat information in the grammar directly.\n\n\nExample:\n\n\n@collect\nsome_objects: some_objects some_object | some_object;\n\n\n\n\n\n\n\nSupport for named matches\n\n\nAt the moment, as a parameter to action you get a list of matched elements. It\nwould be useful to reference these element by name rather than by position.\n\n\nmy_rule: first=first_match_rule second=second_match_rule;\nfirst_match_rule: ...;\nsecond_match_rule: ...;\n\n\n\nNow in your action for \nmy_rule\n you will get \nfirst\n and \nsecond\n as a parameters.\nThis would make it easy to provide a new common action that will return a Python\nobject with supplied parameters as object attributes.\n\n\n\n\n\n\nGLR performance optimization\n\n\nThe GLR parsing has a lot of overhead compared to LR which makes it slower.\nThere are some technique that could be used to cut on this difference in\nspeed.\n\n\n\n\n\n\nQuick intro\n\u00b6\n\n\nThis is just a small example to get the general idea. This example shows how to\nparse and evaluate expressions with 5 operations with different priority and\nassociativity. Evaluation is done using semantic/reduction actions.\n\n\nThe whole expression evaluator is done in under 30 lines of code!\n\n\nfrom parglare import Parser, Grammar\n\ngrammar = r\"\"\"\nE: E '+' E  {left, 1}\n | E '-' E  {left, 1}\n | E '*' E  {left, 2}\n | E '/' E  {left, 2}\n | E '^' E  {right, 3}\n | '(' E ')'\n | number;\nnumber: /\\d+(\\.\\d+)?/;\n\"\"\"\n\nactions = {\n    \"E\": [lambda _, nodes: nodes[0] + nodes[2],\n          lambda _, nodes: nodes[0] - nodes[2],\n          lambda _, nodes: nodes[0] * nodes[2],\n          lambda _, nodes: nodes[0] / nodes[2],\n          lambda _, nodes: nodes[0] ** nodes[2],\n          lambda _, nodes: nodes[1],\n          lambda _, nodes: nodes[0]],\n    \"number\": lambda _, value: float(value),\n}\n\ng = Grammar.from_string(grammar)\nparser = Parser(g, debug=True, actions=actions)\n\nresult = parser.parse(\"34 + 4.6 / 2 * 4^2^2 + 78\")\n\nprint(\"Result = \", result)\n\n# Output\n# -- Debuging/tracing output with detailed info about grammar, productions,\n# -- terminals and nonterminals, DFA states, parsing progress,\n# -- and at the end of the output:\n# Result = 700.8\n\n\n\n\nNote on LR tables calculation\n\u00b6\n\n\nparglare provides both SLR and LALR tables calculation (LALR is the default).\nLALR is modified to avoid REDUCE/REDUCE conflicts on state merging. Although\nnot proven, this should enable handling of all LR(1) grammars with reduced set\nof states and without conflicts. For grammars that are not LR(1) a GLR parsing\nis provided.\n\n\nNote on lexical disambiguation\n\u00b6\n\n\nLexical ambiguity arise if multiple recognizers match at the same location.\nLexical disambiguation is done in the following order:\n\n\n\n\nPriorities are used first.\n\n\nString recognizers are preferred over regexes (i.e. the most specific match).\n\n\nThe longest-match strategy is used if multiple regexes matches with the same\n  priority. For further disambiguation if longest-match fails \nprefer\n rule\n  may be given in terminal definition.\n\n\n\n\nNotice that, since the scanning is integrated into parser, the lexical ambiguity\nmay arise only if there is a real ambiguity in the language definition, i.e. you\nmight have two or more tokens recognized at some place during parsing. For\napproaches where scanning is separate, lexical ambiguities arise whenever you\nhave token recognition overlap no matter what the context is, which is a real\nPITA.\n\n\nWhat does \nparglare\n mean?\n\u00b6\n\n\nIt is an amalgam of the words \nparser\n and \nglare\n where the second word is\nchosen to contain letters GLR and to be easy for pronunciation. I also like one\nof the translations for the word - \nto be very bright and intense\n\n(by \nThe Free Dictionary\n)\n\n\nOh, and the name is non-generic and unique which make it easy to find on the\nnet. ;)\n\n\nThe project logo is my take (not very successful) on drawing the Hydra, a\ncreature with multiple heads from the Greek mythology. According to the legend,\nthe Hydra had a regeneration feature: for every head chopped off, the Hydra\nwould regrow a couple of new heads. That reminds me a lot of the GLR parsing ;)\n\n\nLicense\n\u00b6\n\n\nMIT\n\n\nPython versions\n\u00b6\n\n\nTested with 2.7, 3.3-3.6",
            "title": "Home"
        },
        {
            "location": "/#feature-highlights",
            "text": "Scannerless parsing  There is no lexing as a separate phase. There is no separate lexer grammar.\nThe parser will try to reconize token during parsing at the given location.\nThis brings more parsing power as there are no lexical ambiguities\nintroduced by a separate lexing stage. You want variable names in your\nlanguage to be allowed to be like some of the keywords? No problem.    Generalized parsing - GLR  parglare gives you powerful tools to see where non-determinism in your\ngrammar lies (the notorious shift-reduce and reduce-reduce conflicts) and\ngives detailed info on why that happened. In case your language needs\nnon-deterministic parsing \u2014 either it needs additional lookahead to decide\nor your language is inherently ambiguous \u2014 you can resort to the GLR\nalgorithm by a simple change of the parser class. The grammar stays the\nsame.  In the case of non-determinism (unability for a parser to deterministically\ndecide what to do) the parser will fork and investigate each possibility.\nEventualy, parsers that decided wrong will die leaving only the right one.\nIn case there are multiple interpretation of your input you will get all the\ntrees (a.k.a. \"the parse forest\").    Declarative associativity and priority rules  These problems arise a lot when building expression languages. Even a little\narithmetic expression as  3 + 4 * 5 * 2  have multiple interpretation\ndepending on the associativity and priority of operations. In parglare it is\neasy to specify these rules in the grammar (see the quick intro bellow\nor the calc example ).    Tracing/debuging, visualization and error reporting  There is an extensive support for grammar checking, debugging, automata\nvisualization, and parse tracing. Check out  pglr command .    Parsing arbitrary list of object  parglare is not used only to parse the textual content. It can parse (create\na tree) of an arbitrary list of objects (numbers, bytes, whatever) based on\nthe common parglare grammar. For this you have to\ndefine  token recognizers  for your input stream. The\nbuilt-in recognizers are string and regex recognizers for parsing textual\ninputs. See  recognizers  parameter to grammar construction in\nthe  test_parse_list_of_objects.py test .    Flexible actions calling strategies  During parsing you will want to do something when the grammar rule matches.\nThe whole point of parsing is that you want to transform your input to some\noutput. There are several options:\n- do nothing - this way your parser is a mere recognizer, it produces\n  nothing but just verifies that your input adhere to the grammar;\n- call default actions - the default actions will build a parse tree.\n- call user supplied actions - you write a Python function that is called\n  when the rule matches. You can do whatever you want at this place and the\n  result returned is used in parent rules/actions.  Besides calling your actions in-line - during the parsing process - you can\ndecide to build the tree first and call custom actions afterwards. This is a\ngood option if you want to evaluate your tree in a multiple ways or if you\nare using GLR and want to be sure that actions are called only for the\nsurviving tree.    Support for whitespaces/comments  Support for language comments/whitespaces is done using the special rule LAYOUT . By default whitespaces are skipped. This is controlled by  ws \nparameter to the parser constructor which is by default set to  \\t\\n . If\nset to  None  no whitespace skipping is provided. If there is a rule LAYOUT  in the grammar this rule is used instead. An additional parser with\nthe layout grammar will be built to handle whitespaces.    Error recovery  This is something that often lacks in parsing libraries. More often than not\nyou will want your parser to recover from an error, report it, and continue\nparsing. parglare has a built-in error recovery strategy which is currently\na simplistic one -- it will skip current character and try to continue --\nbut gives you possibility to provide your own. You will write a strategy\nthat will either skip input or introduce non-existing but expected tokens.    Test coverage  Test coverage is high and I'll try to keep it that way.",
            "title": "Feature highlights"
        },
        {
            "location": "/#todoplaned",
            "text": "Docs  This docs is currently work in progress. It should be done soon. Stay tuned.    Table caching  At the moment parser tables are constructed on-the-fly which might be slow\nfor larger grammars. In the future tables will be recalculated only if the\ngrammar has changed and cached.    Specify common actions in the grammar  parglare provides some commonly used custom actions. It would reduce\nboiler-plate in specification of these actions if a syntax is added to provide\nthat information in the grammar directly.  Example:  @collect\nsome_objects: some_objects some_object | some_object;    Support for named matches  At the moment, as a parameter to action you get a list of matched elements. It\nwould be useful to reference these element by name rather than by position.  my_rule: first=first_match_rule second=second_match_rule;\nfirst_match_rule: ...;\nsecond_match_rule: ...;  Now in your action for  my_rule  you will get  first  and  second  as a parameters.\nThis would make it easy to provide a new common action that will return a Python\nobject with supplied parameters as object attributes.    GLR performance optimization  The GLR parsing has a lot of overhead compared to LR which makes it slower.\nThere are some technique that could be used to cut on this difference in\nspeed.",
            "title": "TODO/Planed"
        },
        {
            "location": "/#quick-intro",
            "text": "This is just a small example to get the general idea. This example shows how to\nparse and evaluate expressions with 5 operations with different priority and\nassociativity. Evaluation is done using semantic/reduction actions.  The whole expression evaluator is done in under 30 lines of code!  from parglare import Parser, Grammar\n\ngrammar = r\"\"\"\nE: E '+' E  {left, 1}\n | E '-' E  {left, 1}\n | E '*' E  {left, 2}\n | E '/' E  {left, 2}\n | E '^' E  {right, 3}\n | '(' E ')'\n | number;\nnumber: /\\d+(\\.\\d+)?/;\n\"\"\"\n\nactions = {\n    \"E\": [lambda _, nodes: nodes[0] + nodes[2],\n          lambda _, nodes: nodes[0] - nodes[2],\n          lambda _, nodes: nodes[0] * nodes[2],\n          lambda _, nodes: nodes[0] / nodes[2],\n          lambda _, nodes: nodes[0] ** nodes[2],\n          lambda _, nodes: nodes[1],\n          lambda _, nodes: nodes[0]],\n    \"number\": lambda _, value: float(value),\n}\n\ng = Grammar.from_string(grammar)\nparser = Parser(g, debug=True, actions=actions)\n\nresult = parser.parse(\"34 + 4.6 / 2 * 4^2^2 + 78\")\n\nprint(\"Result = \", result)\n\n# Output\n# -- Debuging/tracing output with detailed info about grammar, productions,\n# -- terminals and nonterminals, DFA states, parsing progress,\n# -- and at the end of the output:\n# Result = 700.8",
            "title": "Quick intro"
        },
        {
            "location": "/#note-on-lr-tables-calculation",
            "text": "parglare provides both SLR and LALR tables calculation (LALR is the default).\nLALR is modified to avoid REDUCE/REDUCE conflicts on state merging. Although\nnot proven, this should enable handling of all LR(1) grammars with reduced set\nof states and without conflicts. For grammars that are not LR(1) a GLR parsing\nis provided.",
            "title": "Note on LR tables calculation"
        },
        {
            "location": "/#note-on-lexical-disambiguation",
            "text": "Lexical ambiguity arise if multiple recognizers match at the same location.\nLexical disambiguation is done in the following order:   Priorities are used first.  String recognizers are preferred over regexes (i.e. the most specific match).  The longest-match strategy is used if multiple regexes matches with the same\n  priority. For further disambiguation if longest-match fails  prefer  rule\n  may be given in terminal definition.   Notice that, since the scanning is integrated into parser, the lexical ambiguity\nmay arise only if there is a real ambiguity in the language definition, i.e. you\nmight have two or more tokens recognized at some place during parsing. For\napproaches where scanning is separate, lexical ambiguities arise whenever you\nhave token recognition overlap no matter what the context is, which is a real\nPITA.",
            "title": "Note on lexical disambiguation"
        },
        {
            "location": "/#what-does-parglare-mean",
            "text": "It is an amalgam of the words  parser  and  glare  where the second word is\nchosen to contain letters GLR and to be easy for pronunciation. I also like one\nof the translations for the word -  to be very bright and intense \n(by  The Free Dictionary )  Oh, and the name is non-generic and unique which make it easy to find on the\nnet. ;)  The project logo is my take (not very successful) on drawing the Hydra, a\ncreature with multiple heads from the Greek mythology. According to the legend,\nthe Hydra had a regeneration feature: for every head chopped off, the Hydra\nwould regrow a couple of new heads. That reminds me a lot of the GLR parsing ;)",
            "title": "What does parglare mean?"
        },
        {
            "location": "/#license",
            "text": "MIT",
            "title": "License"
        },
        {
            "location": "/#python-versions",
            "text": "Tested with 2.7, 3.3-3.6",
            "title": "Python versions"
        },
        {
            "location": "/getting_started/",
            "text": "Getting started\n\u00b6\n\n\nThe first thing to do is to write your language grammar using\nthe \nparglare grammar language\n. You write the grammar either as a\nPython string in your source code or as a separate file. In case you are writing\na grammar of a complex language I would suggest the separate file approach.\n\n\nThe next step is to create the instance of the \nGrammar\n class. This is achieved\nby importing the \nGrammar\n class and calling either \nfrom_file\n or \nfrom_str\n\nmethods supplying the file name for the former and the Python string for the\nlater call.\n\n\nfrom parglare import Grammar\n\nfile_name = .....\ngrammar = Grammar.from_file(file_name)\n\n\n\n\nIf there is no errors in the grammar you now have the grammar instance.\n\n\n\n\nNote\n\n\nThere is also a handy \npglr command line tool\n that can be\nused for grammar checking, visualization and debugging.\n\n\n\n\nThe next step is to create an instance of the parser. There are two options. If\nyou want to use LR parser instantiate \nParser\n class. For GLR instantiate\n\nGLRParser\n class.\n\n\nfrom parglare import Parser\nparser = Parser(grammar)\n\n\n\n\nor\n\n\nfrom parglare import GLRParser\nparser = GLRParser(grammar)\n\n\n\n\nYou can provide additional \nTODO:parser parameters\n during instantiation.\n\n\n\n\nNote\n\n\nLR parser is faster as the GLR machinery brings a significant overhead. So,\nthe general advice is to stick to the LR parsing until you are sure that you\nneed additional power of GLR, i.e. either you need more than one token of\nlookahead or your language is inherently ambiguous. pglr tool will help you in\ninvestigating why you have LR conflicts in your grammar and there are some\nnice \ndisambiguation features\n in parglare\nthat will help you resolve some of those conflicts.\n\n\n\n\nNow parse your input calling \nparse\n method on the parser instance.\n\n\nresult = parser.parse(input_str)\n\n\n\n\nDepending on whether you have configured \nactions\n or not you will\nget a parse tree or some other representation of your input. In case of the GLR\nparser you will get the list of all possible results (a.k.a. \nthe parse\nforest\n).",
            "title": "Getting started"
        },
        {
            "location": "/getting_started/#getting-started",
            "text": "The first thing to do is to write your language grammar using\nthe  parglare grammar language . You write the grammar either as a\nPython string in your source code or as a separate file. In case you are writing\na grammar of a complex language I would suggest the separate file approach.  The next step is to create the instance of the  Grammar  class. This is achieved\nby importing the  Grammar  class and calling either  from_file  or  from_str \nmethods supplying the file name for the former and the Python string for the\nlater call.  from parglare import Grammar\n\nfile_name = .....\ngrammar = Grammar.from_file(file_name)  If there is no errors in the grammar you now have the grammar instance.   Note  There is also a handy  pglr command line tool  that can be\nused for grammar checking, visualization and debugging.   The next step is to create an instance of the parser. There are two options. If\nyou want to use LR parser instantiate  Parser  class. For GLR instantiate GLRParser  class.  from parglare import Parser\nparser = Parser(grammar)  or  from parglare import GLRParser\nparser = GLRParser(grammar)  You can provide additional  TODO:parser parameters  during instantiation.   Note  LR parser is faster as the GLR machinery brings a significant overhead. So,\nthe general advice is to stick to the LR parsing until you are sure that you\nneed additional power of GLR, i.e. either you need more than one token of\nlookahead or your language is inherently ambiguous. pglr tool will help you in\ninvestigating why you have LR conflicts in your grammar and there are some\nnice  disambiguation features  in parglare\nthat will help you resolve some of those conflicts.   Now parse your input calling  parse  method on the parser instance.  result = parser.parse(input_str)  Depending on whether you have configured  actions  or not you will\nget a parse tree or some other representation of your input. In case of the GLR\nparser you will get the list of all possible results (a.k.a.  the parse\nforest ).",
            "title": "Getting started"
        },
        {
            "location": "/grammar/",
            "text": "The parglare grammar language\n\u00b6\n\n\nThe parglare grammar specification language is based\non \nBNF\n. parglare is\nbased\non\n\nContext-Free Grammars (CFGs)\n and\na grammar is written declaratively. You don't have to think about the parsing\nprocess like in\ne.g. \nPEGs\n.\nAmbiguities are dealt with explicitely (see section on conflicts...).\n\n\nGrammar consists of a set of derivation rules where each rule is of the form:\n\n\n<symbol>: <expression> ;\n\n\n\n\nwhere \n<symbol>\n is grammar non-terminal and \n<expression>\n is a sequence of\nterminals and non-terminals separated by choice operator \n|\n.\n\n\nFor example:\n\n\nFields: Field | Fields \",\" Field;\n\n\n\n\nHere \nFields\n is a non-terminal grammar symbol and it is defined as either a\nsingle \nField\n or, recursively, as \nFields\n followed by a string terminal \n,\n\nand than by another \nField\n. It is not given here but \nField\n could also be\ndefined as a non-terminal. For example:\n\n\nField: QuotedField | FieldContent;\n\n\n\n\nOr it could be defined as a terminal:\n\n\nField: /[A-Z]*/;\n\n\n\n\nThis terminal definition uses regular expression recognizer.\n\n\n\n\nNote\n\n\nIf you got use to various BNF extensions\n(like \nKleene star\n) you might find\nthis awkward because you must build \nzero or more\n or \none or more\n pattern from\nscratch using just a sequence, choice and recursion. The grammars are indeed\nmore verbose but, on the other hand, actions are much easier to write and you\nhave full control over tree construction process. parglare might provide some\nsyntactic sugar later that would make some constructs shorter to write.\n\n\n\n\nTerminals\n\u00b6\n\n\nTerminal symbols of the grammar define the fundamental or atomic elements of\nyour language -- tokens or lexemes (e.g. keywords, numbers). In parglare\nterminal is connected to recognizer which is an object used to recognize token\nof particular type in the input. Most of the time you will do parsing of textual\ncontent and you will need textual recognizers. These recognizers are built-in\nand there are two type of textual recognizers:\n\n\n\n\nstring recognizer\n\n\nregular expression recognizer\n\n\n\n\nString recognizer\n\u00b6\n\n\nString recognizer is defined as a plain string inside of double quotes:\n\n\nmy_rule: \"start\" other_rule \"end\";\n\n\n\nIn this example \n\"start\"\n and \n\"end\"\n will be terminals with string recognizers\nthat match exactly the words \nstart\n and \nend\n.\n\n\nYou can write string recognizing terminal directly in the rule expression or you\ncan define terminal separately and reference it by name, like:\n\n\nmy_rule: start other_rule end;\nstart: \"start\";\nend: \"end\";\n\n\n\nEither way it will be the same terminal. You will usually write as a separate\nterminal if the terminal is used at multiple places in the grammar.\n\n\nRegular expression recognizer\n\u00b6\n\n\nOr regex recognizer for short is a regex pattern written inside slashes\n(\n/.../\n).\n\n\nFor example:\n\n\n number: /\\d+/;\n\n\n\nThis rule defines terminal symbol \nnumber\n which has a regex recognizer and will\nrecognize one or more digits as a number.\n\n\nCustom recognizers\n\u00b6\n\n\nIf you are parsing arbitrary input (non-textual) you'll have to provide your own\nrecognizers. In the grammar, you just have to reference your terminal symbol but\nyou don't have to provide the definition. You will provide missing recognizers\nduring grammar instantiation from Python.\n\n\nLets say that we have a list of integers (real list of Python ints, not a text\nwith numbers) and we have some weird requirement to break those numbers\naccording to the following grammar:\n\n\n  Numbers: all_less_than_five  ascending  all_less_than_five EOF;\n  all_less_than_five: all_less_than_five  int_less_than_five\n                    | int_less_than_five;\n\n\n\nSo, we should first match all numbers less than five and collect those, than we\nshould match a list of ascending numbers and than list of less than five again.\n\nint_less_than_five\n and \nascending\n are terminals/recognizers that will be\ndefined in Python and passed to grammar construction. \nint_less_than_five\n will\nrecognize Python integer that is less than five. \nascending\n will recognize a\nsublist of integers in ascending order.\n\n\nFor more details on the usage see \nthis test\n.\n\n\nMore on this topic will be written in a separate section.\n\n\n\n\nNote\n\n\nYou can directly write regex or string recognizer at the place of terminal:\n\n\nsome_rule: \"a\" aterm \"a\";\naterm: \"a\";\n\n\n\nWritting \n\"a\"\n in \nsome_rule\n is equivalent to writing terminal reference\n\naterm\n. Rule \naterm\n is terminal definition rule. All occurences of \n\"a\"\n\nas well as \naterm\n will result in the same terminal in the grammar.\n\n\n\n\nUsual patterns\n\u00b6\n\n\nOne or more\n\u00b6\n\n\ndocument: sections;\n// sections rule bellow will match one or more section.\nsections: sections section | section;\n\n\n\nIn this example \nsections\n will match one or more \nsection\n. Notice the\nrecursive definition of the rule. You can read this as \nsections\n consist of\nsections and a section at the end or \nsections\n is just a single section\n.\n\n\n\n\nNote\n\n\nPlease note that you could do the same with this rule:\n\n\nsections: section sections | section;\n\n\n\nwhich will give you similar result but the resulting tree will be different.\nFormer example will reduce sections early and than add another section to it,\nthus the tree will be expanding to the left. The later example will collect all\nthe sections and than start reducing from the end, thus building a tree\nexpanding to the right. These are subtle differences that are important when you\nstart writing your semantic actions. Most of the time you don't care about this\nso use the first version as it is slightly efficient and parglare provides\ncommon actions for these common cases.\n\n\n\n\nZero or more\n\u00b6\n\n\ndocument: sections;\n// sections rule bellow will match zero or more section.\nsections: sections section | section | EMPTY;\n\n\n\nIn this example \nsections\n will match zero or more \nsection\n. Notice the\naddition of the \nEMPTY\n choice at the end. This means that matching nothing is a\nvalid \nsections\n non-terminal.\n\n\nSame note from above applies here to.\n\n\nOptional\n\u00b6\n\n\ndocument: optheader body;\noptheader: header | EMPTY;\n\n\n\nIn this example \noptheader\n is either a header or nothing.\n\n\nGrammar comments\n\u00b6\n\n\nIn grammar comments are available as both line comments and block comments:\n\n\n// This is a line comment. Everything from the '//' to the end of line is a comment.\n\n/*\n  This is a block comment.\n  Everything in between `/*`  and '*/' is a comment.\n*/\n\n\n\nHandling whitespaces and comments\n\u00b6\n\n\nBy default parser will skip whitespaces. Whitespace skipping is controlled by\n\nws\n parameter to the parser which is by default set to \n'\\n\\t '\n.\n\n\nIf you need more control of the layout, i.e. handling of not only whitespaces by\ncomments also, you can use a special rule \nLAYOUT\n:\n\n\n  LAYOUT: LayoutItem | LAYOUT LayoutItem;\n  LayoutItem: WS | Comment | EMPTY;\n  WS: /\\s+/;\n  Comment: /\\/\\/.*/;\n\n\n\nThis will form a separate layout parser that will parse in-between each matched\ntokens. In this example spaces and line-comments will get consumed by the layout\nparser.\n\n\nIf this special rule is found in the grammar \nws\n parser parameter is ignored.\n\n\nAnother example that gives support for both line comments and block comments\nlike the one used in the grammar language itself:\n\n\n  LAYOUT: LayoutItem | LAYOUT LayoutItem;\n  LayoutItem: WS | Comment | EMPTY;\n  WS: /\\s+/;\n  Comment: '/*' CorNCs '*/' | /\\/\\/.*/;\n  CorNCs: CorNC | CorNCs CorNC | EMPTY;\n  CorNC: Comment | NotComment | WS;\n  NotComment: /((\\*[^\\/])|[^\\s*\\/]|\\/[^\\*])+/;",
            "title": "Grammar"
        },
        {
            "location": "/grammar/#the-parglare-grammar-language",
            "text": "The parglare grammar specification language is based\non  BNF . parglare is\nbased\non Context-Free Grammars (CFGs)  and\na grammar is written declaratively. You don't have to think about the parsing\nprocess like in\ne.g.  PEGs .\nAmbiguities are dealt with explicitely (see section on conflicts...).  Grammar consists of a set of derivation rules where each rule is of the form:  <symbol>: <expression> ;  where  <symbol>  is grammar non-terminal and  <expression>  is a sequence of\nterminals and non-terminals separated by choice operator  | .  For example:  Fields: Field | Fields \",\" Field;  Here  Fields  is a non-terminal grammar symbol and it is defined as either a\nsingle  Field  or, recursively, as  Fields  followed by a string terminal  , \nand than by another  Field . It is not given here but  Field  could also be\ndefined as a non-terminal. For example:  Field: QuotedField | FieldContent;  Or it could be defined as a terminal:  Field: /[A-Z]*/;  This terminal definition uses regular expression recognizer.   Note  If you got use to various BNF extensions\n(like  Kleene star ) you might find\nthis awkward because you must build  zero or more  or  one or more  pattern from\nscratch using just a sequence, choice and recursion. The grammars are indeed\nmore verbose but, on the other hand, actions are much easier to write and you\nhave full control over tree construction process. parglare might provide some\nsyntactic sugar later that would make some constructs shorter to write.",
            "title": "The parglare grammar language"
        },
        {
            "location": "/grammar/#terminals",
            "text": "Terminal symbols of the grammar define the fundamental or atomic elements of\nyour language -- tokens or lexemes (e.g. keywords, numbers). In parglare\nterminal is connected to recognizer which is an object used to recognize token\nof particular type in the input. Most of the time you will do parsing of textual\ncontent and you will need textual recognizers. These recognizers are built-in\nand there are two type of textual recognizers:   string recognizer  regular expression recognizer",
            "title": "Terminals"
        },
        {
            "location": "/grammar/#string-recognizer",
            "text": "String recognizer is defined as a plain string inside of double quotes:  my_rule: \"start\" other_rule \"end\";  In this example  \"start\"  and  \"end\"  will be terminals with string recognizers\nthat match exactly the words  start  and  end .  You can write string recognizing terminal directly in the rule expression or you\ncan define terminal separately and reference it by name, like:  my_rule: start other_rule end;\nstart: \"start\";\nend: \"end\";  Either way it will be the same terminal. You will usually write as a separate\nterminal if the terminal is used at multiple places in the grammar.",
            "title": "String recognizer"
        },
        {
            "location": "/grammar/#regular-expression-recognizer",
            "text": "Or regex recognizer for short is a regex pattern written inside slashes\n( /.../ ).  For example:   number: /\\d+/;  This rule defines terminal symbol  number  which has a regex recognizer and will\nrecognize one or more digits as a number.",
            "title": "Regular expression recognizer"
        },
        {
            "location": "/grammar/#custom-recognizers",
            "text": "If you are parsing arbitrary input (non-textual) you'll have to provide your own\nrecognizers. In the grammar, you just have to reference your terminal symbol but\nyou don't have to provide the definition. You will provide missing recognizers\nduring grammar instantiation from Python.  Lets say that we have a list of integers (real list of Python ints, not a text\nwith numbers) and we have some weird requirement to break those numbers\naccording to the following grammar:    Numbers: all_less_than_five  ascending  all_less_than_five EOF;\n  all_less_than_five: all_less_than_five  int_less_than_five\n                    | int_less_than_five;  So, we should first match all numbers less than five and collect those, than we\nshould match a list of ascending numbers and than list of less than five again. int_less_than_five  and  ascending  are terminals/recognizers that will be\ndefined in Python and passed to grammar construction.  int_less_than_five  will\nrecognize Python integer that is less than five.  ascending  will recognize a\nsublist of integers in ascending order.  For more details on the usage see  this test .  More on this topic will be written in a separate section.   Note  You can directly write regex or string recognizer at the place of terminal:  some_rule: \"a\" aterm \"a\";\naterm: \"a\";  Writting  \"a\"  in  some_rule  is equivalent to writing terminal reference aterm . Rule  aterm  is terminal definition rule. All occurences of  \"a\" \nas well as  aterm  will result in the same terminal in the grammar.",
            "title": "Custom recognizers"
        },
        {
            "location": "/grammar/#usual-patterns",
            "text": "",
            "title": "Usual patterns"
        },
        {
            "location": "/grammar/#one-or-more",
            "text": "document: sections;\n// sections rule bellow will match one or more section.\nsections: sections section | section;  In this example  sections  will match one or more  section . Notice the\nrecursive definition of the rule. You can read this as  sections  consist of\nsections and a section at the end or  sections  is just a single section .   Note  Please note that you could do the same with this rule:  sections: section sections | section;  which will give you similar result but the resulting tree will be different.\nFormer example will reduce sections early and than add another section to it,\nthus the tree will be expanding to the left. The later example will collect all\nthe sections and than start reducing from the end, thus building a tree\nexpanding to the right. These are subtle differences that are important when you\nstart writing your semantic actions. Most of the time you don't care about this\nso use the first version as it is slightly efficient and parglare provides\ncommon actions for these common cases.",
            "title": "One or more"
        },
        {
            "location": "/grammar/#zero-or-more",
            "text": "document: sections;\n// sections rule bellow will match zero or more section.\nsections: sections section | section | EMPTY;  In this example  sections  will match zero or more  section . Notice the\naddition of the  EMPTY  choice at the end. This means that matching nothing is a\nvalid  sections  non-terminal.  Same note from above applies here to.",
            "title": "Zero or more"
        },
        {
            "location": "/grammar/#optional",
            "text": "document: optheader body;\noptheader: header | EMPTY;  In this example  optheader  is either a header or nothing.",
            "title": "Optional"
        },
        {
            "location": "/grammar/#grammar-comments",
            "text": "In grammar comments are available as both line comments and block comments:  // This is a line comment. Everything from the '//' to the end of line is a comment.\n\n/*\n  This is a block comment.\n  Everything in between `/*`  and '*/' is a comment.\n*/",
            "title": "Grammar comments"
        },
        {
            "location": "/grammar/#handling-whitespaces-and-comments",
            "text": "By default parser will skip whitespaces. Whitespace skipping is controlled by ws  parameter to the parser which is by default set to  '\\n\\t ' .  If you need more control of the layout, i.e. handling of not only whitespaces by\ncomments also, you can use a special rule  LAYOUT :    LAYOUT: LayoutItem | LAYOUT LayoutItem;\n  LayoutItem: WS | Comment | EMPTY;\n  WS: /\\s+/;\n  Comment: /\\/\\/.*/;  This will form a separate layout parser that will parse in-between each matched\ntokens. In this example spaces and line-comments will get consumed by the layout\nparser.  If this special rule is found in the grammar  ws  parser parameter is ignored.  Another example that gives support for both line comments and block comments\nlike the one used in the grammar language itself:    LAYOUT: LayoutItem | LAYOUT LayoutItem;\n  LayoutItem: WS | Comment | EMPTY;\n  WS: /\\s+/;\n  Comment: '/*' CorNCs '*/' | /\\/\\/.*/;\n  CorNCs: CorNC | CorNCs CorNC | EMPTY;\n  CorNC: Comment | NotComment | WS;\n  NotComment: /((\\*[^\\/])|[^\\s*\\/]|\\/[^\\*])+/;",
            "title": "Handling whitespaces and comments"
        },
        {
            "location": "/actions/",
            "text": "Actions\n\u00b6\n\n\nActions (a.k.a. \nsemantic actions\n or \nreductions actions\n) are Python callables\n(functions or lambdas mostly) that get called to reduce the recognized pattern\nto some higher concept. E.g. in the calc example actions are called to calculate\nsubexpressions.\n\n\nThere are two consideration to think of:\n\n\n\n\nWhich actions are called?\n\n\nWhen actions are called?\n\n\n\n\nCustom actions and default actions\n\u00b6\n\n\nIf you don't provide actions of your own the default parglare actions will build\nthe parse tree.\n\n\nActions are provided to the parser during parser instantiation as \nactions\n\nparameter which must be a Python dict where the keys are the names of the rules\nfrom the grammar and values are the action callables or a list of callables if\nthe rule has more than one production/choice.\n\n\nLets take a closer look at the quick intro example:\n\n\ngrammar = r\"\"\"\nE: E '+' E  {left, 1}\n | E '-' E  {left, 1}\n | E '*' E  {left, 2}\n | E '/' E  {left, 2}\n | E '^' E  {right, 3}\n | '(' E ')'\n | number;\nnumber: /\\d+(\\.\\d+)?/;\n\"\"\"\n\nactions = {\n    \"E\": [lambda _, nodes: nodes[0] + nodes[2],\n          lambda _, nodes: nodes[0] - nodes[2],\n          lambda _, nodes: nodes[0] * nodes[2],\n          lambda _, nodes: nodes[0] / nodes[2],\n          lambda _, nodes: nodes[0] ** nodes[2],\n          lambda _, nodes: nodes[1],\n          lambda _, nodes: nodes[0]],\n    \"number\": lambda _, value: float(value),\n}\n\ng = Grammar.from_string(grammar)\nparser = Parser(g, actions=actions)\nresult = parser.parse(\"34 + 4.6 / 2 * 4^2^2 + 78\")\n\n\n\nHere you can see that for rule \nE\n we provide a list of lambdas, one lambda for\neach operation. The first element of the list corresponds to the first\nproduction of the \nE\n rule (\nE '+' E {left, 1}\n), the second to the second and\nso on. For \nnumber\n rule there is only a single lambda which converts the\nmatched string to the Python \nfloat\n type, because \nnumber\n has only a single\nproduction. Actually, \nnumber\n is a terminal definition and thus the second\nparameter in action call will not be a list but a matched value itself. At the\nend we instantiate the parser and pass in our \nactions\n using the parameter.\n\n\nEach action callable receive two parameters. The first is the context object\nwhich gives parsing context information (like the start and end position where\nthe match occured, the parser instance etc.). The second parameters \nnodes\n is a\nlist of actual results of subexpressions given in the order defined in the grammar.\n\n\nFor example:\n\n\nlambda _, nodes: nodes[0] * nodes[2],\n\n\n\nIn this line we don't care about context thus giving it the \n_\n name. \nnodes[0]\n\nwill cary the value of the left subexpression while \nnodes[2]\n will carry the\nresult of the right subexpression. \nnodes[1]\n must be \n*\n and we don't need to\ncheck that as the parser already did that for us.\n\n\nThe result of the parsing will be the evaluated expression as the actions will\nget called along the way and the result of each actions will be used as an\nelement of the \nnodes\n parameter in calling actions higher in the hierarchy.\n\n\nIf we don't provide \nactions\n the default will be used. The default parglare\nactions build a \nparse tree\n whose elements are instances of\n\nNodeNonTerm\n and \nNodeTerm\n classes representing a non-terminals and terminals\nrespectively.\n\n\nIf we set \ndefault_actions\n parser parameter to \nFalse\n and don't provide\nactions, no actions will be called making the parser a mere recognizer, i.e.\nparser will parse the input and return nothing if parse is successful or raise\n\nParseError\n if there is an error in the input.\n\n\nTime of actions call\n\u00b6\n\n\nIn parglare actions can be called during parsing (i.e. on the fly) which you\ncould use if you want to transform input imediately without building the parse\ntree. But there are times when you would like to build a tree first and call\nactions afterwards. For example, you might want to do several different\ntransformation of the tree. Or, another very good reason is, you are using GLR\nand you want to be sure that actions are called only on the final tree.\n\n\n\n\nNote\n\n\nIf you are using GLR be sure that your actions has no side-effects, as the\ndying parsers will left those side-effects behind leading to unpredictable\nbehaviour. In case of doubt create trees first, choose the right one and\ncall actions afterwards with the \ncall_actions\n parser method.\n\n\n\n\nTo get the tree and call actions afterwards you don't supply \nactions\n parameter\nto the parser. That way you get the parse tree as a result (or a list of parse\ntrees in case of GLR). After that you call \ncall_actions\n method on the parser\ngiving it the root of the tree and the actions dict:\n\n\nparser = Parser(g)\ntree = parser.parse(\"34 + 4.6 / 2 * 4^2^2 + 78\")\nresult = parser.call_actions(tree, actions=actions)\n\n\n\nThe Context object\n\u00b6\n\n\nThe first parameter passed to the action function is the Context object. This\nobject provides us with the parsing context of where the match occured.\n\n\nThese attributes are available on the context object:\n\n\n\n\n\n\nstart_position\n/\nend_position\n - the beginning and the end in the input\n  stream where the match occured. \nstart_position\n is the location of the first\n  element/character in the input while the \nend_position\n is one past the last\n  element/character of the match. Thus \nend_position - start_position\n will give\n  the lenght of the match including the layout. You can use\n  \nparglare.pos_to_line_col(input, position)\n function to get line and column of\n  the position. This function returns a tuple \n(line, column)\n.\n\n\n\n\n\n\nlayout_content\n - is the layout (whitespaces, comments etc.) that are\n  collected from the previous non-layout match. Default actions will attach this\n  layout to the tree node.\n\n\n\n\n\n\nsymbol\n - the grammar symbol this match is for.\n\n\n\n\n\n\nproduction\n - an instance of \nparglare.grammar.Production\n class available\n  only on reduction actions (not on shifts). Represents the grammar production.\n\n\n\n\n\n\nnode\n - this is available only if the actions are called over the parse tree\n  using \ncall_actions\n. It represens the instance of \nNodeNonTerm\n or \nNodeTerm\n\n  classes from the parse tree where the actions is executed.\n\n\n\n\n\n\nparser\n - is the reference to the parser instance. You should use this only\n  to investigate parser configuration not to alter its state.\n\n\n\n\n\n\nYou can also use context object to pass information between lower level and\nupper level actions. You can attach any attribute you like, the context object\nis shared between action calls. It is shared with the internal layout parser\ntoo.",
            "title": "Actions"
        },
        {
            "location": "/actions/#actions",
            "text": "Actions (a.k.a.  semantic actions  or  reductions actions ) are Python callables\n(functions or lambdas mostly) that get called to reduce the recognized pattern\nto some higher concept. E.g. in the calc example actions are called to calculate\nsubexpressions.  There are two consideration to think of:   Which actions are called?  When actions are called?",
            "title": "Actions"
        },
        {
            "location": "/actions/#custom-actions-and-default-actions",
            "text": "If you don't provide actions of your own the default parglare actions will build\nthe parse tree.  Actions are provided to the parser during parser instantiation as  actions \nparameter which must be a Python dict where the keys are the names of the rules\nfrom the grammar and values are the action callables or a list of callables if\nthe rule has more than one production/choice.  Lets take a closer look at the quick intro example:  grammar = r\"\"\"\nE: E '+' E  {left, 1}\n | E '-' E  {left, 1}\n | E '*' E  {left, 2}\n | E '/' E  {left, 2}\n | E '^' E  {right, 3}\n | '(' E ')'\n | number;\nnumber: /\\d+(\\.\\d+)?/;\n\"\"\"\n\nactions = {\n    \"E\": [lambda _, nodes: nodes[0] + nodes[2],\n          lambda _, nodes: nodes[0] - nodes[2],\n          lambda _, nodes: nodes[0] * nodes[2],\n          lambda _, nodes: nodes[0] / nodes[2],\n          lambda _, nodes: nodes[0] ** nodes[2],\n          lambda _, nodes: nodes[1],\n          lambda _, nodes: nodes[0]],\n    \"number\": lambda _, value: float(value),\n}\n\ng = Grammar.from_string(grammar)\nparser = Parser(g, actions=actions)\nresult = parser.parse(\"34 + 4.6 / 2 * 4^2^2 + 78\")  Here you can see that for rule  E  we provide a list of lambdas, one lambda for\neach operation. The first element of the list corresponds to the first\nproduction of the  E  rule ( E '+' E {left, 1} ), the second to the second and\nso on. For  number  rule there is only a single lambda which converts the\nmatched string to the Python  float  type, because  number  has only a single\nproduction. Actually,  number  is a terminal definition and thus the second\nparameter in action call will not be a list but a matched value itself. At the\nend we instantiate the parser and pass in our  actions  using the parameter.  Each action callable receive two parameters. The first is the context object\nwhich gives parsing context information (like the start and end position where\nthe match occured, the parser instance etc.). The second parameters  nodes  is a\nlist of actual results of subexpressions given in the order defined in the grammar.  For example:  lambda _, nodes: nodes[0] * nodes[2],  In this line we don't care about context thus giving it the  _  name.  nodes[0] \nwill cary the value of the left subexpression while  nodes[2]  will carry the\nresult of the right subexpression.  nodes[1]  must be  *  and we don't need to\ncheck that as the parser already did that for us.  The result of the parsing will be the evaluated expression as the actions will\nget called along the way and the result of each actions will be used as an\nelement of the  nodes  parameter in calling actions higher in the hierarchy.  If we don't provide  actions  the default will be used. The default parglare\nactions build a  parse tree  whose elements are instances of NodeNonTerm  and  NodeTerm  classes representing a non-terminals and terminals\nrespectively.  If we set  default_actions  parser parameter to  False  and don't provide\nactions, no actions will be called making the parser a mere recognizer, i.e.\nparser will parse the input and return nothing if parse is successful or raise ParseError  if there is an error in the input.",
            "title": "Custom actions and default actions"
        },
        {
            "location": "/actions/#time-of-actions-call",
            "text": "In parglare actions can be called during parsing (i.e. on the fly) which you\ncould use if you want to transform input imediately without building the parse\ntree. But there are times when you would like to build a tree first and call\nactions afterwards. For example, you might want to do several different\ntransformation of the tree. Or, another very good reason is, you are using GLR\nand you want to be sure that actions are called only on the final tree.   Note  If you are using GLR be sure that your actions has no side-effects, as the\ndying parsers will left those side-effects behind leading to unpredictable\nbehaviour. In case of doubt create trees first, choose the right one and\ncall actions afterwards with the  call_actions  parser method.   To get the tree and call actions afterwards you don't supply  actions  parameter\nto the parser. That way you get the parse tree as a result (or a list of parse\ntrees in case of GLR). After that you call  call_actions  method on the parser\ngiving it the root of the tree and the actions dict:  parser = Parser(g)\ntree = parser.parse(\"34 + 4.6 / 2 * 4^2^2 + 78\")\nresult = parser.call_actions(tree, actions=actions)",
            "title": "Time of actions call"
        },
        {
            "location": "/actions/#the-context-object",
            "text": "The first parameter passed to the action function is the Context object. This\nobject provides us with the parsing context of where the match occured.  These attributes are available on the context object:    start_position / end_position  - the beginning and the end in the input\n  stream where the match occured.  start_position  is the location of the first\n  element/character in the input while the  end_position  is one past the last\n  element/character of the match. Thus  end_position - start_position  will give\n  the lenght of the match including the layout. You can use\n   parglare.pos_to_line_col(input, position)  function to get line and column of\n  the position. This function returns a tuple  (line, column) .    layout_content  - is the layout (whitespaces, comments etc.) that are\n  collected from the previous non-layout match. Default actions will attach this\n  layout to the tree node.    symbol  - the grammar symbol this match is for.    production  - an instance of  parglare.grammar.Production  class available\n  only on reduction actions (not on shifts). Represents the grammar production.    node  - this is available only if the actions are called over the parse tree\n  using  call_actions . It represens the instance of  NodeNonTerm  or  NodeTerm \n  classes from the parse tree where the actions is executed.    parser  - is the reference to the parser instance. You should use this only\n  to investigate parser configuration not to alter its state.    You can also use context object to pass information between lower level and\nupper level actions. You can attach any attribute you like, the context object\nis shared between action calls. It is shared with the internal layout parser\ntoo.",
            "title": "The Context object"
        },
        {
            "location": "/recognizers/",
            "text": "Recognizers\n\u00b6\n\n\nParglare uses scannerless parsing. Actually, scanner is integrated in the\nparser. Each token is created/recognized in the input during parsing using so\ncalled \nrecognizer\n which is connected to the grammar terminal symbol.\n\n\nThis gives a great flexibility to the parglare.\n\n\nFirst, recognizing tokens during parsing eliminate lexical ambiguities that\narise in separate scanning due to the lack of parsing context.\n\n\nSecond, having a separate recognizers for grammar terminal symbols allows us to\nparse not only text but a stream of anything as parsing is nothing more by\nconstructing a tree (or some other form) out of a flat list of objects. Those\nobjects are characters if text is parsed, but don't have to be.\n\n\nParglare has two built-in recognizers for textual parsing that can be specified\nin \nthe grammar directly\n. Those are usually\nenough if text is parsed, but if non-textual content is parsed you will have to\nsupply your own recognizers that are able to recognize tokens in the input\nstream of objects.\n\n\nRecognizers are Python callables of the following form:\n\n\ndef some_recognizer(input, pos):\n   ...\n   ...\n   return part of input starting at pos\n\n\n\nFor example if we have an input stream of objects that are comparable (e.g.\nnumbers) and we want to recognize the ascending elements starting at the given\nposition but such that the recognized token must have at least two object from\nthe input. We could write following:\n\n\ndef ascending_nosingle(input, pos):\n    \"Match sublist of ascending elements. Matches at least two.\"\n    last = pos + 1\n    while last < len(input) and input[last] > input[last-1]:\n        last += 1\n    if last - pos >= 2:\n        return input[pos:last]\n\n\n\nWe register our recognizers during grammar contstruction. All references in the\ngrammar rules that don't exists in the grammar (i.e. they are not the rule\nthemself) must be resolved as recognizers or the exception will be thrown during\ngrammar construction.\n\n\nIn order to do that, create a Python dict where the key will be a rule name used\nin the grammar references and the value will be recognizer callable.\n\n\nrecognizers = {\n   'ascending': ascending_nosingle\n}\n\ngrammar = Grammar.from_file('mygrammar.pg', recognizers=recognizers)\n\n\n\nNow, in the grammar in file \nmygrammar.pg\n you can reference terminal rule\n\nascending\n (see the key in \nrecognizers\n dict) although it is not defined in\nthe grammar itself. This terminal rule will match ascending sublist of objects\nfrom the input.\n\n\n\n\nNote\n\n\nIf you want more information you could investigate\n\ntest_parse_list_of_objects.py\n test.",
            "title": "Recognizers"
        },
        {
            "location": "/recognizers/#recognizers",
            "text": "Parglare uses scannerless parsing. Actually, scanner is integrated in the\nparser. Each token is created/recognized in the input during parsing using so\ncalled  recognizer  which is connected to the grammar terminal symbol.  This gives a great flexibility to the parglare.  First, recognizing tokens during parsing eliminate lexical ambiguities that\narise in separate scanning due to the lack of parsing context.  Second, having a separate recognizers for grammar terminal symbols allows us to\nparse not only text but a stream of anything as parsing is nothing more by\nconstructing a tree (or some other form) out of a flat list of objects. Those\nobjects are characters if text is parsed, but don't have to be.  Parglare has two built-in recognizers for textual parsing that can be specified\nin  the grammar directly . Those are usually\nenough if text is parsed, but if non-textual content is parsed you will have to\nsupply your own recognizers that are able to recognize tokens in the input\nstream of objects.  Recognizers are Python callables of the following form:  def some_recognizer(input, pos):\n   ...\n   ...\n   return part of input starting at pos  For example if we have an input stream of objects that are comparable (e.g.\nnumbers) and we want to recognize the ascending elements starting at the given\nposition but such that the recognized token must have at least two object from\nthe input. We could write following:  def ascending_nosingle(input, pos):\n    \"Match sublist of ascending elements. Matches at least two.\"\n    last = pos + 1\n    while last < len(input) and input[last] > input[last-1]:\n        last += 1\n    if last - pos >= 2:\n        return input[pos:last]  We register our recognizers during grammar contstruction. All references in the\ngrammar rules that don't exists in the grammar (i.e. they are not the rule\nthemself) must be resolved as recognizers or the exception will be thrown during\ngrammar construction.  In order to do that, create a Python dict where the key will be a rule name used\nin the grammar references and the value will be recognizer callable.  recognizers = {\n   'ascending': ascending_nosingle\n}\n\ngrammar = Grammar.from_file('mygrammar.pg', recognizers=recognizers)  Now, in the grammar in file  mygrammar.pg  you can reference terminal rule ascending  (see the key in  recognizers  dict) although it is not defined in\nthe grammar itself. This terminal rule will match ascending sublist of objects\nfrom the input.   Note  If you want more information you could investigate test_parse_list_of_objects.py  test.",
            "title": "Recognizers"
        },
        {
            "location": "/conflicts/",
            "text": "LR parsing\n\u00b6\n\n\nLR parser operates as a deterministic PDA (Push-down automata). It is a state\nmachine which is always in some state during parsing. The state machine must\ndeterministically decide what is the next state just based on its current state\nand one token of lookahead. This decision is given by LR tables which are\nprecalculated from the grammar before parsing even begins.\n\n\nFor example, let's see what happens if we have a simple expression grammar:\n\n\nE: E '+' E\n | E '*' E\n | number;\nnumber: /\\d+/;\n\n\n\nand we want to parse the following input:\n\n\n 1 + 2 * 3\n\n\n\nLanguage defined by this grammar is ambiguous as the expression can be\ninterpreted either as:\n\n\n ((1 + 2) * 3)\n\n\n\nor:\n\n\n (1 + (2 * 3))\n\n\n\nIn the parsing process, parser starts in state 0 and it sees token \n1\n ahead\n(one lookahead is used - LR(1)).\n\n\nThe only thing a parser can do in this case is to shift, i.e. to consume the\ntoken and advance the position. This operation transition the automata to some\nother state. From each state there is only one valid transition that can be\ntaken or the PDA won't be deterministic, i.e. we could simultaneously follow\ndifferent paths.\n\n\nCurrent position would be (the dot represents the position):\n\n\n 1 . + 2 * 3\n\n\n\nNow the parser sees \n+\n token ahead and the tables will tell him to reduce the\nnumber he just saw to \nE\n (a number is an expression according to the grammar).\nThus, on the stack the parser will have an expression \nE\n (actually LR states\nare kept on stack but that's not important for this little analysis). This\nreduction will advace PDA to some other state again. Each shift/reduce operation\nchange state so I'll not repeat that anymore.\n\n\n\n\nNote\n\n\nSee \npglr command\n which can be used to visualize PDA. Try to\nvisualize automata for this grammar.\n\n\n\n\nAfter reduction parser will do shift of \n+\n token. There is nothing to reduce as\nthe subexpresison on stack is \nE +\n which can't be reduced as it's not complete.\nSo, the only thing we can do is to shift \n2\n token.\n\n\nNow, the position is:\n\n\n  1 + 2 . * 3\n\n\n\nAnd the stack is:\n\n\n  E + 2\n\n\n\nAnd this is a place where the parser can't decide what to do. It can either\nreduce the sum on the stack or shift \n*\n and \n3\n and reduce multiplication\nfirst and the sumation afterwards.\n\n\nIf the sum is reduced first and \n*\n shifted afterwards we would get the\nfollowing result:\n\n\n (1 + 2) * 3\n\n\n\nIf the shift of \n*\n and \n3\n is done instead of reducing, the reduction would\nfirst reduce multiplication and than sum (reduction is always done on the top of\nthe stack). We will have the following result:\n\n\n1 + (2 * 3)\n\n\n\nFrom the point of arithmetic priorities, preffered solution is the last one but\nthe parser don't know arithmentic rules.\n\n\nIf you analyze this grammar using \npglr command\n you will see that\nthe LR tables have Shift/Reduce conflicts as there is a state in which parser\ncan't decide wether to shift or to reduce (we just saw that situation).\n\n\nparglare gives you various tools to be more explicit with your grammar and to\nresolve those conflicts.\n\n\nThere are two situations when conflicts can't be resolved:\n\n\n\n\nyou need more than one lookahead to disambiguate\n\n\nyour language is inherently ambiguous\n\n\n\n\nIf you end up in one of these situations you should use GLR parsing, which will\nfork the parser in state which has multiple path, and explore all possibilities.\n\n\nResolving conflicts\n\u00b6\n\n\nWhen we run:\n\n\n$ pglr -d check expr.pg\n\n\n\nwhere in \nexpr.pg\n we have the above grammar, we get the following output at the end:\n\n\n*** S/R conflicts ***\nThere are 4 S/R conflicts\n\nState 6\n        1: E = E + E .   {+, *, STOP}\n        1: E = E . + E   {+, *, STOP}\n        2: E = E . * E   {+, *, STOP}\n\nIn state 6 and input symbol '+' can't decide whether to shift or\nreduce by production(s) '1: E = E + E'.\n\nState 6\n        1: E = E + E .   {+, *, STOP}\n        1: E = E . + E   {+, *, STOP}\n        2: E = E . * E   {+, *, STOP}\n\nIn state 6 and input symbol '*' can't decide whether to shift or\nreduce by production(s) '1: E = E + E'.\n\nState 7\n        2: E = E * E .   {+, *, STOP}\n        1: E = E . + E   {+, *, STOP}\n        2: E = E . * E   {+, *, STOP}\n\nIn state 7 and input symbol '+' can't decide whether to shift or\nreduce by production(s) '2: E = E * E'.\n\nState 7\n        2: E = E * E .   {+, *, STOP}\n        1: E = E . + E   {+, *, STOP}\n        2: E = E . * E   {+, *, STOP}\n\nIn state 7 and input symbol '*' can't decide whether to shift or\nreduce by production(s) '2: E = E * E'.\nGrammar OK.\nThere are 4 Shift/Reduce conflicts. Either use 'prefer_shifts' parser mode,\ntry to resolve manually or use GLR parsing.\n\n\n\nAs we can see this grammar has 4 Shift/Reduce conflicts. At the end of the\noutput we get an advice to either use \nprefer_shifts\n strategy that will always\nprefer shift over reduce. In this case that's not what we want.\n\n\nIf we look closely at the output we see that parglare gives us an informative\nexplanation why there are conflicts in our grammar.\n\n\nThe first conflict:\n\n\nState 6\n        1: E = E + E .   {+, *, STOP}\n        1: E = E . + E   {+, *, STOP}\n        2: E = E . * E   {+, *, STOP}\n\nIn state 6 and input symbol '+' can't decide whether to shift or\nreduce by production(s) '1: E = E + E'.\n\n\n\nTell us that when the parser saw addition \u2014 the dot in the above productions\nrepresents all possible positions of the parser in the input stream \u2014 and there\nis \n+\n ahead, it doesn't know shoud it reduce the addition or shift the \n+\n\ntoken.\n\n\nThis means that if we have an expression: \n1 + 2 + 3\n should we calculate it as\n\n(1 + 2) + 3\n or as \n1 + (2 + 3)\n. Of course, the result in this case would be\nthe same, but imagine what would happen if we had substration operation instead\nof addition. In arithmetic, this is defined by association which says that\naddition if left associative, thus the operation is executed from left to right.\n\n\nParglare enables you to define associativity for you productions by specifying\n\n{left}\n or \n{right}\n at the end of production. You can think of \nleft\n\nassociativity as telling the parser to prefer reduce over shift for this\nproduction and the \nright\n associativity for preferring shifts over reduces.\n\n\nLet's see the second conflict:\n\n\nState 6\n        1: E = E + E .   {+, *, STOP}\n        1: E = E . + E   {+, *, STOP}\n        2: E = E . * E   {+, *, STOP}\n\nIn state 6 and input symbol '*' can't decide whether to shift or\nreduce by production(s) '1: E = E + E'.\n\n\n\nIn the same state, when we saw addition and have \n*\n ahead parser can't decide.\n\n\nThis means that if we have an expression: \n1 + 2 * 3\n should we calculate it as\n\n(1 + 2) * 3\n or \n1 + (2 * 3)\n. In arithmetic this is handled by operation\npriority. We want multiplication to be executed first, so we should raise the\npriority of multiplication (or lower the priority of addition).\n\n\nE: E '+' E  {left, 1}\n | E '*' E  {left, 2}\n | number;\nnumber: /\\d+/;\n\n\n\nWe have augmented our grammar to state that both operation are left associative,\nthus the parser will know what to do in the case of \n1 + 2 + 3\n or \n1 * 2 * 3\n\nit will reduce from left to right, i.e. prefer reduce over shifts. We also\nspecified that addition has a priority of 1 and multiplication has a priority of\n2, thus the parser will know what to do in case of \n1 + 2 * 3\n, it will shift\n\n*\n instead of reducing addition as the multiplication should be\nreduced/calculated first.\n\n\n\n\nNote\n\n\nThe default priority for rules is 10.\n\n\n\n\nThis change in the grammar resolves all ambiguities and our grammar is now\nLR(1).\n\n\nLexical ambiguities\n\u00b6\n\n\nThere is another source of ambiguities.\n\n\nParglare uses integrated scanner, thus tokens are determined on the fly. This\ngives greater lexical disambiuation power but lexical ambiguities might arise\nnevertheless. Lexical ambiguity is a situation when at some place in the input\nmore than one recognizer match successfully.\n\n\nFor example, if in the input we have \n3.4\n and we expect at this place either an\ninteger or a float. Both of these recognizer can match the input. The integer\nrecognizer would match \n3\n while the float recognizer would match \n3.4\n. What\nshould we use?\n\n\nparglare has implicit lexical disambiguation strategy that will:\n\n\n\n\nUse priorities first.\n\n\nString recognizers are preferred over regexes (i.e. the most specific match).\n\n\nIf priorities are the same and we have no string recognizers use longest-match strategy.\n\n\nIf more recognizers still match use \nprefer\n rule if given.\n\n\nIf all else fails raise an exception. In case of GLR, ambiguity will be\n   handled by parser forking, i.e. you will end up with all solutions/trees.\n\n\n\n\nThus, when terminals are defined you can use priorities to favor some of the\nrecognizers, or we can use \nprefer\n to favor recognizer if there are multiple\nmatches of the same length.\n\n\nExample:\n\n\n  number = /\\d+/ {15};\n\n\n\nor:\n      number = /\\d+/ {prefer};",
            "title": "Conflict resolving"
        },
        {
            "location": "/conflicts/#lr-parsing",
            "text": "LR parser operates as a deterministic PDA (Push-down automata). It is a state\nmachine which is always in some state during parsing. The state machine must\ndeterministically decide what is the next state just based on its current state\nand one token of lookahead. This decision is given by LR tables which are\nprecalculated from the grammar before parsing even begins.  For example, let's see what happens if we have a simple expression grammar:  E: E '+' E\n | E '*' E\n | number;\nnumber: /\\d+/;  and we want to parse the following input:   1 + 2 * 3  Language defined by this grammar is ambiguous as the expression can be\ninterpreted either as:   ((1 + 2) * 3)  or:   (1 + (2 * 3))  In the parsing process, parser starts in state 0 and it sees token  1  ahead\n(one lookahead is used - LR(1)).  The only thing a parser can do in this case is to shift, i.e. to consume the\ntoken and advance the position. This operation transition the automata to some\nother state. From each state there is only one valid transition that can be\ntaken or the PDA won't be deterministic, i.e. we could simultaneously follow\ndifferent paths.  Current position would be (the dot represents the position):   1 . + 2 * 3  Now the parser sees  +  token ahead and the tables will tell him to reduce the\nnumber he just saw to  E  (a number is an expression according to the grammar).\nThus, on the stack the parser will have an expression  E  (actually LR states\nare kept on stack but that's not important for this little analysis). This\nreduction will advace PDA to some other state again. Each shift/reduce operation\nchange state so I'll not repeat that anymore.   Note  See  pglr command  which can be used to visualize PDA. Try to\nvisualize automata for this grammar.   After reduction parser will do shift of  +  token. There is nothing to reduce as\nthe subexpresison on stack is  E +  which can't be reduced as it's not complete.\nSo, the only thing we can do is to shift  2  token.  Now, the position is:    1 + 2 . * 3  And the stack is:    E + 2  And this is a place where the parser can't decide what to do. It can either\nreduce the sum on the stack or shift  *  and  3  and reduce multiplication\nfirst and the sumation afterwards.  If the sum is reduced first and  *  shifted afterwards we would get the\nfollowing result:   (1 + 2) * 3  If the shift of  *  and  3  is done instead of reducing, the reduction would\nfirst reduce multiplication and than sum (reduction is always done on the top of\nthe stack). We will have the following result:  1 + (2 * 3)  From the point of arithmetic priorities, preffered solution is the last one but\nthe parser don't know arithmentic rules.  If you analyze this grammar using  pglr command  you will see that\nthe LR tables have Shift/Reduce conflicts as there is a state in which parser\ncan't decide wether to shift or to reduce (we just saw that situation).  parglare gives you various tools to be more explicit with your grammar and to\nresolve those conflicts.  There are two situations when conflicts can't be resolved:   you need more than one lookahead to disambiguate  your language is inherently ambiguous   If you end up in one of these situations you should use GLR parsing, which will\nfork the parser in state which has multiple path, and explore all possibilities.",
            "title": "LR parsing"
        },
        {
            "location": "/conflicts/#resolving-conflicts",
            "text": "When we run:  $ pglr -d check expr.pg  where in  expr.pg  we have the above grammar, we get the following output at the end:  *** S/R conflicts ***\nThere are 4 S/R conflicts\n\nState 6\n        1: E = E + E .   {+, *, STOP}\n        1: E = E . + E   {+, *, STOP}\n        2: E = E . * E   {+, *, STOP}\n\nIn state 6 and input symbol '+' can't decide whether to shift or\nreduce by production(s) '1: E = E + E'.\n\nState 6\n        1: E = E + E .   {+, *, STOP}\n        1: E = E . + E   {+, *, STOP}\n        2: E = E . * E   {+, *, STOP}\n\nIn state 6 and input symbol '*' can't decide whether to shift or\nreduce by production(s) '1: E = E + E'.\n\nState 7\n        2: E = E * E .   {+, *, STOP}\n        1: E = E . + E   {+, *, STOP}\n        2: E = E . * E   {+, *, STOP}\n\nIn state 7 and input symbol '+' can't decide whether to shift or\nreduce by production(s) '2: E = E * E'.\n\nState 7\n        2: E = E * E .   {+, *, STOP}\n        1: E = E . + E   {+, *, STOP}\n        2: E = E . * E   {+, *, STOP}\n\nIn state 7 and input symbol '*' can't decide whether to shift or\nreduce by production(s) '2: E = E * E'.\nGrammar OK.\nThere are 4 Shift/Reduce conflicts. Either use 'prefer_shifts' parser mode,\ntry to resolve manually or use GLR parsing.  As we can see this grammar has 4 Shift/Reduce conflicts. At the end of the\noutput we get an advice to either use  prefer_shifts  strategy that will always\nprefer shift over reduce. In this case that's not what we want.  If we look closely at the output we see that parglare gives us an informative\nexplanation why there are conflicts in our grammar.  The first conflict:  State 6\n        1: E = E + E .   {+, *, STOP}\n        1: E = E . + E   {+, *, STOP}\n        2: E = E . * E   {+, *, STOP}\n\nIn state 6 and input symbol '+' can't decide whether to shift or\nreduce by production(s) '1: E = E + E'.  Tell us that when the parser saw addition \u2014 the dot in the above productions\nrepresents all possible positions of the parser in the input stream \u2014 and there\nis  +  ahead, it doesn't know shoud it reduce the addition or shift the  + \ntoken.  This means that if we have an expression:  1 + 2 + 3  should we calculate it as (1 + 2) + 3  or as  1 + (2 + 3) . Of course, the result in this case would be\nthe same, but imagine what would happen if we had substration operation instead\nof addition. In arithmetic, this is defined by association which says that\naddition if left associative, thus the operation is executed from left to right.  Parglare enables you to define associativity for you productions by specifying {left}  or  {right}  at the end of production. You can think of  left \nassociativity as telling the parser to prefer reduce over shift for this\nproduction and the  right  associativity for preferring shifts over reduces.  Let's see the second conflict:  State 6\n        1: E = E + E .   {+, *, STOP}\n        1: E = E . + E   {+, *, STOP}\n        2: E = E . * E   {+, *, STOP}\n\nIn state 6 and input symbol '*' can't decide whether to shift or\nreduce by production(s) '1: E = E + E'.  In the same state, when we saw addition and have  *  ahead parser can't decide.  This means that if we have an expression:  1 + 2 * 3  should we calculate it as (1 + 2) * 3  or  1 + (2 * 3) . In arithmetic this is handled by operation\npriority. We want multiplication to be executed first, so we should raise the\npriority of multiplication (or lower the priority of addition).  E: E '+' E  {left, 1}\n | E '*' E  {left, 2}\n | number;\nnumber: /\\d+/;  We have augmented our grammar to state that both operation are left associative,\nthus the parser will know what to do in the case of  1 + 2 + 3  or  1 * 2 * 3 \nit will reduce from left to right, i.e. prefer reduce over shifts. We also\nspecified that addition has a priority of 1 and multiplication has a priority of\n2, thus the parser will know what to do in case of  1 + 2 * 3 , it will shift *  instead of reducing addition as the multiplication should be\nreduced/calculated first.   Note  The default priority for rules is 10.   This change in the grammar resolves all ambiguities and our grammar is now\nLR(1).",
            "title": "Resolving conflicts"
        },
        {
            "location": "/conflicts/#lexical-ambiguities",
            "text": "There is another source of ambiguities.  Parglare uses integrated scanner, thus tokens are determined on the fly. This\ngives greater lexical disambiuation power but lexical ambiguities might arise\nnevertheless. Lexical ambiguity is a situation when at some place in the input\nmore than one recognizer match successfully.  For example, if in the input we have  3.4  and we expect at this place either an\ninteger or a float. Both of these recognizer can match the input. The integer\nrecognizer would match  3  while the float recognizer would match  3.4 . What\nshould we use?  parglare has implicit lexical disambiguation strategy that will:   Use priorities first.  String recognizers are preferred over regexes (i.e. the most specific match).  If priorities are the same and we have no string recognizers use longest-match strategy.  If more recognizers still match use  prefer  rule if given.  If all else fails raise an exception. In case of GLR, ambiguity will be\n   handled by parser forking, i.e. you will end up with all solutions/trees.   Thus, when terminals are defined you can use priorities to favor some of the\nrecognizers, or we can use  prefer  to favor recognizer if there are multiple\nmatches of the same length.  Example:    number = /\\d+/ {15};  or:\n      number = /\\d+/ {prefer};",
            "title": "Lexical ambiguities"
        },
        {
            "location": "/parse_trees/",
            "text": "Parse trees\n\u00b6\n\n\nDuring shift/reduce operations parser will call \nactions\n. If no\nuser actions are provided the default actions will be called which will build the\nparse tree. In the case of GLR parser multiple trees can be built simultaneously\n(the parse forest). If user actions are provided, no tree building takes place\nbut the user actions decide what to create for each shift/reduction.\n\n\nThe nodes of parse trees are instances of either \nNodeTerm\n for terminal nodes\n(leafs of the tree) or \nNodeNonTerm\n for non-terminal nodes (intermediate\nnodes).\n\n\nEach node of the tree has following attributes:\n\n\n\n\nstart_position\n/\nend_position\n - the start and end position in the input\n  stream where the node starts/ends. It is given in absolute 0-based offset. To\n  convert to line/column format for textual inputs you can use\n  \nparglare.pos_to_linecol(input_str, position)\n function which returns tuple\n  \n(line, column)\n. Of course, this call doesn't make any sense if you are\n  parsing a non-textual content.\n\n\nlayout_content\n - the layout that preceeds the given tree node. The layout\n  consists of whitespaces/comments.\n\n\nsymbol\n - a grammar symbol this node is created for.\n\n\n\n\nAdditionally, each \nNodeTerm\n has:\n\n\n\n\nvalue\n - the value (a part of input_str) which this terminal represents. It\n  can be viewed as \ninput_str[start_position:end_position]\n.\n\n\n\n\nAdditionally, each \nNodeNonTerm\n has:\n\n\n\n\nchildren\n - subnodes which are also of \nNodeNonTerm\n/\nNodeTerm\n type.\n  \nNodeNonTerm\n is iterable. Iterating over it will iterate over its children.\n\n\nproduction\n - a grammar production which reduction created this node.\n\n\n\n\nEach node has a \ntree_str()\n method which will return a string representation of\nthe subtree starting from the given node. If called on root node it will return\nthe string representation of the whole tree.",
            "title": "Parse trees"
        },
        {
            "location": "/parse_trees/#parse-trees",
            "text": "During shift/reduce operations parser will call  actions . If no\nuser actions are provided the default actions will be called which will build the\nparse tree. In the case of GLR parser multiple trees can be built simultaneously\n(the parse forest). If user actions are provided, no tree building takes place\nbut the user actions decide what to create for each shift/reduction.  The nodes of parse trees are instances of either  NodeTerm  for terminal nodes\n(leafs of the tree) or  NodeNonTerm  for non-terminal nodes (intermediate\nnodes).  Each node of the tree has following attributes:   start_position / end_position  - the start and end position in the input\n  stream where the node starts/ends. It is given in absolute 0-based offset. To\n  convert to line/column format for textual inputs you can use\n   parglare.pos_to_linecol(input_str, position)  function which returns tuple\n   (line, column) . Of course, this call doesn't make any sense if you are\n  parsing a non-textual content.  layout_content  - the layout that preceeds the given tree node. The layout\n  consists of whitespaces/comments.  symbol  - a grammar symbol this node is created for.   Additionally, each  NodeTerm  has:   value  - the value (a part of input_str) which this terminal represents. It\n  can be viewed as  input_str[start_position:end_position] .   Additionally, each  NodeNonTerm  has:   children  - subnodes which are also of  NodeNonTerm / NodeTerm  type.\n   NodeNonTerm  is iterable. Iterating over it will iterate over its children.  production  - a grammar production which reduction created this node.   Each node has a  tree_str()  method which will return a string representation of\nthe subtree starting from the given node. If called on root node it will return\nthe string representation of the whole tree.",
            "title": "Parse trees"
        },
        {
            "location": "/pglr/",
            "text": "The \npglr\n command\n\u00b6\n\n\npglr\n command is available after parglare is installed. This command is used to\ndebug the grammar, visualize the LR automata and make a visual trace of the GLR\nparsing.\n\n\nTo get the help on the command run:\n\n\n$ pglr --help\n\nusage: pglr [-h] [-d] [-i] cmd grammar [input_file]\n\nparglare checker and visualizer\n\npositional arguments:\n  cmd         Command - 'viz' or 'check' or 'trace'\n  grammar     parglare grammar file\n  input_file  input file for GLR trace subcommand.\n\noptional arguments:\n  -h, --help  show this help message and exit\n  -d          run in debug mode\n  -i          input_file for trace is input string, not file.\n\n\n\nChecking the grammar\n\u00b6\n\n\nTo check your grammar run:\n\n\n$ pglr check <grammar_file>\n\n\n\nwhere \n<grammar_file>\n is the path to your grammar file.\n\n\nIf there is no error in the grammar you will get \nGrammar OK.\n message. In case\nof error you will get error message with the information what is the error and\nwhere it is in the grammar.\n\n\nFor example:\n\n\n$ pglr check calc.pg\nError in the grammar file.\nError in file \"calc.pg\" at position 4,16 => \"/' E  left*, 2}\\n | E \".\nExpected: { or | or ; or Name or RegExTerm or StrTerm\n\n\n\nGetting detailed information\n\u00b6\n\n\nTo get the detailed information on the grammar run \npglr\n command in the debug mode.\n\n\n$ pglr -d check calc.pg\n\n\n*** GRAMMAR ***\nTerminals:\nnumber STOP + - ^ EMPTY ) \\d+(\\.\\d+)? ( EOF / *\nNonTerminals:\nS' E\nProductions:\n0: S' = E STOP\n1: E = E + E\n2: E = E - E\n3: E = E * E\n4: E = E / E\n5: E = E ^ E\n6: E = ( E )\n7: E = number\n\n\n*** STATES ***\n\nState 0\n        0: S' = . E STOP   {}\n        1: E = . E + E   {STOP, -, +, ^, ), /, *}\n        2: E = . E - E   {STOP, -, +, ^, ), /, *}\n        3: E = . E * E   {STOP, -, +, ^, ), /, *}\n        4: E = . E / E   {STOP, -, +, ^, ), /, *}\n        5: E = . E ^ E   {STOP, -, +, ^, ), /, *}\n        6: E = . ( E )   {STOP, -, +, ^, ), /, *}\n        7: E = . number   {STOP, -, +, ^, ), /, *}\n\n\n\n    GOTO:\n     E->1\n\n    ACTIONS:\n     (->SHIFT:2, number->SHIFT:3\n\n...\n\n\n\nThis will give enumerated all the productions of your grammars and all the\nstates. For each state you get the LR items with lookahead, elements of GOTO\ntable and elements of ACTIONS table. In the previous example state 0 will have a\ntransition to state 1 when \nE\n is reduced, transition to state 2 if \n(\n can\nbe shifted and transition to state 3 if \nnumber\n can be shifted.\n\n\nIn addition you will get a detailed information on all Shift/Reduce and\nReduce/Reduce conflicts which makes much easier to see the exact cause of\nambiguity and to use \ndisambiguation rules\n\nto resolve the conflicts or to go with GLR if the grammar is not LR(1).\n\n\n\n\nNote\n\n\nYou can use \n-d\n option with any \npglr\n command to put the parser in the debug\nmode and get a detailed output.\n\n\n\n\nVisualizing LR automata\n\u00b6\n\n\nTo visualize your automata with all the states and possible transitions run the\ncommand:\n\n\n$ pglr viz calc.pg\nGrammar OK.\nGenerating 'calc.pg.dot' file for the grammar PDA.\nUse dot viewer (e.g. xdot) or convert to pdf by running 'dot -Tpdf -O calc.pg.dot'\n\n\n\nAs given in the output you will get a \ndot\n file which represents LR automata\nvisualization. You can see this diagram using dot viewers or you can transform\nit to other file formats using the \ndot\n tool (you have to install Graphviz\nsoftware for that).\n\n\nThis is an example of LR automata visualization for the \ncalc\n grammar from the\nquick intro (click on the image to enlarge):\n\n\n\n\nTracing GLR parsing\n\u00b6\n\n\nGLR parser uses a graph-like stack (\nGraph-Structured Stack - GSS\n) and to\nunderstand what's going on during GLR operation GLR parser and \npglr\n command\nprovide a way to trace the GSS.\n\n\nTo run the GLR trace:\n\n\n$ pglr -i trace calc.pg \"2 + 3 * 5\"\n\n\n\nThe \n-i\n switch tells the command to treat the last parameter as the input\nstring to parse and not as a file name of the input.\n\n\n\n\nNote\n\n\nSince the GSS can be quite large and complex for larger inputs the advice is\nto use a minimal input that will exibit the intended behaviour for a\nvisualization to be usable.\n\n\n\n\nThe \ntrace\n subcommand implies \n-d\n switch so the parser will run in debug mode\nand produce the detailed output on the grammar, LR automata and the parsing\nprocess.\n\n\nAdditionally, a \ndot\n file will be created, with the name \nparglare_trace\n if\ninput is given on command line or \n<input_file_name>_trace.dot\n if input is\ngiven as a file. The \ndot\n file can be visualized using dot viewers or\ntransformed to other file formats using the \ndot\n tool.\n\n\nFor the command above, GLR trace visualization will be (click on the image to\nenlarge):\n\n\n\n\nDotted red arrows represent each step in the parsing process. They are numbered\nconsecutively. After the ordinal number is the action (either S-Shift or\nR-reduce). For shift action a grammar symbol and the shifted value is given. For\nreduction a production is given and the resulting head will have a parent node\ncloser to the beginning.\n\n\nBlack solid arrows are the links to the parent node in the GSS.\n\n\nThere are also dotted orange arrows (not shown in this example) that shows dropped\nempty reductions. Dropping happens when parser has found a better solution (i.e. a\nsolution with less empty reductions).\n\n\n\n\nNote\n\n\nPutting the GLR parser in debug mode from code (setting \ndebug\n constructor\nparameter to \nTrue\n) will give the debug/trace output as well as generate the\ntrace visualization.",
            "title": "pglr command"
        },
        {
            "location": "/pglr/#the-pglr-command",
            "text": "pglr  command is available after parglare is installed. This command is used to\ndebug the grammar, visualize the LR automata and make a visual trace of the GLR\nparsing.  To get the help on the command run:  $ pglr --help\n\nusage: pglr [-h] [-d] [-i] cmd grammar [input_file]\n\nparglare checker and visualizer\n\npositional arguments:\n  cmd         Command - 'viz' or 'check' or 'trace'\n  grammar     parglare grammar file\n  input_file  input file for GLR trace subcommand.\n\noptional arguments:\n  -h, --help  show this help message and exit\n  -d          run in debug mode\n  -i          input_file for trace is input string, not file.",
            "title": "The pglr command"
        },
        {
            "location": "/pglr/#checking-the-grammar",
            "text": "To check your grammar run:  $ pglr check <grammar_file>  where  <grammar_file>  is the path to your grammar file.  If there is no error in the grammar you will get  Grammar OK.  message. In case\nof error you will get error message with the information what is the error and\nwhere it is in the grammar.  For example:  $ pglr check calc.pg\nError in the grammar file.\nError in file \"calc.pg\" at position 4,16 => \"/' E  left*, 2}\\n | E \".\nExpected: { or | or ; or Name or RegExTerm or StrTerm",
            "title": "Checking the grammar"
        },
        {
            "location": "/pglr/#getting-detailed-information",
            "text": "To get the detailed information on the grammar run  pglr  command in the debug mode.  $ pglr -d check calc.pg  *** GRAMMAR ***\nTerminals:\nnumber STOP + - ^ EMPTY ) \\d+(\\.\\d+)? ( EOF / *\nNonTerminals:\nS' E\nProductions:\n0: S' = E STOP\n1: E = E + E\n2: E = E - E\n3: E = E * E\n4: E = E / E\n5: E = E ^ E\n6: E = ( E )\n7: E = number\n\n\n*** STATES ***\n\nState 0\n        0: S' = . E STOP   {}\n        1: E = . E + E   {STOP, -, +, ^, ), /, *}\n        2: E = . E - E   {STOP, -, +, ^, ), /, *}\n        3: E = . E * E   {STOP, -, +, ^, ), /, *}\n        4: E = . E / E   {STOP, -, +, ^, ), /, *}\n        5: E = . E ^ E   {STOP, -, +, ^, ), /, *}\n        6: E = . ( E )   {STOP, -, +, ^, ), /, *}\n        7: E = . number   {STOP, -, +, ^, ), /, *}\n\n\n\n    GOTO:\n     E->1\n\n    ACTIONS:\n     (->SHIFT:2, number->SHIFT:3\n\n...  This will give enumerated all the productions of your grammars and all the\nstates. For each state you get the LR items with lookahead, elements of GOTO\ntable and elements of ACTIONS table. In the previous example state 0 will have a\ntransition to state 1 when  E  is reduced, transition to state 2 if  (  can\nbe shifted and transition to state 3 if  number  can be shifted.  In addition you will get a detailed information on all Shift/Reduce and\nReduce/Reduce conflicts which makes much easier to see the exact cause of\nambiguity and to use  disambiguation rules \nto resolve the conflicts or to go with GLR if the grammar is not LR(1).   Note  You can use  -d  option with any  pglr  command to put the parser in the debug\nmode and get a detailed output.",
            "title": "Getting detailed information"
        },
        {
            "location": "/pglr/#visualizing-lr-automata",
            "text": "To visualize your automata with all the states and possible transitions run the\ncommand:  $ pglr viz calc.pg\nGrammar OK.\nGenerating 'calc.pg.dot' file for the grammar PDA.\nUse dot viewer (e.g. xdot) or convert to pdf by running 'dot -Tpdf -O calc.pg.dot'  As given in the output you will get a  dot  file which represents LR automata\nvisualization. You can see this diagram using dot viewers or you can transform\nit to other file formats using the  dot  tool (you have to install Graphviz\nsoftware for that).  This is an example of LR automata visualization for the  calc  grammar from the\nquick intro (click on the image to enlarge):",
            "title": "Visualizing LR automata"
        },
        {
            "location": "/pglr/#tracing-glr-parsing",
            "text": "GLR parser uses a graph-like stack ( Graph-Structured Stack - GSS ) and to\nunderstand what's going on during GLR operation GLR parser and  pglr  command\nprovide a way to trace the GSS.  To run the GLR trace:  $ pglr -i trace calc.pg \"2 + 3 * 5\"  The  -i  switch tells the command to treat the last parameter as the input\nstring to parse and not as a file name of the input.   Note  Since the GSS can be quite large and complex for larger inputs the advice is\nto use a minimal input that will exibit the intended behaviour for a\nvisualization to be usable.   The  trace  subcommand implies  -d  switch so the parser will run in debug mode\nand produce the detailed output on the grammar, LR automata and the parsing\nprocess.  Additionally, a  dot  file will be created, with the name  parglare_trace  if\ninput is given on command line or  <input_file_name>_trace.dot  if input is\ngiven as a file. The  dot  file can be visualized using dot viewers or\ntransformed to other file formats using the  dot  tool.  For the command above, GLR trace visualization will be (click on the image to\nenlarge):   Dotted red arrows represent each step in the parsing process. They are numbered\nconsecutively. After the ordinal number is the action (either S-Shift or\nR-reduce). For shift action a grammar symbol and the shifted value is given. For\nreduction a production is given and the resulting head will have a parent node\ncloser to the beginning.  Black solid arrows are the links to the parent node in the GSS.  There are also dotted orange arrows (not shown in this example) that shows dropped\nempty reductions. Dropping happens when parser has found a better solution (i.e. a\nsolution with less empty reductions).   Note  Putting the GLR parser in debug mode from code (setting  debug  constructor\nparameter to  True ) will give the debug/trace output as well as generate the\ntrace visualization.",
            "title": "Tracing GLR parsing"
        },
        {
            "location": "/about/CONTRIBUTING/",
            "text": "Contributing\n\u00b6\n\n\nContributions are welcome, and they are greatly appreciated! Every\nlittle bit helps, and credit will always be given.\n\n\nYou can contribute in many ways:\n\n\nTypes of Contributions\n\u00b6\n\n\nReport Bugs\n\u00b6\n\n\nReport bugs at https://github.com/igordejanovic/parglare/issues.\n\n\nIf you are reporting a bug, please include:\n\n\n\n\nYour operating system name and version.\n\n\nAny details about your local setup that might be helpful in troubleshooting.\n\n\nDetailed steps to reproduce the bug.\n\n\n\n\nFix Bugs\n\u00b6\n\n\nLook through the GitHub issues for bugs. Anything tagged with \"bug\"\nand \"help wanted\" is open to whoever wants to implement it.\n\n\nImplement Features\n\u00b6\n\n\nLook through the GitHub issues for features. Anything tagged with \"enhancement\"\nand \"help wanted\" is open to whoever wants to implement it.\n\n\nWrite Documentation\n\u00b6\n\n\nparglare could always use more documentation, whether as part of the\nofficial parglare docs, in docstrings, or even on the web in blog posts,\narticles, and such.\n\n\nSubmit Feedback\n\u00b6\n\n\nThe best way to send feedback is to file an issue at https://github.com/igordejanovic/parglare/issues.\n\n\nIf you are proposing a feature:\n\n\n\n\nExplain in detail how it would work.\n\n\nKeep the scope as narrow as possible, to make it easier to implement.\n\n\nRemember that this is a volunteer-driven project, and that contributions\n  are welcome :)\n\n\n\n\nGet Started!\n\u00b6\n\n\nReady to contribute? Here's how to set up \nparglare\n for local development.\n\n\n\n\nFork the \nparglare\n repo on GitHub.\n\n\n\n\nClone your fork locally:\n\n\nbash\n  $ git clone git@github.com:your_name_here/parglare.git\n\n\n\n\n\n\nInstall your local copy into a virtualenv. Assuming you have\n   virtualenvwrapper installed, this is how you set up your fork for local\n   development:\n\n\nbash\n$ mkvirtualenv parglare\n$ cd parglare/\n$ python setup.py develop\n\n\n\n\n\n\nCreate a branch for local development::\n\n\nbash\n  $ git checkout -b name-of-your-bugfix-or-feature\n\n\n\n\n\n\nNow you can make your changes locally.\n\n\n\n\n\n\nWhen you're done making changes, check that your changes pass flake8 and the\n   tests, including testing other Python versions with tox:\n\n\nbash\n$ flake8 parglare tests\n$ python setup.py test or py.test\n$ tox\n\n\n\n\n\n\nTo get flake8 and tox, just pip install them into your virtualenv.\n\n\n\n\n\n\nCommit your changes and push your branch to GitHub:\n\n\nbash\n$ git add .\n$ git commit -m \"Your detailed description of your changes.\"\n$ git push origin name-of-your-bugfix-or-feature\n\n\n\n\n\n\nSubmit a pull request through the GitHub website.\n\n\n\n\n\n\nPull Request Guidelines\n\u00b6\n\n\nBefore you submit a pull request, check that it meets these guidelines:\n\n\n\n\nThe pull request should include tests.\n\n\nIf the pull request adds functionality, the docs should be updated. Put\n   your new functionality into a function with a docstring, and add the\n   feature to the list in README.rst.\n\n\nThe pull request should work for Python 2.6, 2.7, 3.3-3.6, and for\n   PyPy. Check https://travis-ci.org/igordejanovic/parglare/pull_requests and\n   make sure that the tests pass for all supported Python versions.\n\n\n\n\nTips\n\u00b6\n\n\nTo run a subset of tests:\n\n\n$ py.test tests.test_parglare",
            "title": "Contributing"
        },
        {
            "location": "/about/CONTRIBUTING/#contributing",
            "text": "Contributions are welcome, and they are greatly appreciated! Every\nlittle bit helps, and credit will always be given.  You can contribute in many ways:",
            "title": "Contributing"
        },
        {
            "location": "/about/CONTRIBUTING/#types-of-contributions",
            "text": "",
            "title": "Types of Contributions"
        },
        {
            "location": "/about/CONTRIBUTING/#report-bugs",
            "text": "Report bugs at https://github.com/igordejanovic/parglare/issues.  If you are reporting a bug, please include:   Your operating system name and version.  Any details about your local setup that might be helpful in troubleshooting.  Detailed steps to reproduce the bug.",
            "title": "Report Bugs"
        },
        {
            "location": "/about/CONTRIBUTING/#fix-bugs",
            "text": "Look through the GitHub issues for bugs. Anything tagged with \"bug\"\nand \"help wanted\" is open to whoever wants to implement it.",
            "title": "Fix Bugs"
        },
        {
            "location": "/about/CONTRIBUTING/#implement-features",
            "text": "Look through the GitHub issues for features. Anything tagged with \"enhancement\"\nand \"help wanted\" is open to whoever wants to implement it.",
            "title": "Implement Features"
        },
        {
            "location": "/about/CONTRIBUTING/#write-documentation",
            "text": "parglare could always use more documentation, whether as part of the\nofficial parglare docs, in docstrings, or even on the web in blog posts,\narticles, and such.",
            "title": "Write Documentation"
        },
        {
            "location": "/about/CONTRIBUTING/#submit-feedback",
            "text": "The best way to send feedback is to file an issue at https://github.com/igordejanovic/parglare/issues.  If you are proposing a feature:   Explain in detail how it would work.  Keep the scope as narrow as possible, to make it easier to implement.  Remember that this is a volunteer-driven project, and that contributions\n  are welcome :)",
            "title": "Submit Feedback"
        },
        {
            "location": "/about/CONTRIBUTING/#get-started",
            "text": "Ready to contribute? Here's how to set up  parglare  for local development.   Fork the  parglare  repo on GitHub.   Clone your fork locally:  bash\n  $ git clone git@github.com:your_name_here/parglare.git    Install your local copy into a virtualenv. Assuming you have\n   virtualenvwrapper installed, this is how you set up your fork for local\n   development:  bash\n$ mkvirtualenv parglare\n$ cd parglare/\n$ python setup.py develop    Create a branch for local development::  bash\n  $ git checkout -b name-of-your-bugfix-or-feature    Now you can make your changes locally.    When you're done making changes, check that your changes pass flake8 and the\n   tests, including testing other Python versions with tox:  bash\n$ flake8 parglare tests\n$ python setup.py test or py.test\n$ tox    To get flake8 and tox, just pip install them into your virtualenv.    Commit your changes and push your branch to GitHub:  bash\n$ git add .\n$ git commit -m \"Your detailed description of your changes.\"\n$ git push origin name-of-your-bugfix-or-feature    Submit a pull request through the GitHub website.",
            "title": "Get Started!"
        },
        {
            "location": "/about/CONTRIBUTING/#pull-request-guidelines",
            "text": "Before you submit a pull request, check that it meets these guidelines:   The pull request should include tests.  If the pull request adds functionality, the docs should be updated. Put\n   your new functionality into a function with a docstring, and add the\n   feature to the list in README.rst.  The pull request should work for Python 2.6, 2.7, 3.3-3.6, and for\n   PyPy. Check https://travis-ci.org/igordejanovic/parglare/pull_requests and\n   make sure that the tests pass for all supported Python versions.",
            "title": "Pull Request Guidelines"
        },
        {
            "location": "/about/CONTRIBUTING/#tips",
            "text": "To run a subset of tests:  $ py.test tests.test_parglare",
            "title": "Tips"
        },
        {
            "location": "/about/LICENSE/",
            "text": "MIT License\n\n\nCopyright (c) 2016-2017, Igor R. Dejanovic\n\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of\nthis software and associated documentation files (the \"Software\"), to deal in\nthe Software without restriction, including without limitation the rights to\nuse, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\nthe Software, and to permit persons to whom the Software is furnished to do so,\nsubject to the following conditions:\n\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\nFOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\nCOPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\nIN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\nCONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.",
            "title": "License"
        }
    ]
}